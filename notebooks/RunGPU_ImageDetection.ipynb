{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FkCRP0O_M-QC",
        "outputId": "31d65cf5-3a02-43a4-bca4-c92eddeab17b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.65-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
            "Collecting pillow-heif>=0.18.0 (from roboflow)\n",
            "  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "Downloading roboflow-1.1.65-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: filetype, python-dotenv, pillow-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.65\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "suc7R8nAJZIb"
      },
      "outputs": [],
      "source": [
        "# Dataset Loading\n",
        "# from .utils import move_dataset_detection\n",
        "from pathlib import Path\n",
        "from roboflow import Roboflow\n",
        "\n",
        "#Scripts/utils.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "#model/utils.py\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#model/detection/train.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "# from .utils import collate_fn, CarDataset, Averager, load_dataset, training_model\n",
        "# from .model import build_model_object_detection\n",
        "# from ..utils import plot_loss_history\n",
        "\n",
        "#model/detection/utils.py\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# from ..utils import save_model, writing_training_logs\n",
        "\n",
        "#model/detection/model.py\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "\n",
        "#model/detection/eval.py\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from submission.model.detection.utils import CarDataset, collate_fn, Averager\n",
        "# from submission.model.detection.model import build_model_object_detection\n",
        "# from ..utils import load_model\n",
        "# from .utils import loss_epoch, load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihX7JCF9Jkao"
      },
      "source": [
        "### Scripts/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRe3FxWPJj2H"
      },
      "outputs": [],
      "source": [
        "def move_dataset_detection(source_folder, destination_folder, remove=False):\n",
        "    \"\"\"\n",
        "    Moves specified folders from the source directory to the destination directory.\n",
        "    Args:\n",
        "        source_folder (str): The path to the source directory containing the dataset.\n",
        "        destination_folder (str): The path to the destination directory where folders will be copied.\n",
        "        remove (bool): If True, removes the source directory after copying. Default is False.\n",
        "    \"\"\"\n",
        "    source_dir = source_folder\n",
        "    destination_dir = destination_folder\n",
        "\n",
        "    # Define the folders to copy\n",
        "    folders_to_copy = ['test', 'train', 'valid']\n",
        "\n",
        "    print(f\"Contents of the downloaded directory: {source_dir}\")\n",
        "    try:\n",
        "        print(os.listdir(source_dir))\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The source directory {source_dir} was not found.\")\n",
        "\n",
        "\n",
        "    # Copy each folder\n",
        "    for folder in folders_to_copy:\n",
        "        source_folder_path = os.path.join(source_dir, folder)\n",
        "        destination_folder_path = os.path.join(destination_dir, folder)\n",
        "        print(f\"Copying {source_folder_path} to {destination_folder_path}\")\n",
        "        try:\n",
        "            shutil.copytree(source_folder_path, destination_folder_path, dirs_exist_ok=True)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Source folder {source_folder_path} not found. Please check the directory structure of the downloaded dataset.\")\n",
        "\n",
        "\n",
        "    print(\"Finished copying specified folders.\")\n",
        "\n",
        "    # Delete the source folder\n",
        "    # Remove the source directory and its contents\n",
        "    \n",
        "    if remove:\n",
        "        if os.path.exists(source_dir):\n",
        "            print(f\"Removing source directory: {source_dir}\")\n",
        "            shutil.rmtree(source_dir)\n",
        "            print(f\"Source directory {source_dir} removed successfully.\")\n",
        "        else:\n",
        "            print(f\"Source directory {source_dir} does not exist. No action needed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ofCfnIwJ_eo"
      },
      "source": [
        "### model/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyDRXX_tKFes"
      },
      "outputs": [],
      "source": [
        "def save_model(model, path_to_save):\n",
        "    \"\"\"\n",
        "    Save the model weights to the specified path.\n",
        "    \n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        path_to_save: Path to save the model weights.\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), path_to_save)\n",
        "    print(f\"Model weights saved to {path_to_save}\")\n",
        "\n",
        "def load_model(model, path_load_from, device):\n",
        "    \"\"\"\n",
        "    Save the model weights to the specified path.\n",
        "    \n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        path_to_save: Path to save the model weights.\n",
        "    \"\"\"\n",
        "    # path_load_from = '/content/best_model_weights.pth'\n",
        "    model.load_state_dict(torch.load(path_load_from, map_location=device))\n",
        "    print(f\"Model weights loaded successfully from {path_load_from}\")\n",
        "\n",
        "def writing_training_logs(path_to_save, message):\n",
        "    \"\"\"\n",
        "    Write training logs to a file.\n",
        "    \n",
        "    Args:\n",
        "        path_to_save: Path to save the training logs.\n",
        "        message: Message to write in the log file.\n",
        "    \"\"\"\n",
        "    with open(f'{path_to_save}', 'a') as f:\n",
        "        f.write(message + '\\n')\n",
        "    print(message)\n",
        "\n",
        "def plot_loss_history(loss_history, path_to_save):\n",
        "    \"\"\"\n",
        "    Plot the training and validation loss history.\n",
        "    \n",
        "    Args:\n",
        "        loss_history: Dictionary containing training and validation loss history.\n",
        "        path_to_save: Path to save the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(loss_history['train'], label='Training Loss')\n",
        "    plt.plot(loss_history['val'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss History')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(f'{path_to_save}/loss_history.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_metric_history(metric_history, path_to_save):\n",
        "    \"\"\"\n",
        "    Plot the training and validation metric history.\n",
        "    \n",
        "    Args:\n",
        "        metric_history: Dictionary containing training and validation metric history.\n",
        "        path_to_save: Path to save the plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(metric_history['train'], label='Training Metric')\n",
        "    plt.plot(metric_history['val'], label='Validation Metric')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metric')\n",
        "    plt.title('Training and Validation Metric History')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(f'{path_to_save}/metric_history.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3_pZzaFKMUK"
      },
      "source": [
        "### model/detection/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsQTZYUVL0GM"
      },
      "outputs": [],
      "source": [
        "def filter_car_data(df_annot):\n",
        "    \"\"\"Filter the annotations DataFrame to include only car-related categories.\"\"\"\n",
        "    df_annot = df_annot[(df_annot['category_id'] == 3) | (df_annot['category_id'] == 6)]\n",
        "    return df_annot\n",
        "\n",
        "def read_annotation(dir_name, text_name):\n",
        "    \"\"\"Read annotations from a JSON file and return a DataFrame.\"\"\"\n",
        "    with open(f'{dir_name}/_annotations.coco.json', 'r') as file:\n",
        "        data = json.load(file)\n",
        "        df = pd.DataFrame(data[text_name])\n",
        "    return df\n",
        "  \n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function to combine a list of samples into a batch.\"\"\"\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def handle_bbox_format(df):\n",
        "    \"\"\"Handle the bounding box format in the DataFrame.\"\"\"\n",
        "    splited_bbox = df['bbox'].apply(pd.Series)\n",
        "    #change columns\n",
        "    splited_bbox.columns =['x','y','w','h']\n",
        "    df = pd.concat([df,splited_bbox], axis=1)\n",
        "\n",
        "    df = df.drop(['bbox','segmentation'], axis=1)\n",
        "    df['x'] = df['x'].astype(np.float64)\n",
        "    df['y'] = df['y'].astype(np.float64)\n",
        "    df['w'] = df['w'].astype(np.float64)\n",
        "    df['h'] = df['h'].astype(np.float64)\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_dataset(dir_name):\n",
        "    \"\"\"Load the dataset from the specified directory.\"\"\"\n",
        "    df_categories = read_annotation(dir_name, 'categories')\n",
        "    df_images = read_annotation(dir_name, 'images')\n",
        "    df_annot = read_annotation(dir_name, 'annotations')\n",
        "\n",
        "    df_annot = handle_bbox_format(df_annot)\n",
        "    df_annot_filter = filter_car_data(df_annot)\n",
        "\n",
        "    return df_images, df_annot_filter\n",
        "\n",
        "class CarDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe_image, dataframe_annot, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_ids = dataframe_annot['image_id'].unique()\n",
        "        self.df_image = dataframe_image\n",
        "        self.df_annot = dataframe_annot\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "\n",
        "        image_id = self.image_ids[index]\n",
        "        # file_name = self.df_image[self.df_image['id'] == image_id]['file_name'].item()\n",
        "        file_name = self.df_image[self.df_image['id'] == image_id]['file_name'].iloc[0]\n",
        "\n",
        "        # Get all the bbox records for the current image_id\n",
        "        records = self.df_annot[self.df_annot['image_id'] == image_id]\n",
        "\n",
        "        # Read the image, and convert it to RGB format\n",
        "        # Normalize the image to [0, 1] range\n",
        "        image = cv2.imread(f'{self.image_dir}/{file_name}', cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        image /= 255.0\n",
        "\n",
        "\n",
        "        #Change the bboxe format from xywh to xyxy\n",
        "        boxes = records[['x', 'y', 'w', 'h']].values\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
        "\n",
        "        # Calculate the area of the bounding boxes\n",
        "        # area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        area = records['area'].values\n",
        "        area = torch.as_tensor(area, dtype=torch.float32)\n",
        "\n",
        "        # there is only one class\n",
        "        # set the labels to 1 for each bounding box\n",
        "        # records.shape[0] gives the number of bounding boxes for each image\n",
        "        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = torch.tensor([index])\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {\n",
        "                'image': image,\n",
        "                'bboxes': target['boxes'],\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "\n",
        "            # Ensure transformed boxes are correctly handled and converted back to a tensor\n",
        "            # We need to convert this back to a tensor of shape (N, 4)\n",
        "            if sample['bboxes']: # Check if bboxes is not empty\n",
        "                target['boxes'] = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
        "                target['labels'] = torch.tensor(sample['labels'], dtype=torch.int64)\n",
        "\n",
        "            else:\n",
        "                # Handle cases where transforms might result in no bounding boxes\n",
        "                target['boxes'] = torch.empty((0, 4), dtype=torch.float32)\n",
        "                target['labels'] = torch.empty((0,), dtype=torch.int64)\n",
        "                target['area'] = torch.empty((0,), dtype=torch.float32)\n",
        "                target['iscrowd'] = torch.empty((0,), dtype=torch.int64)\n",
        "\n",
        "        return image, target, image_id\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]\n",
        "\n",
        "class Averager:\n",
        "    def __init__(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "    def send(self, value):\n",
        "        self.current_total += value\n",
        "        self.iterations += 1\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        if self.iterations == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return 1.0 * self.current_total / self.iterations\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_total = 0.0\n",
        "        self.iterations = 0.0\n",
        "\n",
        "def loss_epoch(process_name=\"Training\", \n",
        "               loss_tracker=None,\n",
        "               data_loader=None,\n",
        "               model=None,\n",
        "               optimizer=None,\n",
        "               device=None,\n",
        "               path_to_save=None):\n",
        "    \"\"\"\n",
        "    Function to compute the loss for a single epoch.\n",
        "    Args:\n",
        "        process_name (str): Name of the process, either \"Training\" or \"Validation\".\n",
        "        loss_tracker (Averager): Tracker to accumulate loss values.\n",
        "        data_loader (DataLoader): DataLoader for the dataset.   \n",
        "        model (torch.nn.Module): The model to compute the loss.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for the model.\n",
        "        device (torch.device): Device to run the model on (CPU or GPU).\n",
        "        path_to_save (str): Path to save the training logs.\n",
        "    Returns:\n",
        "        loss_value (float): The average loss value for the epoch.\n",
        "    \"\"\"\n",
        "    \n",
        "    if path_to_save is not None:\n",
        "        path_batches_logs = f'{path_to_save}/training_history/{process_name.lower()}_loss_batches.txt'\n",
        "    \n",
        "    for itr, (images, targets, image_ids) in enumerate(data_loader):\n",
        "        images = [torch.from_numpy(image).permute(2, 0, 1).to(device) for image in images]\n",
        "        targets = [{k: torch.as_tensor(v).to(device) if isinstance(v, np.ndarray) else v.to(device) if hasattr(v, 'to') else v\n",
        "        for k, v in t.items()} for t in targets]\n",
        "\n",
        "        # Updated  \n",
        "        if process_name != \"Training\":\n",
        "            with torch.no_grad():\n",
        "                model.train()  # Set the model in training mode to compute the loss\n",
        "                loss_dict = model(images, targets)\n",
        "\n",
        "                model.eval() # Set the model to evaluation mode after computing the loss\n",
        "        else:\n",
        "            model.train() # Set training mode for the model to compute the loss\n",
        "            loss_dict = model(images, targets)\n",
        "        \n",
        "        if isinstance(loss_dict, dict):\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            loss_value = losses.item()\n",
        "            loss_tracker.send(loss_value)\n",
        "\n",
        "            #Only run this when training\n",
        "            if process_name == \"Training\":\n",
        "                optimizer.zero_grad()\n",
        "                losses.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Clear CUDA cache to help free up memory\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # TODO : CREATE A FUNCTION TO LOGS THE TRAINING LOGS\n",
        "            if (itr + 1) % 50 == 0:\n",
        "                print(f\"{process_name} Iteration #{(itr + 1)} loss: {loss_value}\")\n",
        "                if path_to_save is not None:\n",
        "                    writing_training_logs(f'{path_batches_logs}', \n",
        "                                      f\"Iteration #{(itr + 1)} loss: {loss_value:.4f}\")\n",
        "    \n",
        "    #Loss value after processing all batches\n",
        "    loss_value = loss_tracker.value\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "def training_model(train_data_loader,\n",
        "                   training_loss_tracker,\n",
        "                   valid_data_loader,\n",
        "                   validation_loss_tracker,\n",
        "                   model,\n",
        "                   optimizer,\n",
        "                   device,\n",
        "                   num_epochs,\n",
        "                   lr_scheduler=None,\n",
        "                   path_to_save=None):\n",
        "    \"\"\"\n",
        "    Function to train the model for a specified number of epochs.\n",
        "    Args:\n",
        "        train_data_loader (DataLoader): DataLoader for the training dataset.\n",
        "        training_loss_tracker (Averager): Tracker to accumulate training loss values.\n",
        "        valid_data_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        validation_loss_tracker (Averager): Tracker to accumulate validation loss values.\n",
        "        model (torch.nn.Module): The model to be trained.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for the model.\n",
        "        device (torch.device): Device to run the model on (CPU or GPU).\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "        lr_scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler. Defaults to None.\n",
        "        path_to_save (str, optional): Path to save training logs and best model weights. Defaults to None.\n",
        "    Returns:\n",
        "        loss_history (dict): Dictionary containing training and validation loss history.\n",
        "    \"\"\"\n",
        "    loss_history = {\"train\": [], \"val\": []}  # history of loss values in each epoch\n",
        "\n",
        "    best_validation_loss = float('inf')  # Initialize best validation loss to a large value\n",
        "\n",
        "    path_training_logs = f'{path_to_save}/training_history/training_logs.txt'\n",
        "    path_best_model = f'{path_to_save}/best_model/best_model_weights.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        training_loss_tracker.reset() # Reset the loss tracker for each epoch\n",
        "        validation_loss_tracker.reset() # Reset the validation loss tracker for each epoch\n",
        "\n",
        "        #path_to_save = root_dir + '/logs/detection'\n",
        "        writing_training_logs(f'{path_training_logs}', \n",
        "                              f\"Starting Epoch #{epoch+1}...\")\n",
        "        print(f\"Starting training for Epoch #{epoch+1}...\")\n",
        "\n",
        "        training_loss = loss_epoch(\n",
        "            process_name=\"Training\", \n",
        "            loss_tracker=training_loss_tracker,\n",
        "            data_loader=train_data_loader,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "            path_to_save=path_to_save\n",
        "        )\n",
        "\n",
        "        # update the learning rate\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "        \n",
        "        loss_history[\"train\"].append(training_loss)\n",
        "\n",
        "        print(f\"Epoch #{epoch+1} Training loss: {training_loss}\")\n",
        "        writing_training_logs(f'{path_training_logs}', \n",
        "                              f\"Epoch #{epoch+1} Training loss: {training_loss:.4f}\")\n",
        "        \n",
        "        # VALIDATION PHASE PART\n",
        "        writing_training_logs(f'{path_training_logs}', \n",
        "                              f\"Start validation for Epoch #{epoch+1}...\")\n",
        "        print(f\"Starting validation for Epoch #{epoch+1}...\")\n",
        "\n",
        "        val_loss = loss_epoch(\n",
        "            process_name=\"Validation\", \n",
        "            loss_tracker=validation_loss_tracker,\n",
        "            data_loader=valid_data_loader,\n",
        "            model=model,\n",
        "            optimizer=None,\n",
        "            device=device,\n",
        "            path_to_save=path_to_save\n",
        "        )\n",
        "        loss_history[\"val\"].append(val_loss)\n",
        "        \n",
        "        writing_training_logs(f'{path_training_logs}',\n",
        "                              f\"Epoch #{epoch+1} Validation loss: {val_loss:.4f}\")\n",
        "        \n",
        "        if val_loss < best_validation_loss:\n",
        "            best_validation_loss = val_loss \n",
        "            writing_training_logs(f'{path_training_logs}',\n",
        "                                  f\"New best validation loss: {best_validation_loss:.4f} at epoch #{epoch+1}. Saving model...\")\n",
        "            print(f\"New best validation loss: {best_validation_loss:.4f} at epoch #{epoch+1}. Saving model...\")\n",
        "            \n",
        "            # Save the model weights\n",
        "            save_model(model, f\"{path_best_model}\")\n",
        "            writing_training_logs(f'{path_training_logs}',\n",
        "                                  f\"Model weights saved to {path_best_model}\")\n",
        "\n",
        "        writing_training_logs(f'{path_training_logs}',\n",
        "                                f\"Finished Training.\")\n",
        "        print(f\"Training logs can be found at {path_training_logs}\")\n",
        "        print(f\"Best Weight Model can be found at {path_best_model}\")\n",
        "    return loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRXYUv2BKQ5C"
      },
      "source": [
        "### model/detection/model.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFsl7JrEMLhl"
      },
      "outputs": [],
      "source": [
        "def build_model_object_detection(backbone='resnet50', num_class=2, use_pretrained=True):\n",
        "    # 1. Create the same backbone used in the pretrained model\n",
        "    # This will create resnet50 with FPN (Feature Pyramid Network)\n",
        "    backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "\n",
        "    # 2. Define AnchorGenerator (same as default)\n",
        "    # Correct the aspect_ratios to match the default pretrained model's RPN head expectation.\n",
        "    # The default config uses 3 aspect ratios per spatial location.\n",
        "    anchor_generator = AnchorGenerator(\n",
        "      sizes=((32,), (64,), (128,), (256,), (512,)),\n",
        "      aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
        "    )\n",
        "\n",
        "    # 3. Define ROI Pooler (same as in torchvision)\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "      featmap_names=['0', '1', '2', '3'], # The default resnet_fpn_backbone returns 5 feature maps (p2, p3, p4, p5, p6).\n",
        "                                              # Check the return_layers of the default backbone.\n",
        "                                              # Looking at resnet_fpn_backbone source, it returns {\"0\": p2, \"1\": p3, \"2\": p4, \"3\": p5, \"4\": p6}.\n",
        "                                              # So we need 5 feature map names.\n",
        "      output_size=7,\n",
        "      sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    # 4. Assemble the Faster R-CNN model\n",
        "    model = FasterRCNN(\n",
        "      backbone=backbone,\n",
        "      num_classes=num_class,  # use 2 for our case, car and background\n",
        "      rpn_anchor_generator=anchor_generator,\n",
        "      box_roi_pool=roi_pooler\n",
        "    )\n",
        "\n",
        "    if use_pretrained:\n",
        "\n",
        "        pretrained_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "        pretrained_dict = pretrained_model.state_dict()\n",
        "        model_dict = model.state_dict()\n",
        "\n",
        "        #Filtered the last layer, because the difference number of class\n",
        "        filtered_dict = {\n",
        "            k: v for k, v in pretrained_dict.items()\n",
        "            if k in model_dict and not k.startswith('roi_heads.box_predictor')\n",
        "        }\n",
        "        model.load_state_dict(filtered_dict, strict=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES4HeuJKMP2z"
      },
      "source": [
        "## Main Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBVOsHc7MhkL",
        "outputId": "c0cb0a34-0c41-4675-e176-88913e4502cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "project_dir = os.getcwd()\n",
        "print(project_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-72tskpKMnPX"
      },
      "source": [
        "### Scripts/download_data_detection.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUTfndPoq3d4",
        "outputId": "10e0fdd1-3e19-40e0-9f2d-311846b72b1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in LabelChangeTest-1 to coco:: 100%|██████████| 56822/56822 [00:02<00:00, 19391.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to LabelChangeTest-1 in coco:: 100%|██████████| 908/908 [00:00<00:00, 4752.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of the downloaded directory: /content/LabelChangeTest-1\n",
            "['README.dataset.txt', 'README.roboflow.txt', 'train', 'test', 'valid']\n",
            "Copying /content/LabelChangeTest-1/test to /content/data/detection/test\n",
            "Copying /content/LabelChangeTest-1/train to /content/data/detection/train\n",
            "Copying /content/LabelChangeTest-1/valid to /content/data/detection/valid\n",
            "Finished copying specified folders.\n",
            "Removing source directory: /content/LabelChangeTest-1\n",
            "Source directory /content/LabelChangeTest-1 removed successfully.\n",
            "Dataset downloaded to: /content/data/detection\n"
          ]
        }
      ],
      "source": [
        "# current_file_path = Path(__file__)\n",
        "# project_dir = current_file_path.parent.parent\n",
        "\n",
        "# Replace with your actual API key\n",
        "rf = Roboflow(api_key=\"<PUT_YOUR_ROBOFLOW_API_KEY_HERE>\")\n",
        "\n",
        "# Replace with your workspace and project names\n",
        "project = rf.workspace(\"dallmeier\").project(\"labelchangetest\")\n",
        "\n",
        "# Replace with the desired version number\n",
        "version = project.version(1)\n",
        "\n",
        "# Define the directory where you want to download the dataset\n",
        "# download_dir = f\"{root_dir}/data/detection/\" # You can change this to your desired path\n",
        "dataset = version.download(\"coco\")\n",
        "\n",
        "\n",
        "\n",
        "source_dir = f\"{project_dir}/LabelChangeTest-1\"\n",
        "destination_dir = f\"{project_dir}/data/detection\"\n",
        "move_dataset_detection(source_dir, destination_dir, remove=True)\n",
        "\n",
        "print(f\"Dataset downloaded to: {destination_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNVfSDx6Muxb"
      },
      "source": [
        "### model/detection/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "OiEoRnUgMUmx",
        "outputId": "71845346-b186-449a-ad70-c817f7197701"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 174MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 173MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration #147 loss: 0.0485\n",
            "Iteration #148 loss: 0.0291\n",
            "Iteration #149 loss: 0.0237\n",
            "Iteration #150 loss: 0.0447\n",
            "Training Iteration #150 loss: 0.04468454417587357\n",
            "Iteration #151 loss: 0.0643\n",
            "Iteration #152 loss: 0.0627\n",
            "Iteration #153 loss: 0.0351\n",
            "Iteration #154 loss: 0.0551\n",
            "Iteration #155 loss: 0.1440\n",
            "Iteration #156 loss: 0.0407\n",
            "Iteration #157 loss: 0.0336\n",
            "Iteration #158 loss: 0.0540\n",
            "Iteration #159 loss: 0.0349\n",
            "Iteration #160 loss: 0.0389\n",
            "Iteration #161 loss: 0.0393\n",
            "Iteration #162 loss: 0.0246\n",
            "Iteration #163 loss: 0.0377\n",
            "Iteration #164 loss: 0.0349\n",
            "Iteration #165 loss: 0.0312\n",
            "Iteration #166 loss: 0.0922\n",
            "Iteration #167 loss: 0.0239\n",
            "Iteration #168 loss: 0.0781\n",
            "Iteration #169 loss: 0.1003\n",
            "Iteration #170 loss: 0.0418\n",
            "Iteration #171 loss: 0.0479\n",
            "Iteration #172 loss: 0.0496\n",
            "Iteration #173 loss: 0.0187\n",
            "Iteration #174 loss: 0.0866\n",
            "Iteration #175 loss: 0.0169\n",
            "Iteration #176 loss: 0.0788\n",
            "Iteration #177 loss: 0.0481\n",
            "Iteration #178 loss: 0.0456\n",
            "Iteration #179 loss: 0.0519\n",
            "Iteration #180 loss: 0.1133\n",
            "Iteration #181 loss: 0.1303\n",
            "Iteration #182 loss: 0.0464\n",
            "Iteration #183 loss: 0.0587\n",
            "Iteration #184 loss: 0.0346\n",
            "Iteration #185 loss: 0.0406\n",
            "Iteration #186 loss: 0.0332\n",
            "Iteration #187 loss: 0.0326\n",
            "Iteration #188 loss: 0.0404\n",
            "Iteration #189 loss: 0.0358\n",
            "Iteration #190 loss: 0.0523\n",
            "Iteration #191 loss: 0.2003\n",
            "Iteration #192 loss: 0.1118\n",
            "Iteration #193 loss: 0.0640\n",
            "Iteration #194 loss: 0.0709\n",
            "Iteration #195 loss: 0.0592\n",
            "Iteration #196 loss: 0.0305\n",
            "Iteration #197 loss: 0.0898\n",
            "Iteration #198 loss: 0.0366\n",
            "Iteration #199 loss: 0.0324\n",
            "Iteration #200 loss: 0.0230\n",
            "Training Iteration #200 loss: 0.022962575687509253\n",
            "Iteration #201 loss: 0.0459\n",
            "Iteration #202 loss: 0.0285\n",
            "Iteration #203 loss: 0.2305\n",
            "Iteration #204 loss: 0.0508\n",
            "Iteration #205 loss: 0.0354\n",
            "Iteration #206 loss: 0.0309\n",
            "Iteration #207 loss: 0.0950\n",
            "Iteration #208 loss: 0.0386\n",
            "Iteration #209 loss: 0.0487\n",
            "Iteration #210 loss: 0.1740\n",
            "Iteration #211 loss: 0.0337\n",
            "Iteration #212 loss: 0.0890\n",
            "Iteration #213 loss: 0.0596\n",
            "Iteration #214 loss: 0.0630\n",
            "Iteration #215 loss: 0.0558\n",
            "Iteration #216 loss: 0.0739\n",
            "Iteration #217 loss: 0.0292\n",
            "Iteration #218 loss: 0.0438\n",
            "Iteration #219 loss: 0.1192\n",
            "Iteration #220 loss: 0.0326\n",
            "Iteration #221 loss: 0.0634\n",
            "Iteration #222 loss: 0.0290\n",
            "Iteration #223 loss: 0.0336\n",
            "Iteration #224 loss: 0.0287\n",
            "Iteration #225 loss: 0.0800\n",
            "Iteration #226 loss: 0.0932\n",
            "Iteration #227 loss: 0.0299\n",
            "Iteration #228 loss: 0.0424\n",
            "Iteration #229 loss: 0.0501\n",
            "Iteration #230 loss: 0.0332\n",
            "Iteration #231 loss: 0.0331\n",
            "Iteration #232 loss: 0.0272\n",
            "Iteration #233 loss: 0.0395\n",
            "Iteration #234 loss: 0.0343\n",
            "Iteration #235 loss: 0.0500\n",
            "Iteration #236 loss: 0.0338\n",
            "Iteration #237 loss: 0.0858\n",
            "Iteration #238 loss: 0.0592\n",
            "Iteration #239 loss: 0.1827\n",
            "Iteration #240 loss: 0.0330\n",
            "Iteration #241 loss: 0.0573\n",
            "Iteration #242 loss: 0.0359\n",
            "Iteration #243 loss: 0.0337\n",
            "Iteration #244 loss: 0.0405\n",
            "Iteration #245 loss: 0.0309\n",
            "Iteration #246 loss: 0.0270\n",
            "Iteration #247 loss: 0.1725\n",
            "Iteration #248 loss: 0.0375\n",
            "Iteration #249 loss: 0.0465\n",
            "Iteration #250 loss: 0.0365\n",
            "Training Iteration #250 loss: 0.03647153881444138\n",
            "Iteration #251 loss: 0.1628\n",
            "Iteration #252 loss: 0.0281\n",
            "Iteration #253 loss: 0.0684\n",
            "Epoch #3 Training loss: 0.06619045478213408\n",
            "Epoch #3 Training loss: 0.0662\n",
            "Start validation for Epoch #3...\n",
            "Starting validation for Epoch #3...\n",
            "Iteration #1 loss: 0.0902\n",
            "Iteration #2 loss: 0.0411\n",
            "Iteration #3 loss: 0.1122\n",
            "Iteration #4 loss: 0.0460\n",
            "Iteration #5 loss: 0.0571\n",
            "Iteration #6 loss: 0.0499\n",
            "Iteration #7 loss: 0.0650\n",
            "Iteration #8 loss: 0.0550\n",
            "Iteration #9 loss: 0.0507\n",
            "Iteration #10 loss: 0.0794\n",
            "Iteration #11 loss: 0.0457\n",
            "Iteration #12 loss: 0.0698\n",
            "Iteration #13 loss: 0.0803\n",
            "Iteration #14 loss: 0.0826\n",
            "Iteration #15 loss: 0.0952\n",
            "Iteration #16 loss: 0.0479\n",
            "Iteration #17 loss: 0.0694\n",
            "Epoch #3 Validation loss: 0.0669\n",
            "New best validation loss: 0.0669 at epoch #3. Saving model...\n",
            "New best validation loss: 0.0669 at epoch #3. Saving model...\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #4...\n",
            "Starting training for Epoch #4...\n",
            "Iteration #1 loss: 0.0507\n",
            "Iteration #2 loss: 0.0651\n",
            "Iteration #3 loss: 0.0364\n",
            "Iteration #4 loss: 0.0536\n",
            "Iteration #5 loss: 0.1841\n",
            "Iteration #6 loss: 0.0696\n",
            "Iteration #7 loss: 0.0405\n",
            "Iteration #8 loss: 0.0514\n",
            "Iteration #9 loss: 0.0523\n",
            "Iteration #10 loss: 0.0367\n",
            "Iteration #11 loss: 0.0317\n",
            "Iteration #12 loss: 0.1434\n",
            "Iteration #13 loss: 0.0489\n",
            "Iteration #14 loss: 0.0700\n",
            "Iteration #15 loss: 0.0374\n",
            "Iteration #16 loss: 0.0780\n",
            "Iteration #17 loss: 0.0464\n",
            "Iteration #18 loss: 0.0427\n",
            "Iteration #19 loss: 0.0392\n",
            "Iteration #20 loss: 0.2439\n",
            "Iteration #21 loss: 0.0495\n",
            "Iteration #22 loss: 0.1102\n",
            "Iteration #23 loss: 0.0600\n",
            "Iteration #24 loss: 0.0429\n",
            "Iteration #25 loss: 0.0320\n",
            "Iteration #26 loss: 0.0474\n",
            "Iteration #27 loss: 0.0874\n",
            "Iteration #28 loss: 0.0440\n",
            "Iteration #29 loss: 0.0290\n",
            "Iteration #30 loss: 0.0254\n",
            "Iteration #31 loss: 0.0270\n",
            "Iteration #32 loss: 0.0535\n",
            "Iteration #33 loss: 0.0377\n",
            "Iteration #34 loss: 0.0343\n",
            "Iteration #35 loss: 0.0387\n",
            "Iteration #36 loss: 0.0247\n",
            "Iteration #37 loss: 0.0295\n",
            "Iteration #38 loss: 0.0237\n",
            "Iteration #39 loss: 0.1625\n",
            "Iteration #40 loss: 0.0289\n",
            "Iteration #41 loss: 0.0310\n",
            "Iteration #42 loss: 0.0365\n",
            "Iteration #43 loss: 0.0341\n",
            "Iteration #44 loss: 0.0311\n",
            "Iteration #45 loss: 0.0402\n",
            "Iteration #46 loss: 0.0345\n",
            "Iteration #47 loss: 0.0228\n",
            "Iteration #48 loss: 0.0354\n",
            "Iteration #49 loss: 0.0378\n",
            "Iteration #50 loss: 0.1472\n",
            "Training Iteration #50 loss: 0.14715059271620795\n",
            "Iteration #51 loss: 0.0735\n",
            "Iteration #52 loss: 0.0606\n",
            "Iteration #53 loss: 0.0685\n",
            "Iteration #54 loss: 0.0251\n",
            "Iteration #55 loss: 0.0279\n",
            "Iteration #56 loss: 0.0431\n",
            "Iteration #57 loss: 0.0334\n",
            "Iteration #58 loss: 0.0332\n",
            "Iteration #59 loss: 0.0392\n",
            "Iteration #60 loss: 0.0265\n",
            "Iteration #61 loss: 0.0329\n",
            "Iteration #62 loss: 0.0745\n",
            "Iteration #63 loss: 0.0604\n",
            "Iteration #64 loss: 0.0417\n",
            "Iteration #65 loss: 0.0332\n",
            "Iteration #66 loss: 0.0707\n",
            "Iteration #67 loss: 0.0495\n",
            "Iteration #68 loss: 0.0398\n",
            "Iteration #69 loss: 0.0417\n",
            "Iteration #70 loss: 0.0810\n",
            "Iteration #71 loss: 0.0396\n",
            "Iteration #72 loss: 0.0221\n",
            "Iteration #73 loss: 0.0254\n",
            "Iteration #74 loss: 0.1999\n",
            "Iteration #75 loss: 0.0397\n",
            "Iteration #76 loss: 0.0944\n",
            "Iteration #77 loss: 0.0296\n",
            "Iteration #78 loss: 0.0486\n",
            "Iteration #79 loss: 0.0554\n",
            "Iteration #80 loss: 0.2578\n",
            "Iteration #81 loss: 0.0525\n",
            "Iteration #82 loss: 0.0405\n",
            "Iteration #83 loss: 0.1235\n",
            "Iteration #84 loss: 0.0422\n",
            "Iteration #85 loss: 0.0495\n",
            "Iteration #86 loss: 0.1276\n",
            "Iteration #87 loss: 0.0362\n",
            "Iteration #88 loss: 0.0350\n",
            "Iteration #89 loss: 0.0825\n",
            "Iteration #90 loss: 0.0599\n",
            "Iteration #91 loss: 0.0384\n",
            "Iteration #92 loss: 0.0495\n",
            "Iteration #93 loss: 0.0351\n",
            "Iteration #94 loss: 0.0322\n",
            "Iteration #95 loss: 0.0292\n",
            "Iteration #96 loss: 0.1065\n",
            "Iteration #97 loss: 0.0921\n",
            "Iteration #98 loss: 0.1041\n",
            "Iteration #99 loss: 0.0327\n",
            "Iteration #100 loss: 0.0225\n",
            "Training Iteration #100 loss: 0.02245588855250594\n",
            "Iteration #101 loss: 0.0347\n",
            "Iteration #102 loss: 0.0302\n",
            "Iteration #103 loss: 0.0209\n",
            "Iteration #104 loss: 0.0339\n",
            "Iteration #105 loss: 0.0467\n",
            "Iteration #106 loss: 0.0442\n",
            "Iteration #107 loss: 0.0238\n",
            "Iteration #108 loss: 0.0229\n",
            "Iteration #109 loss: 0.0272\n",
            "Iteration #110 loss: 0.0207\n",
            "Iteration #111 loss: 0.0274\n",
            "Iteration #112 loss: 0.0374\n",
            "Iteration #113 loss: 0.0463\n",
            "Iteration #114 loss: 0.0320\n",
            "Iteration #115 loss: 0.0386\n",
            "Iteration #116 loss: 0.0380\n",
            "Iteration #117 loss: 0.0581\n",
            "Iteration #118 loss: 0.0316\n",
            "Iteration #119 loss: 0.0422\n",
            "Iteration #120 loss: 0.2029\n",
            "Iteration #121 loss: 0.0272\n",
            "Iteration #122 loss: 0.0337\n",
            "Iteration #123 loss: 0.0352\n",
            "Iteration #124 loss: 0.0281\n",
            "Iteration #125 loss: 0.0391\n",
            "Iteration #126 loss: 0.0447\n",
            "Iteration #127 loss: 0.0356\n",
            "Iteration #128 loss: 0.1206\n",
            "Iteration #129 loss: 0.0273\n",
            "Iteration #130 loss: 0.1591\n",
            "Iteration #131 loss: 0.0408\n",
            "Iteration #132 loss: 0.0370\n",
            "Iteration #133 loss: 0.0357\n",
            "Iteration #134 loss: 0.0512\n",
            "Iteration #135 loss: 0.0261\n",
            "Iteration #136 loss: 0.0339\n",
            "Iteration #137 loss: 0.0818\n",
            "Iteration #138 loss: 0.0299\n",
            "Iteration #139 loss: 0.0389\n",
            "Iteration #140 loss: 0.0862\n",
            "Iteration #141 loss: 0.0779\n",
            "Iteration #142 loss: 0.1805\n",
            "Iteration #143 loss: 0.0289\n",
            "Iteration #144 loss: 0.0789\n",
            "Iteration #145 loss: 0.0824\n",
            "Iteration #146 loss: 0.0634\n",
            "Iteration #147 loss: 0.0608\n",
            "Iteration #148 loss: 0.1275\n",
            "Iteration #149 loss: 0.0290\n",
            "Iteration #150 loss: 0.0427\n",
            "Training Iteration #150 loss: 0.042677865384568435\n",
            "Iteration #151 loss: 0.0366\n",
            "Iteration #152 loss: 0.0787\n",
            "Iteration #153 loss: 0.0249\n",
            "Iteration #154 loss: 0.0347\n",
            "Iteration #155 loss: 0.0561\n",
            "Iteration #156 loss: 0.0772\n",
            "Iteration #157 loss: 0.0403\n",
            "Iteration #158 loss: 0.0811\n",
            "Iteration #159 loss: 0.0399\n",
            "Iteration #160 loss: 0.0296\n",
            "Iteration #161 loss: 0.0378\n",
            "Iteration #162 loss: 0.0331\n",
            "Iteration #163 loss: 0.0461\n",
            "Iteration #164 loss: 0.0370\n",
            "Iteration #165 loss: 0.0332\n",
            "Iteration #166 loss: 0.0345\n",
            "Iteration #167 loss: 0.0353\n",
            "Iteration #168 loss: 0.0322\n",
            "Iteration #169 loss: 0.1133\n",
            "Iteration #170 loss: 0.0436\n",
            "Iteration #171 loss: 0.0288\n",
            "Iteration #172 loss: 0.0498\n",
            "Iteration #173 loss: 0.0971\n",
            "Iteration #174 loss: 0.0338\n",
            "Iteration #175 loss: 0.0535\n",
            "Iteration #176 loss: 0.0400\n",
            "Iteration #177 loss: 0.0409\n",
            "Iteration #178 loss: 0.0446\n",
            "Iteration #179 loss: 0.0238\n",
            "Iteration #180 loss: 0.0287\n",
            "Iteration #181 loss: 0.0346\n",
            "Iteration #182 loss: 0.0299\n",
            "Iteration #183 loss: 0.0449\n",
            "Iteration #184 loss: 0.3182\n",
            "Iteration #185 loss: 0.0564\n",
            "Iteration #186 loss: 0.0483\n",
            "Iteration #187 loss: 0.1428\n",
            "Iteration #188 loss: 0.0369\n",
            "Iteration #189 loss: 0.1825\n",
            "Iteration #190 loss: 0.0419\n",
            "Iteration #191 loss: 0.0362\n",
            "Iteration #192 loss: 0.0443\n",
            "Iteration #193 loss: 0.0314\n",
            "Iteration #194 loss: 0.0469\n",
            "Iteration #195 loss: 0.0346\n",
            "Iteration #196 loss: 0.0938\n",
            "Iteration #197 loss: 0.0523\n",
            "Iteration #198 loss: 0.0359\n",
            "Iteration #199 loss: 0.0330\n",
            "Iteration #200 loss: 0.0309\n",
            "Training Iteration #200 loss: 0.03090372500199569\n",
            "Iteration #201 loss: 0.0306\n",
            "Iteration #202 loss: 0.0628\n",
            "Iteration #203 loss: 0.0543\n",
            "Iteration #204 loss: 0.0295\n",
            "Iteration #205 loss: 0.0337\n",
            "Iteration #206 loss: 0.0917\n",
            "Iteration #207 loss: 0.1051\n",
            "Iteration #208 loss: 0.0432\n",
            "Iteration #209 loss: 0.0358\n",
            "Iteration #210 loss: 0.1166\n",
            "Iteration #211 loss: 0.0427\n",
            "Iteration #212 loss: 0.0421\n",
            "Iteration #213 loss: 0.0200\n",
            "Iteration #214 loss: 0.1310\n",
            "Iteration #215 loss: 0.0606\n",
            "Iteration #216 loss: 0.0456\n",
            "Iteration #217 loss: 0.0360\n",
            "Iteration #218 loss: 0.0374\n",
            "Iteration #219 loss: 0.0304\n",
            "Iteration #220 loss: 0.0423\n",
            "Iteration #221 loss: 0.0381\n",
            "Iteration #222 loss: 0.0469\n",
            "Iteration #223 loss: 0.1483\n",
            "Iteration #224 loss: 0.0707\n",
            "Iteration #225 loss: 0.0283\n",
            "Iteration #226 loss: 0.0466\n",
            "Iteration #227 loss: 0.0456\n",
            "Iteration #228 loss: 0.0954\n",
            "Iteration #229 loss: 0.0404\n",
            "Iteration #230 loss: 0.1315\n",
            "Iteration #231 loss: 0.0816\n",
            "Iteration #232 loss: 0.0217\n",
            "Iteration #233 loss: 0.0419\n",
            "Iteration #234 loss: 0.0513\n",
            "Iteration #235 loss: 0.1740\n",
            "Iteration #236 loss: 0.1387\n",
            "Iteration #237 loss: 0.0701\n",
            "Iteration #238 loss: 0.0333\n",
            "Iteration #239 loss: 0.0692\n",
            "Iteration #240 loss: 0.0419\n",
            "Iteration #241 loss: 0.0277\n",
            "Iteration #242 loss: 0.0508\n",
            "Iteration #243 loss: 0.0398\n",
            "Iteration #244 loss: 0.1549\n",
            "Iteration #245 loss: 0.0432\n",
            "Iteration #246 loss: 0.0518\n",
            "Iteration #247 loss: 0.0740\n",
            "Iteration #248 loss: 0.0423\n",
            "Iteration #249 loss: 0.0512\n",
            "Iteration #250 loss: 0.0332\n",
            "Training Iteration #250 loss: 0.03317770734736074\n",
            "Iteration #251 loss: 0.0761\n",
            "Iteration #252 loss: 0.0435\n",
            "Iteration #253 loss: 0.0275\n",
            "Epoch #4 Training loss: 0.05711464383186869\n",
            "Epoch #4 Training loss: 0.0571\n",
            "Start validation for Epoch #4...\n",
            "Starting validation for Epoch #4...\n",
            "Iteration #1 loss: 0.0781\n",
            "Iteration #2 loss: 0.0433\n",
            "Iteration #3 loss: 0.0872\n",
            "Iteration #4 loss: 0.0409\n",
            "Iteration #5 loss: 0.0552\n",
            "Iteration #6 loss: 0.0453\n",
            "Iteration #7 loss: 0.0550\n",
            "Iteration #8 loss: 0.0438\n",
            "Iteration #9 loss: 0.0526\n",
            "Iteration #10 loss: 0.0820\n",
            "Iteration #11 loss: 0.0414\n",
            "Iteration #12 loss: 0.0703\n",
            "Iteration #13 loss: 0.0722\n",
            "Iteration #14 loss: 0.0709\n",
            "Iteration #15 loss: 0.0971\n",
            "Iteration #16 loss: 0.0454\n",
            "Iteration #17 loss: 0.0627\n",
            "Epoch #4 Validation loss: 0.0614\n",
            "New best validation loss: 0.0614 at epoch #4. Saving model...\n",
            "New best validation loss: 0.0614 at epoch #4. Saving model...\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #5...\n",
            "Starting training for Epoch #5...\n",
            "Iteration #1 loss: 0.0812\n",
            "Iteration #2 loss: 0.0428\n",
            "Iteration #3 loss: 0.0619\n",
            "Iteration #4 loss: 0.0298\n",
            "Iteration #5 loss: 0.0224\n",
            "Iteration #6 loss: 0.0221\n",
            "Iteration #7 loss: 0.0571\n",
            "Iteration #8 loss: 0.0355\n",
            "Iteration #9 loss: 0.0456\n",
            "Iteration #10 loss: 0.0354\n",
            "Iteration #11 loss: 0.1109\n",
            "Iteration #12 loss: 0.0458\n",
            "Iteration #13 loss: 0.1402\n",
            "Iteration #14 loss: 0.0358\n",
            "Iteration #15 loss: 0.0211\n",
            "Iteration #16 loss: 0.0256\n",
            "Iteration #17 loss: 0.0328\n",
            "Iteration #18 loss: 0.0182\n",
            "Iteration #19 loss: 0.1626\n",
            "Iteration #20 loss: 0.0290\n",
            "Iteration #21 loss: 0.0541\n",
            "Iteration #22 loss: 0.0270\n",
            "Iteration #23 loss: 0.0222\n",
            "Iteration #24 loss: 0.0711\n",
            "Iteration #25 loss: 0.0233\n",
            "Iteration #26 loss: 0.0419\n",
            "Iteration #27 loss: 0.1850\n",
            "Iteration #28 loss: 0.0817\n",
            "Iteration #29 loss: 0.0486\n",
            "Iteration #30 loss: 0.0287\n",
            "Iteration #31 loss: 0.0793\n",
            "Iteration #32 loss: 0.0727\n",
            "Iteration #33 loss: 0.0381\n",
            "Iteration #34 loss: 0.0223\n",
            "Iteration #35 loss: 0.1505\n",
            "Iteration #36 loss: 0.0978\n",
            "Iteration #37 loss: 0.0296\n",
            "Iteration #38 loss: 0.0537\n",
            "Iteration #39 loss: 0.0302\n",
            "Iteration #40 loss: 0.0385\n",
            "Iteration #41 loss: 0.0438\n",
            "Iteration #42 loss: 0.0339\n",
            "Iteration #43 loss: 0.0436\n",
            "Iteration #44 loss: 0.0376\n",
            "Iteration #45 loss: 0.0559\n",
            "Iteration #46 loss: 0.0669\n",
            "Iteration #47 loss: 0.0290\n",
            "Iteration #48 loss: 0.0815\n",
            "Iteration #49 loss: 0.0525\n",
            "Iteration #50 loss: 0.0322\n",
            "Training Iteration #50 loss: 0.032163283311988915\n",
            "Iteration #51 loss: 0.0274\n",
            "Iteration #52 loss: 0.0331\n",
            "Iteration #53 loss: 0.0333\n",
            "Iteration #54 loss: 0.0277\n",
            "Iteration #55 loss: 0.1155\n",
            "Iteration #56 loss: 0.0417\n",
            "Iteration #57 loss: 0.0456\n",
            "Iteration #58 loss: 0.0716\n",
            "Iteration #59 loss: 0.0435\n",
            "Iteration #60 loss: 0.0442\n",
            "Iteration #61 loss: 0.0485\n",
            "Iteration #62 loss: 0.0767\n",
            "Iteration #63 loss: 0.0438\n",
            "Iteration #64 loss: 0.0359\n",
            "Iteration #65 loss: 0.0341\n",
            "Iteration #66 loss: 0.1248\n",
            "Iteration #67 loss: 0.0544\n",
            "Iteration #68 loss: 0.0550\n",
            "Iteration #69 loss: 0.0683\n",
            "Iteration #70 loss: 0.0326\n",
            "Iteration #71 loss: 0.0287\n",
            "Iteration #72 loss: 0.0262\n",
            "Iteration #73 loss: 0.0318\n",
            "Iteration #74 loss: 0.0288\n",
            "Iteration #75 loss: 0.0504\n",
            "Iteration #76 loss: 0.0256\n",
            "Iteration #77 loss: 0.1373\n",
            "Iteration #78 loss: 0.0322\n",
            "Iteration #79 loss: 0.0804\n",
            "Iteration #80 loss: 0.0301\n",
            "Iteration #81 loss: 0.0282\n",
            "Iteration #82 loss: 0.0520\n",
            "Iteration #83 loss: 0.0435\n",
            "Iteration #84 loss: 0.1004\n",
            "Iteration #85 loss: 0.0289\n",
            "Iteration #86 loss: 0.0285\n",
            "Iteration #87 loss: 0.0279\n",
            "Iteration #88 loss: 0.0701\n",
            "Iteration #89 loss: 0.0990\n",
            "Iteration #90 loss: 0.0604\n",
            "Iteration #91 loss: 0.0398\n",
            "Iteration #92 loss: 0.0691\n",
            "Iteration #93 loss: 0.0520\n",
            "Iteration #94 loss: 0.0439\n",
            "Iteration #95 loss: 0.0559\n",
            "Iteration #96 loss: 0.0983\n",
            "Iteration #97 loss: 0.0396\n",
            "Iteration #98 loss: 0.0415\n",
            "Iteration #99 loss: 0.0444\n",
            "Iteration #100 loss: 0.0360\n",
            "Training Iteration #100 loss: 0.03595788744185537\n",
            "Iteration #101 loss: 0.0786\n",
            "Iteration #102 loss: 0.0329\n",
            "Iteration #103 loss: 0.0339\n",
            "Iteration #104 loss: 0.0555\n",
            "Iteration #105 loss: 0.0703\n",
            "Iteration #106 loss: 0.0432\n",
            "Iteration #107 loss: 0.0510\n",
            "Iteration #108 loss: 0.0515\n",
            "Iteration #109 loss: 0.0241\n",
            "Iteration #110 loss: 0.0651\n",
            "Iteration #111 loss: 0.0337\n",
            "Iteration #112 loss: 0.0640\n",
            "Iteration #113 loss: 0.0265\n",
            "Iteration #114 loss: 0.0257\n",
            "Iteration #115 loss: 0.0194\n",
            "Iteration #116 loss: 0.1106\n",
            "Iteration #117 loss: 0.0226\n",
            "Iteration #118 loss: 0.0306\n",
            "Iteration #119 loss: 0.0270\n",
            "Iteration #120 loss: 0.0394\n",
            "Iteration #121 loss: 0.0492\n",
            "Iteration #122 loss: 0.0356\n",
            "Iteration #123 loss: 0.0318\n",
            "Iteration #124 loss: 0.0527\n",
            "Iteration #125 loss: 0.0308\n",
            "Iteration #126 loss: 0.0554\n",
            "Iteration #127 loss: 0.0538\n",
            "Iteration #128 loss: 0.0378\n",
            "Iteration #129 loss: 0.0418\n",
            "Iteration #130 loss: 0.0359\n",
            "Iteration #131 loss: 0.1242\n",
            "Iteration #132 loss: 0.0321\n",
            "Iteration #133 loss: 0.0263\n",
            "Iteration #134 loss: 0.0455\n",
            "Iteration #135 loss: 0.0537\n",
            "Iteration #136 loss: 0.2331\n",
            "Iteration #137 loss: 0.0719\n",
            "Iteration #138 loss: 0.1076\n",
            "Iteration #139 loss: 0.0555\n",
            "Iteration #140 loss: 0.1249\n",
            "Iteration #141 loss: 0.0236\n",
            "Iteration #142 loss: 0.0493\n",
            "Iteration #143 loss: 0.0400\n",
            "Iteration #144 loss: 0.0341\n",
            "Iteration #145 loss: 0.0497\n",
            "Iteration #146 loss: 0.0281\n",
            "Iteration #147 loss: 0.0913\n",
            "Iteration #148 loss: 0.0332\n",
            "Iteration #149 loss: 0.0392\n",
            "Iteration #150 loss: 0.0510\n",
            "Training Iteration #150 loss: 0.05101521836449496\n",
            "Iteration #151 loss: 0.0422\n",
            "Iteration #152 loss: 0.0390\n",
            "Iteration #153 loss: 0.0578\n",
            "Iteration #154 loss: 0.0334\n",
            "Iteration #155 loss: 0.0532\n",
            "Iteration #156 loss: 0.0342\n",
            "Iteration #157 loss: 0.0262\n",
            "Iteration #158 loss: 0.0391\n",
            "Iteration #159 loss: 0.0297\n",
            "Iteration #160 loss: 0.0481\n",
            "Iteration #161 loss: 0.0613\n",
            "Iteration #162 loss: 0.0437\n",
            "Iteration #163 loss: 0.0307\n",
            "Iteration #164 loss: 0.0188\n",
            "Iteration #165 loss: 0.0752\n",
            "Iteration #166 loss: 0.0250\n",
            "Iteration #167 loss: 0.0398\n",
            "Iteration #168 loss: 0.0206\n",
            "Iteration #169 loss: 0.0219\n",
            "Iteration #170 loss: 0.0396\n",
            "Iteration #171 loss: 0.0267\n",
            "Iteration #172 loss: 0.0557\n",
            "Iteration #173 loss: 0.0400\n",
            "Iteration #174 loss: 0.0255\n",
            "Iteration #175 loss: 0.0209\n",
            "Iteration #176 loss: 0.0331\n",
            "Iteration #177 loss: 0.0236\n",
            "Iteration #178 loss: 0.0260\n",
            "Iteration #179 loss: 0.0220\n",
            "Iteration #180 loss: 0.1155\n",
            "Iteration #181 loss: 0.0318\n",
            "Iteration #182 loss: 0.0240\n",
            "Iteration #183 loss: 0.0211\n",
            "Iteration #184 loss: 0.0359\n",
            "Iteration #185 loss: 0.0287\n",
            "Iteration #186 loss: 0.0264\n",
            "Iteration #187 loss: 0.1279\n",
            "Iteration #188 loss: 0.0340\n",
            "Iteration #189 loss: 0.0348\n",
            "Iteration #190 loss: 0.0450\n",
            "Iteration #191 loss: 0.0301\n",
            "Iteration #192 loss: 0.0199\n",
            "Iteration #193 loss: 0.0958\n",
            "Iteration #194 loss: 0.0313\n",
            "Iteration #195 loss: 0.0288\n",
            "Iteration #196 loss: 0.0403\n",
            "Iteration #197 loss: 0.0325\n",
            "Iteration #198 loss: 0.0216\n",
            "Iteration #199 loss: 0.0592\n",
            "Iteration #200 loss: 0.0198\n",
            "Training Iteration #200 loss: 0.01983309349642034\n",
            "Iteration #201 loss: 0.0967\n",
            "Iteration #202 loss: 0.0298\n",
            "Iteration #203 loss: 0.0623\n",
            "Iteration #204 loss: 0.0344\n",
            "Iteration #205 loss: 0.0293\n",
            "Iteration #206 loss: 0.0223\n",
            "Iteration #207 loss: 0.0490\n",
            "Iteration #208 loss: 0.0295\n",
            "Iteration #209 loss: 0.0295\n",
            "Iteration #210 loss: 0.1185\n",
            "Iteration #211 loss: 0.0209\n",
            "Iteration #212 loss: 0.0952\n",
            "Iteration #213 loss: 0.2126\n",
            "Iteration #214 loss: 0.0381\n",
            "Iteration #215 loss: 0.0307\n",
            "Iteration #216 loss: 0.0364\n",
            "Iteration #217 loss: 0.0204\n",
            "Iteration #218 loss: 0.2256\n",
            "Iteration #219 loss: 0.0391\n",
            "Iteration #220 loss: 0.0361\n",
            "Iteration #221 loss: 0.0354\n",
            "Iteration #222 loss: 0.0558\n",
            "Iteration #223 loss: 0.0326\n",
            "Iteration #224 loss: 0.0979\n",
            "Iteration #225 loss: 0.0270\n",
            "Iteration #226 loss: 0.0380\n",
            "Iteration #227 loss: 0.0794\n",
            "Iteration #228 loss: 0.0198\n",
            "Iteration #229 loss: 0.0236\n",
            "Iteration #230 loss: 0.0318\n",
            "Iteration #231 loss: 0.0415\n",
            "Iteration #232 loss: 0.0588\n",
            "Iteration #233 loss: 0.0382\n",
            "Iteration #234 loss: 0.0229\n",
            "Iteration #235 loss: 0.0792\n",
            "Iteration #236 loss: 0.0228\n",
            "Iteration #237 loss: 0.0302\n",
            "Iteration #238 loss: 0.0394\n",
            "Iteration #239 loss: 0.0870\n",
            "Iteration #240 loss: 0.1660\n",
            "Iteration #241 loss: 0.0460\n",
            "Iteration #242 loss: 0.0529\n",
            "Iteration #243 loss: 0.0203\n",
            "Iteration #244 loss: 0.0443\n",
            "Iteration #245 loss: 0.0328\n",
            "Iteration #246 loss: 0.0343\n",
            "Iteration #247 loss: 0.0269\n",
            "Iteration #248 loss: 0.0338\n",
            "Iteration #249 loss: 0.1075\n",
            "Iteration #250 loss: 0.0433\n",
            "Training Iteration #250 loss: 0.04329371417353544\n",
            "Iteration #251 loss: 0.0793\n",
            "Iteration #252 loss: 0.0674\n",
            "Iteration #253 loss: 0.0297\n",
            "Epoch #5 Training loss: 0.05067725454743176\n",
            "Epoch #5 Training loss: 0.0507\n",
            "Start validation for Epoch #5...\n",
            "Starting validation for Epoch #5...\n",
            "Iteration #1 loss: 0.0806\n",
            "Iteration #2 loss: 0.0379\n",
            "Iteration #3 loss: 0.1097\n",
            "Iteration #4 loss: 0.0453\n",
            "Iteration #5 loss: 0.0539\n",
            "Iteration #6 loss: 0.0433\n",
            "Iteration #7 loss: 0.0633\n",
            "Iteration #8 loss: 0.0449\n",
            "Iteration #9 loss: 0.0540\n",
            "Iteration #10 loss: 0.0804\n",
            "Iteration #11 loss: 0.0388\n",
            "Iteration #12 loss: 0.0660\n",
            "Iteration #13 loss: 0.0798\n",
            "Iteration #14 loss: 0.0763\n",
            "Iteration #15 loss: 0.0960\n",
            "Iteration #16 loss: 0.0502\n",
            "Iteration #17 loss: 0.0611\n",
            "Epoch #5 Validation loss: 0.0636\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #6...\n",
            "Starting training for Epoch #6...\n",
            "Iteration #1 loss: 0.1535\n",
            "Iteration #2 loss: 0.0295\n",
            "Iteration #3 loss: 0.0945\n",
            "Iteration #4 loss: 0.0281\n",
            "Iteration #5 loss: 0.1623\n",
            "Iteration #6 loss: 0.0297\n",
            "Iteration #7 loss: 0.0393\n",
            "Iteration #8 loss: 0.0389\n",
            "Iteration #9 loss: 0.0389\n",
            "Iteration #10 loss: 0.0991\n",
            "Iteration #11 loss: 0.0336\n",
            "Iteration #12 loss: 0.0343\n",
            "Iteration #13 loss: 0.0713\n",
            "Iteration #14 loss: 0.0226\n",
            "Iteration #15 loss: 0.0297\n",
            "Iteration #16 loss: 0.0339\n",
            "Iteration #17 loss: 0.0458\n",
            "Iteration #18 loss: 0.0402\n",
            "Iteration #19 loss: 0.0599\n",
            "Iteration #20 loss: 0.0322\n",
            "Iteration #21 loss: 0.0197\n",
            "Iteration #22 loss: 0.0387\n",
            "Iteration #23 loss: 0.0486\n",
            "Iteration #24 loss: 0.0281\n",
            "Iteration #25 loss: 0.0383\n",
            "Iteration #26 loss: 0.0371\n",
            "Iteration #27 loss: 0.0458\n",
            "Iteration #28 loss: 0.0519\n",
            "Iteration #29 loss: 0.0507\n",
            "Iteration #30 loss: 0.0312\n",
            "Iteration #31 loss: 0.0367\n",
            "Iteration #32 loss: 0.0364\n",
            "Iteration #33 loss: 0.0300\n",
            "Iteration #34 loss: 0.0549\n",
            "Iteration #35 loss: 0.0280\n",
            "Iteration #36 loss: 0.0252\n",
            "Iteration #37 loss: 0.0333\n",
            "Iteration #38 loss: 0.0235\n",
            "Iteration #39 loss: 0.0228\n",
            "Iteration #40 loss: 0.0262\n",
            "Iteration #41 loss: 0.0343\n",
            "Iteration #42 loss: 0.0369\n",
            "Iteration #43 loss: 0.1111\n",
            "Iteration #44 loss: 0.0688\n",
            "Iteration #45 loss: 0.0646\n",
            "Iteration #46 loss: 0.0349\n",
            "Iteration #47 loss: 0.0319\n",
            "Iteration #48 loss: 0.0675\n",
            "Iteration #49 loss: 0.1120\n",
            "Iteration #50 loss: 0.0558\n",
            "Training Iteration #50 loss: 0.0558097488293334\n",
            "Iteration #51 loss: 0.0422\n",
            "Iteration #52 loss: 0.0448\n",
            "Iteration #53 loss: 0.0380\n",
            "Iteration #54 loss: 0.0408\n",
            "Iteration #55 loss: 0.0978\n",
            "Iteration #56 loss: 0.0208\n",
            "Iteration #57 loss: 0.0189\n",
            "Iteration #58 loss: 0.1062\n",
            "Iteration #59 loss: 0.0397\n",
            "Iteration #60 loss: 0.0437\n",
            "Iteration #61 loss: 0.0316\n",
            "Iteration #62 loss: 0.0547\n",
            "Iteration #63 loss: 0.0207\n",
            "Iteration #64 loss: 0.0569\n",
            "Iteration #65 loss: 0.0231\n",
            "Iteration #66 loss: 0.0314\n",
            "Iteration #67 loss: 0.0220\n",
            "Iteration #68 loss: 0.0472\n",
            "Iteration #69 loss: 0.0366\n",
            "Iteration #70 loss: 0.0541\n",
            "Iteration #71 loss: 0.0240\n",
            "Iteration #72 loss: 0.0355\n",
            "Iteration #73 loss: 0.0330\n",
            "Iteration #74 loss: 0.0285\n",
            "Iteration #75 loss: 0.0427\n",
            "Iteration #76 loss: 0.0408\n",
            "Iteration #77 loss: 0.0612\n",
            "Iteration #78 loss: 0.0314\n",
            "Iteration #79 loss: 0.0283\n",
            "Iteration #80 loss: 0.0383\n",
            "Iteration #81 loss: 0.0536\n",
            "Iteration #82 loss: 0.0262\n",
            "Iteration #83 loss: 0.0658\n",
            "Iteration #84 loss: 0.0327\n",
            "Iteration #85 loss: 0.1434\n",
            "Iteration #86 loss: 0.0237\n",
            "Iteration #87 loss: 0.0296\n",
            "Iteration #88 loss: 0.0421\n",
            "Iteration #89 loss: 0.1223\n",
            "Iteration #90 loss: 0.0176\n",
            "Iteration #91 loss: 0.0335\n",
            "Iteration #92 loss: 0.0254\n",
            "Iteration #93 loss: 0.0551\n",
            "Iteration #94 loss: 0.0764\n",
            "Iteration #95 loss: 0.0366\n",
            "Iteration #96 loss: 0.0251\n",
            "Iteration #97 loss: 0.0772\n",
            "Iteration #98 loss: 0.0232\n",
            "Iteration #99 loss: 0.1994\n",
            "Iteration #100 loss: 0.0311\n",
            "Training Iteration #100 loss: 0.031109203482307973\n",
            "Iteration #101 loss: 0.0372\n",
            "Iteration #102 loss: 0.1109\n",
            "Iteration #103 loss: 0.0483\n",
            "Iteration #104 loss: 0.0315\n",
            "Iteration #105 loss: 0.1101\n",
            "Iteration #106 loss: 0.1531\n",
            "Iteration #107 loss: 0.0493\n",
            "Iteration #108 loss: 0.0359\n",
            "Iteration #109 loss: 0.0475\n",
            "Iteration #110 loss: 0.0359\n",
            "Iteration #111 loss: 0.0400\n",
            "Iteration #112 loss: 0.0264\n",
            "Iteration #113 loss: 0.0230\n",
            "Iteration #114 loss: 0.0597\n",
            "Iteration #115 loss: 0.0346\n",
            "Iteration #116 loss: 0.0299\n",
            "Iteration #117 loss: 0.1567\n",
            "Iteration #118 loss: 0.0635\n",
            "Iteration #119 loss: 0.1229\n",
            "Iteration #120 loss: 0.0299\n",
            "Iteration #121 loss: 0.0616\n",
            "Iteration #122 loss: 0.0275\n",
            "Iteration #123 loss: 0.0380\n",
            "Iteration #124 loss: 0.0371\n",
            "Iteration #125 loss: 0.0257\n",
            "Iteration #126 loss: 0.0345\n",
            "Iteration #127 loss: 0.0598\n",
            "Iteration #128 loss: 0.0461\n",
            "Iteration #129 loss: 0.0304\n",
            "Iteration #130 loss: 0.0550\n",
            "Iteration #131 loss: 0.0331\n",
            "Iteration #132 loss: 0.0328\n",
            "Iteration #133 loss: 0.0322\n",
            "Iteration #134 loss: 0.0801\n",
            "Iteration #135 loss: 0.0493\n",
            "Iteration #136 loss: 0.0314\n",
            "Iteration #137 loss: 0.0266\n",
            "Iteration #138 loss: 0.0246\n",
            "Iteration #139 loss: 0.0451\n",
            "Iteration #140 loss: 0.0328\n",
            "Iteration #141 loss: 0.0252\n",
            "Iteration #142 loss: 0.0363\n",
            "Iteration #143 loss: 0.0568\n",
            "Iteration #144 loss: 0.0174\n",
            "Iteration #145 loss: 0.0987\n",
            "Iteration #146 loss: 0.0952\n",
            "Iteration #147 loss: 0.0660\n",
            "Iteration #148 loss: 0.0482\n",
            "Iteration #149 loss: 0.0275\n",
            "Iteration #150 loss: 0.0486\n",
            "Training Iteration #150 loss: 0.04859866962617171\n",
            "Iteration #151 loss: 0.0234\n",
            "Iteration #152 loss: 0.0346\n",
            "Iteration #153 loss: 0.0289\n",
            "Iteration #154 loss: 0.0285\n",
            "Iteration #155 loss: 0.0227\n",
            "Iteration #156 loss: 0.0184\n",
            "Iteration #157 loss: 0.0198\n",
            "Iteration #158 loss: 0.0166\n",
            "Iteration #159 loss: 0.0243\n",
            "Iteration #160 loss: 0.0272\n",
            "Iteration #161 loss: 0.0257\n",
            "Iteration #162 loss: 0.2349\n",
            "Iteration #163 loss: 0.0276\n",
            "Iteration #164 loss: 0.0318\n",
            "Iteration #165 loss: 0.0279\n",
            "Iteration #166 loss: 0.0333\n",
            "Iteration #167 loss: 0.0422\n",
            "Iteration #168 loss: 0.0287\n",
            "Iteration #169 loss: 0.0781\n",
            "Iteration #170 loss: 0.0402\n",
            "Iteration #171 loss: 0.0701\n",
            "Iteration #172 loss: 0.0236\n",
            "Iteration #173 loss: 0.0301\n",
            "Iteration #174 loss: 0.0355\n",
            "Iteration #175 loss: 0.1840\n",
            "Iteration #176 loss: 0.0313\n",
            "Iteration #177 loss: 0.0253\n",
            "Iteration #178 loss: 0.0778\n",
            "Iteration #179 loss: 0.0386\n",
            "Iteration #180 loss: 0.0504\n",
            "Iteration #181 loss: 0.0253\n",
            "Iteration #182 loss: 0.0307\n",
            "Iteration #183 loss: 0.0203\n",
            "Iteration #184 loss: 0.0240\n",
            "Iteration #185 loss: 0.0368\n",
            "Iteration #186 loss: 0.0419\n",
            "Iteration #187 loss: 0.0225\n",
            "Iteration #188 loss: 0.0486\n",
            "Iteration #189 loss: 0.0314\n",
            "Iteration #190 loss: 0.0153\n",
            "Iteration #191 loss: 0.0233\n",
            "Iteration #192 loss: 0.0692\n",
            "Iteration #193 loss: 0.0735\n",
            "Iteration #194 loss: 0.0366\n",
            "Iteration #195 loss: 0.0445\n",
            "Iteration #196 loss: 0.0244\n",
            "Iteration #197 loss: 0.0160\n",
            "Iteration #198 loss: 0.0411\n",
            "Iteration #199 loss: 0.0288\n",
            "Iteration #200 loss: 0.0172\n",
            "Training Iteration #200 loss: 0.017203152335169294\n",
            "Iteration #201 loss: 0.0574\n",
            "Iteration #202 loss: 0.0419\n",
            "Iteration #203 loss: 0.0199\n",
            "Iteration #204 loss: 0.0151\n",
            "Iteration #205 loss: 0.0386\n",
            "Iteration #206 loss: 0.0276\n",
            "Iteration #207 loss: 0.0265\n",
            "Iteration #208 loss: 0.0430\n",
            "Iteration #209 loss: 0.0468\n",
            "Iteration #210 loss: 0.0461\n",
            "Iteration #211 loss: 0.0874\n",
            "Iteration #212 loss: 0.0287\n",
            "Iteration #213 loss: 0.0464\n",
            "Iteration #214 loss: 0.1583\n",
            "Iteration #215 loss: 0.0364\n",
            "Iteration #216 loss: 0.0319\n",
            "Iteration #217 loss: 0.0416\n",
            "Iteration #218 loss: 0.0384\n",
            "Iteration #219 loss: 0.1738\n",
            "Iteration #220 loss: 0.0241\n",
            "Iteration #221 loss: 0.0290\n",
            "Iteration #222 loss: 0.1292\n",
            "Iteration #223 loss: 0.0299\n",
            "Iteration #224 loss: 0.0329\n",
            "Iteration #225 loss: 0.0369\n",
            "Iteration #226 loss: 0.0310\n",
            "Iteration #227 loss: 0.0221\n",
            "Iteration #228 loss: 0.0829\n",
            "Iteration #229 loss: 0.0766\n",
            "Iteration #230 loss: 0.0265\n",
            "Iteration #231 loss: 0.0256\n",
            "Iteration #232 loss: 0.0423\n",
            "Iteration #233 loss: 0.0359\n",
            "Iteration #234 loss: 0.0331\n",
            "Iteration #235 loss: 0.0341\n",
            "Iteration #236 loss: 0.0288\n",
            "Iteration #237 loss: 0.0636\n",
            "Iteration #238 loss: 0.0346\n",
            "Iteration #239 loss: 0.0472\n",
            "Iteration #240 loss: 0.0388\n",
            "Iteration #241 loss: 0.0312\n",
            "Iteration #242 loss: 0.0301\n",
            "Iteration #243 loss: 0.0201\n",
            "Iteration #244 loss: 0.0274\n",
            "Iteration #245 loss: 0.0364\n",
            "Iteration #246 loss: 0.0271\n",
            "Iteration #247 loss: 0.0422\n",
            "Iteration #248 loss: 0.0267\n",
            "Iteration #249 loss: 0.0579\n",
            "Iteration #250 loss: 0.0567\n",
            "Training Iteration #250 loss: 0.056702645378573335\n",
            "Iteration #251 loss: 0.0815\n",
            "Iteration #252 loss: 0.0328\n",
            "Iteration #253 loss: 0.0449\n",
            "Epoch #6 Training loss: 0.04690021739325835\n",
            "Epoch #6 Training loss: 0.0469\n",
            "Start validation for Epoch #6...\n",
            "Starting validation for Epoch #6...\n",
            "Iteration #1 loss: 0.0741\n",
            "Iteration #2 loss: 0.0394\n",
            "Iteration #3 loss: 0.1247\n",
            "Iteration #4 loss: 0.0493\n",
            "Iteration #5 loss: 0.0549\n",
            "Iteration #6 loss: 0.0444\n",
            "Iteration #7 loss: 0.0565\n",
            "Iteration #8 loss: 0.0455\n",
            "Iteration #9 loss: 0.0506\n",
            "Iteration #10 loss: 0.0814\n",
            "Iteration #11 loss: 0.0456\n",
            "Iteration #12 loss: 0.0739\n",
            "Iteration #13 loss: 0.0752\n",
            "Iteration #14 loss: 0.0750\n",
            "Iteration #15 loss: 0.0977\n",
            "Iteration #16 loss: 0.0455\n",
            "Iteration #17 loss: 0.0731\n",
            "Epoch #6 Validation loss: 0.0651\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #7...\n",
            "Starting training for Epoch #7...\n",
            "Iteration #1 loss: 0.0414\n",
            "Iteration #2 loss: 0.0300\n",
            "Iteration #3 loss: 0.0306\n",
            "Iteration #4 loss: 0.0887\n",
            "Iteration #5 loss: 0.0512\n",
            "Iteration #6 loss: 0.0301\n",
            "Iteration #7 loss: 0.0601\n",
            "Iteration #8 loss: 0.0486\n",
            "Iteration #9 loss: 0.0285\n",
            "Iteration #10 loss: 0.0263\n",
            "Iteration #11 loss: 0.0220\n",
            "Iteration #12 loss: 0.0657\n",
            "Iteration #13 loss: 0.0328\n",
            "Iteration #14 loss: 0.0258\n",
            "Iteration #15 loss: 0.0247\n",
            "Iteration #16 loss: 0.0372\n",
            "Iteration #17 loss: 0.0674\n",
            "Iteration #18 loss: 0.0732\n",
            "Iteration #19 loss: 0.0340\n",
            "Iteration #20 loss: 0.0276\n",
            "Iteration #21 loss: 0.0278\n",
            "Iteration #22 loss: 0.0317\n",
            "Iteration #23 loss: 0.0476\n",
            "Iteration #24 loss: 0.1414\n",
            "Iteration #25 loss: 0.0460\n",
            "Iteration #26 loss: 0.0696\n",
            "Iteration #27 loss: 0.0243\n",
            "Iteration #28 loss: 0.0214\n",
            "Iteration #29 loss: 0.0208\n",
            "Iteration #30 loss: 0.0376\n",
            "Iteration #31 loss: 0.0238\n",
            "Iteration #32 loss: 0.1008\n",
            "Iteration #33 loss: 0.0486\n",
            "Iteration #34 loss: 0.0371\n",
            "Iteration #35 loss: 0.0285\n",
            "Iteration #36 loss: 0.0247\n",
            "Iteration #37 loss: 0.0761\n",
            "Iteration #38 loss: 0.0325\n",
            "Iteration #39 loss: 0.0323\n",
            "Iteration #40 loss: 0.0224\n",
            "Iteration #41 loss: 0.0210\n",
            "Iteration #42 loss: 0.0766\n",
            "Iteration #43 loss: 0.0220\n",
            "Iteration #44 loss: 0.0756\n",
            "Iteration #45 loss: 0.0804\n",
            "Iteration #46 loss: 0.0272\n",
            "Iteration #47 loss: 0.0640\n",
            "Iteration #48 loss: 0.0241\n",
            "Iteration #49 loss: 0.0177\n",
            "Iteration #50 loss: 0.0200\n",
            "Training Iteration #50 loss: 0.019994641523457615\n",
            "Iteration #51 loss: 0.0266\n",
            "Iteration #52 loss: 0.0575\n",
            "Iteration #53 loss: 0.0222\n",
            "Iteration #54 loss: 0.0199\n",
            "Iteration #55 loss: 0.0514\n",
            "Iteration #56 loss: 0.1203\n",
            "Iteration #57 loss: 0.0311\n",
            "Iteration #58 loss: 0.0199\n",
            "Iteration #59 loss: 0.0267\n",
            "Iteration #60 loss: 0.0168\n",
            "Iteration #61 loss: 0.0248\n",
            "Iteration #62 loss: 0.0205\n",
            "Iteration #63 loss: 0.0731\n",
            "Iteration #64 loss: 0.0188\n",
            "Iteration #65 loss: 0.0193\n",
            "Iteration #66 loss: 0.0563\n",
            "Iteration #67 loss: 0.0237\n",
            "Iteration #68 loss: 0.0237\n",
            "Iteration #69 loss: 0.0426\n",
            "Iteration #70 loss: 0.0308\n",
            "Iteration #71 loss: 0.0348\n",
            "Iteration #72 loss: 0.0748\n",
            "Iteration #73 loss: 0.0194\n",
            "Iteration #74 loss: 0.0517\n",
            "Iteration #75 loss: 0.0281\n",
            "Iteration #76 loss: 0.0265\n",
            "Iteration #77 loss: 0.0266\n",
            "Iteration #78 loss: 0.0362\n",
            "Iteration #79 loss: 0.0259\n",
            "Iteration #80 loss: 0.0340\n",
            "Iteration #81 loss: 0.0252\n",
            "Iteration #82 loss: 0.0279\n",
            "Iteration #83 loss: 0.0417\n",
            "Iteration #84 loss: 0.0300\n",
            "Iteration #85 loss: 0.0292\n",
            "Iteration #86 loss: 0.0244\n",
            "Iteration #87 loss: 0.0273\n",
            "Iteration #88 loss: 0.0326\n",
            "Iteration #89 loss: 0.0248\n",
            "Iteration #90 loss: 0.0511\n",
            "Iteration #91 loss: 0.0308\n",
            "Iteration #92 loss: 0.0255\n",
            "Iteration #93 loss: 0.0301\n",
            "Iteration #94 loss: 0.0481\n",
            "Iteration #95 loss: 0.0701\n",
            "Iteration #96 loss: 0.0187\n",
            "Iteration #97 loss: 0.0301\n",
            "Iteration #98 loss: 0.0712\n",
            "Iteration #99 loss: 0.0326\n",
            "Iteration #100 loss: 0.0311\n",
            "Training Iteration #100 loss: 0.031092304792346048\n",
            "Iteration #101 loss: 0.0475\n",
            "Iteration #102 loss: 0.0456\n",
            "Iteration #103 loss: 0.0269\n",
            "Iteration #104 loss: 0.0364\n",
            "Iteration #105 loss: 0.0447\n",
            "Iteration #106 loss: 0.0246\n",
            "Iteration #107 loss: 0.0198\n",
            "Iteration #108 loss: 0.0273\n",
            "Iteration #109 loss: 0.0561\n",
            "Iteration #110 loss: 0.1017\n",
            "Iteration #111 loss: 0.1161\n",
            "Iteration #112 loss: 0.0205\n",
            "Iteration #113 loss: 0.0244\n",
            "Iteration #114 loss: 0.0691\n",
            "Iteration #115 loss: 0.0330\n",
            "Iteration #116 loss: 0.0252\n",
            "Iteration #117 loss: 0.0187\n",
            "Iteration #118 loss: 0.1405\n",
            "Iteration #119 loss: 0.0636\n",
            "Iteration #120 loss: 0.0205\n",
            "Iteration #121 loss: 0.0475\n",
            "Iteration #122 loss: 0.0340\n",
            "Iteration #123 loss: 0.0592\n",
            "Iteration #124 loss: 0.0390\n",
            "Iteration #125 loss: 0.0229\n",
            "Iteration #126 loss: 0.0218\n",
            "Iteration #127 loss: 0.0203\n",
            "Iteration #128 loss: 0.0204\n",
            "Iteration #129 loss: 0.0228\n",
            "Iteration #130 loss: 0.0208\n",
            "Iteration #131 loss: 0.0415\n",
            "Iteration #132 loss: 0.0214\n",
            "Iteration #133 loss: 0.0474\n",
            "Iteration #134 loss: 0.0309\n",
            "Iteration #135 loss: 0.0460\n",
            "Iteration #136 loss: 0.0420\n",
            "Iteration #137 loss: 0.0498\n",
            "Iteration #138 loss: 0.0470\n",
            "Iteration #139 loss: 0.1064\n",
            "Iteration #140 loss: 0.0305\n",
            "Iteration #141 loss: 0.0403\n",
            "Iteration #142 loss: 0.0323\n",
            "Iteration #143 loss: 0.1784\n",
            "Iteration #144 loss: 0.0195\n",
            "Iteration #145 loss: 0.0740\n",
            "Iteration #146 loss: 0.0255\n",
            "Iteration #147 loss: 0.0215\n",
            "Iteration #148 loss: 0.0703\n",
            "Iteration #149 loss: 0.0346\n",
            "Iteration #150 loss: 0.0244\n",
            "Training Iteration #150 loss: 0.024361604921744558\n",
            "Iteration #151 loss: 0.0361\n",
            "Iteration #152 loss: 0.0242\n",
            "Iteration #153 loss: 0.2540\n",
            "Iteration #154 loss: 0.0340\n",
            "Iteration #155 loss: 0.0221\n",
            "Iteration #156 loss: 0.0287\n",
            "Iteration #157 loss: 0.0248\n",
            "Iteration #158 loss: 0.0536\n",
            "Iteration #159 loss: 0.0294\n",
            "Iteration #160 loss: 0.0298\n",
            "Iteration #161 loss: 0.0260\n",
            "Iteration #162 loss: 0.0333\n",
            "Iteration #163 loss: 0.0256\n",
            "Iteration #164 loss: 0.0202\n",
            "Iteration #165 loss: 0.0242\n",
            "Iteration #166 loss: 0.0206\n",
            "Iteration #167 loss: 0.0368\n",
            "Iteration #168 loss: 0.0348\n",
            "Iteration #169 loss: 0.1173\n",
            "Iteration #170 loss: 0.0441\n",
            "Iteration #171 loss: 0.0408\n",
            "Iteration #172 loss: 0.0301\n",
            "Iteration #173 loss: 0.0298\n",
            "Iteration #174 loss: 0.0185\n",
            "Iteration #175 loss: 0.0685\n",
            "Iteration #176 loss: 0.0392\n",
            "Iteration #177 loss: 0.0332\n",
            "Iteration #178 loss: 0.0280\n",
            "Iteration #179 loss: 0.0238\n",
            "Iteration #180 loss: 0.0225\n",
            "Iteration #181 loss: 0.0521\n",
            "Iteration #182 loss: 0.0302\n",
            "Iteration #183 loss: 0.0462\n",
            "Iteration #184 loss: 0.0177\n",
            "Iteration #185 loss: 0.0484\n",
            "Iteration #186 loss: 0.0241\n",
            "Iteration #187 loss: 0.0896\n",
            "Iteration #188 loss: 0.0353\n",
            "Iteration #189 loss: 0.0599\n",
            "Iteration #190 loss: 0.0395\n",
            "Iteration #191 loss: 0.0256\n",
            "Iteration #192 loss: 0.0480\n",
            "Iteration #193 loss: 0.0188\n",
            "Iteration #194 loss: 0.0276\n",
            "Iteration #195 loss: 0.0210\n",
            "Iteration #196 loss: 0.0410\n",
            "Iteration #197 loss: 0.0269\n",
            "Iteration #198 loss: 0.0312\n",
            "Iteration #199 loss: 0.0342\n",
            "Iteration #200 loss: 0.0616\n",
            "Training Iteration #200 loss: 0.06156461128287639\n",
            "Iteration #201 loss: 0.0257\n",
            "Iteration #202 loss: 0.0310\n",
            "Iteration #203 loss: 0.0288\n",
            "Iteration #204 loss: 0.0191\n",
            "Iteration #205 loss: 0.0249\n",
            "Iteration #206 loss: 0.1311\n",
            "Iteration #207 loss: 0.0352\n",
            "Iteration #208 loss: 0.0261\n",
            "Iteration #209 loss: 0.0242\n",
            "Iteration #210 loss: 0.0520\n",
            "Iteration #211 loss: 0.0392\n",
            "Iteration #212 loss: 0.0296\n",
            "Iteration #213 loss: 0.0450\n",
            "Iteration #214 loss: 0.0256\n",
            "Iteration #215 loss: 0.0636\n",
            "Iteration #216 loss: 0.0287\n",
            "Iteration #217 loss: 0.0307\n",
            "Iteration #218 loss: 0.0257\n",
            "Iteration #219 loss: 0.0269\n",
            "Iteration #220 loss: 0.0573\n",
            "Iteration #221 loss: 0.1163\n",
            "Iteration #222 loss: 0.1004\n",
            "Iteration #223 loss: 0.0155\n",
            "Iteration #224 loss: 0.1285\n",
            "Iteration #225 loss: 0.0274\n",
            "Iteration #226 loss: 0.0222\n",
            "Iteration #227 loss: 0.0243\n",
            "Iteration #228 loss: 0.0160\n",
            "Iteration #229 loss: 0.1133\n",
            "Iteration #230 loss: 0.0307\n",
            "Iteration #231 loss: 0.0226\n",
            "Iteration #232 loss: 0.0960\n",
            "Iteration #233 loss: 0.0576\n",
            "Iteration #234 loss: 0.0146\n",
            "Iteration #235 loss: 0.0285\n",
            "Iteration #236 loss: 0.0992\n",
            "Iteration #237 loss: 0.0397\n",
            "Iteration #238 loss: 0.0176\n",
            "Iteration #239 loss: 0.0206\n",
            "Iteration #240 loss: 0.0385\n",
            "Iteration #241 loss: 0.0390\n",
            "Iteration #242 loss: 0.0261\n",
            "Iteration #243 loss: 0.0326\n",
            "Iteration #244 loss: 0.0428\n",
            "Iteration #245 loss: 0.0611\n",
            "Iteration #246 loss: 0.0307\n",
            "Iteration #247 loss: 0.0375\n",
            "Iteration #248 loss: 0.0256\n",
            "Iteration #249 loss: 0.0688\n",
            "Iteration #250 loss: 0.0241\n",
            "Training Iteration #250 loss: 0.0241004017834296\n",
            "Iteration #251 loss: 0.0347\n",
            "Iteration #252 loss: 0.0284\n",
            "Iteration #253 loss: 0.0816\n",
            "Epoch #7 Training loss: 0.04180401137996745\n",
            "Epoch #7 Training loss: 0.0418\n",
            "Start validation for Epoch #7...\n",
            "Starting validation for Epoch #7...\n",
            "Iteration #1 loss: 0.0822\n",
            "Iteration #2 loss: 0.0316\n",
            "Iteration #3 loss: 0.1187\n",
            "Iteration #4 loss: 0.0427\n",
            "Iteration #5 loss: 0.0619\n",
            "Iteration #6 loss: 0.0483\n",
            "Iteration #7 loss: 0.0728\n",
            "Iteration #8 loss: 0.0446\n",
            "Iteration #9 loss: 0.0558\n",
            "Iteration #10 loss: 0.0689\n",
            "Iteration #11 loss: 0.0380\n",
            "Iteration #12 loss: 0.0737\n",
            "Iteration #13 loss: 0.0847\n",
            "Iteration #14 loss: 0.0856\n",
            "Iteration #15 loss: 0.0846\n",
            "Iteration #16 loss: 0.0376\n",
            "Iteration #17 loss: 0.0611\n",
            "Epoch #7 Validation loss: 0.0643\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #8...\n",
            "Starting training for Epoch #8...\n",
            "Iteration #1 loss: 0.0346\n",
            "Iteration #2 loss: 0.0184\n",
            "Iteration #3 loss: 0.0316\n",
            "Iteration #4 loss: 0.0810\n",
            "Iteration #5 loss: 0.0880\n",
            "Iteration #6 loss: 0.0276\n",
            "Iteration #7 loss: 0.0291\n",
            "Iteration #8 loss: 0.0348\n",
            "Iteration #9 loss: 0.0342\n",
            "Iteration #10 loss: 0.0301\n",
            "Iteration #11 loss: 0.0282\n",
            "Iteration #12 loss: 0.0327\n",
            "Iteration #13 loss: 0.0215\n",
            "Iteration #14 loss: 0.0484\n",
            "Iteration #15 loss: 0.0230\n",
            "Iteration #16 loss: 0.0193\n",
            "Iteration #17 loss: 0.0239\n",
            "Iteration #18 loss: 0.0233\n",
            "Iteration #19 loss: 0.0266\n",
            "Iteration #20 loss: 0.0156\n",
            "Iteration #21 loss: 0.0398\n",
            "Iteration #22 loss: 0.0727\n",
            "Iteration #23 loss: 0.1298\n",
            "Iteration #24 loss: 0.0360\n",
            "Iteration #25 loss: 0.0350\n",
            "Iteration #26 loss: 0.0224\n",
            "Iteration #27 loss: 0.0276\n",
            "Iteration #28 loss: 0.0357\n",
            "Iteration #29 loss: 0.0441\n",
            "Iteration #30 loss: 0.0292\n",
            "Iteration #31 loss: 0.0188\n",
            "Iteration #32 loss: 0.0289\n",
            "Iteration #33 loss: 0.0388\n",
            "Iteration #34 loss: 0.0270\n",
            "Iteration #35 loss: 0.0228\n",
            "Iteration #36 loss: 0.0283\n",
            "Iteration #37 loss: 0.0761\n",
            "Iteration #38 loss: 0.1158\n",
            "Iteration #39 loss: 0.0323\n",
            "Iteration #40 loss: 0.0266\n",
            "Iteration #41 loss: 0.0372\n",
            "Iteration #42 loss: 0.0433\n",
            "Iteration #43 loss: 0.0527\n",
            "Iteration #44 loss: 0.0183\n",
            "Iteration #45 loss: 0.0186\n",
            "Iteration #46 loss: 0.0242\n",
            "Iteration #47 loss: 0.0408\n",
            "Iteration #48 loss: 0.0516\n",
            "Iteration #49 loss: 0.0947\n",
            "Iteration #50 loss: 0.0919\n",
            "Training Iteration #50 loss: 0.09187565135732813\n",
            "Iteration #51 loss: 0.0575\n",
            "Iteration #52 loss: 0.0386\n",
            "Iteration #53 loss: 0.0348\n",
            "Iteration #54 loss: 0.0326\n",
            "Iteration #55 loss: 0.0406\n",
            "Iteration #56 loss: 0.0451\n",
            "Iteration #57 loss: 0.0253\n",
            "Iteration #58 loss: 0.0431\n",
            "Iteration #59 loss: 0.0178\n",
            "Iteration #60 loss: 0.0335\n",
            "Iteration #61 loss: 0.0485\n",
            "Iteration #62 loss: 0.0260\n",
            "Iteration #63 loss: 0.0294\n",
            "Iteration #64 loss: 0.0437\n",
            "Iteration #65 loss: 0.0390\n",
            "Iteration #66 loss: 0.0355\n",
            "Iteration #67 loss: 0.0688\n",
            "Iteration #68 loss: 0.0295\n",
            "Iteration #69 loss: 0.0451\n",
            "Iteration #70 loss: 0.0285\n",
            "Iteration #71 loss: 0.0333\n",
            "Iteration #72 loss: 0.0279\n",
            "Iteration #73 loss: 0.0262\n",
            "Iteration #74 loss: 0.0467\n",
            "Iteration #75 loss: 0.0369\n",
            "Iteration #76 loss: 0.0648\n",
            "Iteration #77 loss: 0.0419\n",
            "Iteration #78 loss: 0.0426\n",
            "Iteration #79 loss: 0.0419\n",
            "Iteration #80 loss: 0.0281\n",
            "Iteration #81 loss: 0.0234\n",
            "Iteration #82 loss: 0.0444\n",
            "Iteration #83 loss: 0.0267\n",
            "Iteration #84 loss: 0.0610\n",
            "Iteration #85 loss: 0.0263\n",
            "Iteration #86 loss: 0.0271\n",
            "Iteration #87 loss: 0.0565\n",
            "Iteration #88 loss: 0.0421\n",
            "Iteration #89 loss: 0.0466\n",
            "Iteration #90 loss: 0.0844\n",
            "Iteration #91 loss: 0.0234\n",
            "Iteration #92 loss: 0.0296\n",
            "Iteration #93 loss: 0.0225\n",
            "Iteration #94 loss: 0.0298\n",
            "Iteration #95 loss: 0.0439\n",
            "Iteration #96 loss: 0.1314\n",
            "Iteration #97 loss: 0.0244\n",
            "Iteration #98 loss: 0.0344\n",
            "Iteration #99 loss: 0.0203\n",
            "Iteration #100 loss: 0.0270\n",
            "Training Iteration #100 loss: 0.026956346426480895\n",
            "Iteration #101 loss: 0.0936\n",
            "Iteration #102 loss: 0.0255\n",
            "Iteration #103 loss: 0.0239\n",
            "Iteration #104 loss: 0.0218\n",
            "Iteration #105 loss: 0.0284\n",
            "Iteration #106 loss: 0.0451\n",
            "Iteration #107 loss: 0.0289\n",
            "Iteration #108 loss: 0.0284\n",
            "Iteration #109 loss: 0.0369\n",
            "Iteration #110 loss: 0.0175\n",
            "Iteration #111 loss: 0.0289\n",
            "Iteration #112 loss: 0.0300\n",
            "Iteration #113 loss: 0.0191\n",
            "Iteration #114 loss: 0.0267\n",
            "Iteration #115 loss: 0.0207\n",
            "Iteration #116 loss: 0.0308\n",
            "Iteration #117 loss: 0.0296\n",
            "Iteration #118 loss: 0.0184\n",
            "Iteration #119 loss: 0.1687\n",
            "Iteration #120 loss: 0.0905\n",
            "Iteration #121 loss: 0.0747\n",
            "Iteration #122 loss: 0.0781\n",
            "Iteration #123 loss: 0.0488\n",
            "Iteration #124 loss: 0.0218\n",
            "Iteration #125 loss: 0.0597\n",
            "Iteration #126 loss: 0.0200\n",
            "Iteration #127 loss: 0.0235\n",
            "Iteration #128 loss: 0.0336\n",
            "Iteration #129 loss: 0.0527\n",
            "Iteration #130 loss: 0.0306\n",
            "Iteration #131 loss: 0.0321\n",
            "Iteration #132 loss: 0.0272\n",
            "Iteration #133 loss: 0.0291\n",
            "Iteration #134 loss: 0.0270\n",
            "Iteration #135 loss: 0.1168\n",
            "Iteration #136 loss: 0.0412\n",
            "Iteration #137 loss: 0.0928\n",
            "Iteration #138 loss: 0.0235\n",
            "Iteration #139 loss: 0.0538\n",
            "Iteration #140 loss: 0.0217\n",
            "Iteration #141 loss: 0.0410\n",
            "Iteration #142 loss: 0.0281\n",
            "Iteration #143 loss: 0.0836\n",
            "Iteration #144 loss: 0.0224\n",
            "Iteration #145 loss: 0.0167\n",
            "Iteration #146 loss: 0.0393\n",
            "Iteration #147 loss: 0.0243\n",
            "Iteration #148 loss: 0.0693\n",
            "Iteration #149 loss: 0.0258\n",
            "Iteration #150 loss: 0.0378\n",
            "Training Iteration #150 loss: 0.037785197956588426\n",
            "Iteration #151 loss: 0.0324\n",
            "Iteration #152 loss: 0.0378\n",
            "Iteration #153 loss: 0.0884\n",
            "Iteration #154 loss: 0.0263\n",
            "Iteration #155 loss: 0.0394\n",
            "Iteration #156 loss: 0.0258\n",
            "Iteration #157 loss: 0.0332\n",
            "Iteration #158 loss: 0.0386\n",
            "Iteration #159 loss: 0.0342\n",
            "Iteration #160 loss: 0.0278\n",
            "Iteration #161 loss: 0.0359\n",
            "Iteration #162 loss: 0.0283\n",
            "Iteration #163 loss: 0.0292\n",
            "Iteration #164 loss: 0.0131\n",
            "Iteration #165 loss: 0.0278\n",
            "Iteration #166 loss: 0.0343\n",
            "Iteration #167 loss: 0.0466\n",
            "Iteration #168 loss: 0.0406\n",
            "Iteration #169 loss: 0.0458\n",
            "Iteration #170 loss: 0.0256\n",
            "Iteration #171 loss: 0.0230\n",
            "Iteration #172 loss: 0.0237\n",
            "Iteration #173 loss: 0.0542\n",
            "Iteration #174 loss: 0.1218\n",
            "Iteration #175 loss: 0.0200\n",
            "Iteration #176 loss: 0.0707\n",
            "Iteration #177 loss: 0.0225\n",
            "Iteration #178 loss: 0.0163\n",
            "Iteration #179 loss: 0.0322\n",
            "Iteration #180 loss: 0.0279\n",
            "Iteration #181 loss: 0.0701\n",
            "Iteration #182 loss: 0.0206\n",
            "Iteration #183 loss: 0.0749\n",
            "Iteration #184 loss: 0.0428\n",
            "Iteration #185 loss: 0.0247\n",
            "Iteration #186 loss: 0.0220\n",
            "Iteration #187 loss: 0.0270\n",
            "Iteration #188 loss: 0.0246\n",
            "Iteration #189 loss: 0.0264\n",
            "Iteration #190 loss: 0.0375\n",
            "Iteration #191 loss: 0.0438\n",
            "Iteration #192 loss: 0.0240\n",
            "Iteration #193 loss: 0.0260\n",
            "Iteration #194 loss: 0.0236\n",
            "Iteration #195 loss: 0.0192\n",
            "Iteration #196 loss: 0.0279\n",
            "Iteration #197 loss: 0.0327\n",
            "Iteration #198 loss: 0.0642\n",
            "Iteration #199 loss: 0.0206\n",
            "Iteration #200 loss: 0.0246\n",
            "Training Iteration #200 loss: 0.02461086350049268\n",
            "Iteration #201 loss: 0.0125\n",
            "Iteration #202 loss: 0.0824\n",
            "Iteration #203 loss: 0.0279\n",
            "Iteration #204 loss: 0.0204\n",
            "Iteration #205 loss: 0.0140\n",
            "Iteration #206 loss: 0.0299\n",
            "Iteration #207 loss: 0.0208\n",
            "Iteration #208 loss: 0.0320\n",
            "Iteration #209 loss: 0.0452\n",
            "Iteration #210 loss: 0.0244\n",
            "Iteration #211 loss: 0.0278\n",
            "Iteration #212 loss: 0.0391\n",
            "Iteration #213 loss: 0.0186\n",
            "Iteration #214 loss: 0.0493\n",
            "Iteration #215 loss: 0.0197\n",
            "Iteration #216 loss: 0.0161\n",
            "Iteration #217 loss: 0.1051\n",
            "Iteration #218 loss: 0.0356\n",
            "Iteration #219 loss: 0.0458\n",
            "Iteration #220 loss: 0.0338\n",
            "Iteration #221 loss: 0.1142\n",
            "Iteration #222 loss: 0.0506\n",
            "Iteration #223 loss: 0.0228\n",
            "Iteration #224 loss: 0.0265\n",
            "Iteration #225 loss: 0.0209\n",
            "Iteration #226 loss: 0.0208\n",
            "Iteration #227 loss: 0.0235\n",
            "Iteration #228 loss: 0.0569\n",
            "Iteration #229 loss: 0.0304\n",
            "Iteration #230 loss: 0.0200\n",
            "Iteration #231 loss: 0.0182\n",
            "Iteration #232 loss: 0.0504\n",
            "Iteration #233 loss: 0.0303\n",
            "Iteration #234 loss: 0.0344\n",
            "Iteration #235 loss: 0.0372\n",
            "Iteration #236 loss: 0.0605\n",
            "Iteration #237 loss: 0.0382\n",
            "Iteration #238 loss: 0.0263\n",
            "Iteration #239 loss: 0.0231\n",
            "Iteration #240 loss: 0.0220\n",
            "Iteration #241 loss: 0.0463\n",
            "Iteration #242 loss: 0.0439\n",
            "Iteration #243 loss: 0.0322\n",
            "Iteration #244 loss: 0.0268\n",
            "Iteration #245 loss: 0.0251\n",
            "Iteration #246 loss: 0.0203\n",
            "Iteration #247 loss: 0.0167\n",
            "Iteration #248 loss: 0.0209\n",
            "Iteration #249 loss: 0.0197\n",
            "Iteration #250 loss: 0.0229\n",
            "Training Iteration #250 loss: 0.02287298326815934\n",
            "Iteration #251 loss: 0.0379\n",
            "Iteration #252 loss: 0.0234\n",
            "Iteration #253 loss: 0.0411\n",
            "Epoch #8 Training loss: 0.03844649346813927\n",
            "Epoch #8 Training loss: 0.0384\n",
            "Start validation for Epoch #8...\n",
            "Starting validation for Epoch #8...\n",
            "Iteration #1 loss: 0.0771\n",
            "Iteration #2 loss: 0.0312\n",
            "Iteration #3 loss: 0.1101\n",
            "Iteration #4 loss: 0.0398\n",
            "Iteration #5 loss: 0.0510\n",
            "Iteration #6 loss: 0.0378\n",
            "Iteration #7 loss: 0.0574\n",
            "Iteration #8 loss: 0.0387\n",
            "Iteration #9 loss: 0.0569\n",
            "Iteration #10 loss: 0.0789\n",
            "Iteration #11 loss: 0.0346\n",
            "Iteration #12 loss: 0.0575\n",
            "Iteration #13 loss: 0.0789\n",
            "Iteration #14 loss: 0.0699\n",
            "Iteration #15 loss: 0.0883\n",
            "Iteration #16 loss: 0.0357\n",
            "Iteration #17 loss: 0.0559\n",
            "Epoch #8 Validation loss: 0.0588\n",
            "New best validation loss: 0.0588 at epoch #8. Saving model...\n",
            "New best validation loss: 0.0588 at epoch #8. Saving model...\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #9...\n",
            "Starting training for Epoch #9...\n",
            "Iteration #1 loss: 0.0194\n",
            "Iteration #2 loss: 0.0444\n",
            "Iteration #3 loss: 0.0743\n",
            "Iteration #4 loss: 0.0984\n",
            "Iteration #5 loss: 0.0174\n",
            "Iteration #6 loss: 0.0654\n",
            "Iteration #7 loss: 0.0216\n",
            "Iteration #8 loss: 0.0215\n",
            "Iteration #9 loss: 0.0182\n",
            "Iteration #10 loss: 0.0738\n",
            "Iteration #11 loss: 0.0446\n",
            "Iteration #12 loss: 0.0443\n",
            "Iteration #13 loss: 0.0225\n",
            "Iteration #14 loss: 0.0294\n",
            "Iteration #15 loss: 0.0197\n",
            "Iteration #16 loss: 0.0356\n",
            "Iteration #17 loss: 0.0418\n",
            "Iteration #18 loss: 0.0246\n",
            "Iteration #19 loss: 0.0284\n",
            "Iteration #20 loss: 0.0257\n",
            "Iteration #21 loss: 0.0209\n",
            "Iteration #22 loss: 0.0265\n",
            "Iteration #23 loss: 0.0247\n",
            "Iteration #24 loss: 0.0822\n",
            "Iteration #25 loss: 0.0182\n",
            "Iteration #26 loss: 0.0313\n",
            "Iteration #27 loss: 0.0520\n",
            "Iteration #28 loss: 0.0281\n",
            "Iteration #29 loss: 0.0740\n",
            "Iteration #30 loss: 0.0230\n",
            "Iteration #31 loss: 0.0248\n",
            "Iteration #32 loss: 0.0218\n",
            "Iteration #33 loss: 0.0357\n",
            "Iteration #34 loss: 0.0606\n",
            "Iteration #35 loss: 0.0719\n",
            "Iteration #36 loss: 0.0293\n",
            "Iteration #37 loss: 0.0157\n",
            "Iteration #38 loss: 0.0374\n",
            "Iteration #39 loss: 0.0217\n",
            "Iteration #40 loss: 0.0325\n",
            "Iteration #41 loss: 0.0144\n",
            "Iteration #42 loss: 0.0281\n",
            "Iteration #43 loss: 0.0157\n",
            "Iteration #44 loss: 0.0199\n",
            "Iteration #45 loss: 0.0313\n",
            "Iteration #46 loss: 0.0498\n",
            "Iteration #47 loss: 0.0242\n",
            "Iteration #48 loss: 0.0605\n",
            "Iteration #49 loss: 0.0210\n",
            "Iteration #50 loss: 0.0177\n",
            "Training Iteration #50 loss: 0.01774163386773459\n",
            "Iteration #51 loss: 0.0250\n",
            "Iteration #52 loss: 0.0483\n",
            "Iteration #53 loss: 0.0261\n",
            "Iteration #54 loss: 0.0344\n",
            "Iteration #55 loss: 0.0812\n",
            "Iteration #56 loss: 0.0300\n",
            "Iteration #57 loss: 0.0226\n",
            "Iteration #58 loss: 0.0311\n",
            "Iteration #59 loss: 0.0179\n",
            "Iteration #60 loss: 0.0208\n",
            "Iteration #61 loss: 0.0506\n",
            "Iteration #62 loss: 0.0149\n",
            "Iteration #63 loss: 0.0907\n",
            "Iteration #64 loss: 0.0195\n",
            "Iteration #65 loss: 0.0193\n",
            "Iteration #66 loss: 0.0385\n",
            "Iteration #67 loss: 0.0235\n",
            "Iteration #68 loss: 0.0444\n",
            "Iteration #69 loss: 0.0268\n",
            "Iteration #70 loss: 0.0150\n",
            "Iteration #71 loss: 0.0214\n",
            "Iteration #72 loss: 0.0348\n",
            "Iteration #73 loss: 0.0216\n",
            "Iteration #74 loss: 0.0285\n",
            "Iteration #75 loss: 0.0188\n",
            "Iteration #76 loss: 0.0338\n",
            "Iteration #77 loss: 0.0263\n",
            "Iteration #78 loss: 0.0404\n",
            "Iteration #79 loss: 0.0179\n",
            "Iteration #80 loss: 0.0158\n",
            "Iteration #81 loss: 0.0308\n",
            "Iteration #82 loss: 0.0288\n",
            "Iteration #83 loss: 0.0215\n",
            "Iteration #84 loss: 0.0587\n",
            "Iteration #85 loss: 0.0337\n",
            "Iteration #86 loss: 0.0272\n",
            "Iteration #87 loss: 0.0271\n",
            "Iteration #88 loss: 0.0213\n",
            "Iteration #89 loss: 0.1279\n",
            "Iteration #90 loss: 0.0273\n",
            "Iteration #91 loss: 0.0197\n",
            "Iteration #92 loss: 0.0240\n",
            "Iteration #93 loss: 0.0274\n",
            "Iteration #94 loss: 0.0308\n",
            "Iteration #95 loss: 0.0178\n",
            "Iteration #96 loss: 0.0311\n",
            "Iteration #97 loss: 0.0489\n",
            "Iteration #98 loss: 0.0235\n",
            "Iteration #99 loss: 0.0526\n",
            "Iteration #100 loss: 0.0227\n",
            "Training Iteration #100 loss: 0.022705567397408918\n",
            "Iteration #101 loss: 0.0122\n",
            "Iteration #102 loss: 0.0540\n",
            "Iteration #103 loss: 0.0354\n",
            "Iteration #104 loss: 0.0335\n",
            "Iteration #105 loss: 0.1691\n",
            "Iteration #106 loss: 0.0347\n",
            "Iteration #107 loss: 0.0233\n",
            "Iteration #108 loss: 0.0319\n",
            "Iteration #109 loss: 0.0329\n",
            "Iteration #110 loss: 0.0214\n",
            "Iteration #111 loss: 0.0438\n",
            "Iteration #112 loss: 0.0298\n",
            "Iteration #113 loss: 0.0259\n",
            "Iteration #114 loss: 0.0313\n",
            "Iteration #115 loss: 0.0336\n",
            "Iteration #116 loss: 0.0234\n",
            "Iteration #117 loss: 0.0254\n",
            "Iteration #118 loss: 0.0522\n",
            "Iteration #119 loss: 0.0302\n",
            "Iteration #120 loss: 0.0373\n",
            "Iteration #121 loss: 0.0307\n",
            "Iteration #122 loss: 0.0320\n",
            "Iteration #123 loss: 0.0263\n",
            "Iteration #124 loss: 0.0520\n",
            "Iteration #125 loss: 0.0255\n",
            "Iteration #126 loss: 0.0869\n",
            "Iteration #127 loss: 0.0320\n",
            "Iteration #128 loss: 0.0164\n",
            "Iteration #129 loss: 0.0191\n",
            "Iteration #130 loss: 0.0214\n",
            "Iteration #131 loss: 0.0830\n",
            "Iteration #132 loss: 0.0422\n",
            "Iteration #133 loss: 0.0744\n",
            "Iteration #134 loss: 0.0191\n",
            "Iteration #135 loss: 0.0252\n",
            "Iteration #136 loss: 0.0769\n",
            "Iteration #137 loss: 0.0243\n",
            "Iteration #138 loss: 0.0277\n",
            "Iteration #139 loss: 0.0564\n",
            "Iteration #140 loss: 0.0316\n",
            "Iteration #141 loss: 0.0207\n",
            "Iteration #142 loss: 0.0242\n",
            "Iteration #143 loss: 0.0183\n",
            "Iteration #144 loss: 0.0223\n",
            "Iteration #145 loss: 0.1688\n",
            "Iteration #146 loss: 0.0327\n",
            "Iteration #147 loss: 0.0225\n",
            "Iteration #148 loss: 0.0351\n",
            "Iteration #149 loss: 0.0290\n",
            "Iteration #150 loss: 0.0219\n",
            "Training Iteration #150 loss: 0.02191224827417661\n",
            "Iteration #151 loss: 0.1299\n",
            "Iteration #152 loss: 0.0381\n",
            "Iteration #153 loss: 0.0191\n",
            "Iteration #154 loss: 0.0231\n",
            "Iteration #155 loss: 0.0499\n",
            "Iteration #156 loss: 0.0361\n",
            "Iteration #157 loss: 0.0258\n",
            "Iteration #158 loss: 0.0303\n",
            "Iteration #159 loss: 0.0399\n",
            "Iteration #160 loss: 0.0424\n",
            "Iteration #161 loss: 0.0227\n",
            "Iteration #162 loss: 0.0273\n",
            "Iteration #163 loss: 0.0228\n",
            "Iteration #164 loss: 0.0729\n",
            "Iteration #165 loss: 0.0266\n",
            "Iteration #166 loss: 0.0306\n",
            "Iteration #167 loss: 0.0214\n",
            "Iteration #168 loss: 0.0281\n",
            "Iteration #169 loss: 0.0267\n",
            "Iteration #170 loss: 0.0782\n",
            "Iteration #171 loss: 0.0201\n",
            "Iteration #172 loss: 0.1757\n",
            "Iteration #173 loss: 0.0205\n",
            "Iteration #174 loss: 0.0553\n",
            "Iteration #175 loss: 0.0685\n",
            "Iteration #176 loss: 0.0240\n",
            "Iteration #177 loss: 0.0248\n",
            "Iteration #178 loss: 0.0313\n",
            "Iteration #179 loss: 0.0320\n",
            "Iteration #180 loss: 0.0769\n",
            "Iteration #181 loss: 0.0207\n",
            "Iteration #182 loss: 0.0291\n",
            "Iteration #183 loss: 0.0217\n",
            "Iteration #184 loss: 0.0187\n",
            "Iteration #185 loss: 0.0294\n",
            "Iteration #186 loss: 0.0306\n",
            "Iteration #187 loss: 0.0321\n",
            "Iteration #188 loss: 0.0204\n",
            "Iteration #189 loss: 0.0327\n",
            "Iteration #190 loss: 0.0338\n",
            "Iteration #191 loss: 0.0339\n",
            "Iteration #192 loss: 0.0498\n",
            "Iteration #193 loss: 0.0133\n",
            "Iteration #194 loss: 0.0457\n",
            "Iteration #195 loss: 0.0300\n",
            "Iteration #196 loss: 0.0300\n",
            "Iteration #197 loss: 0.0467\n",
            "Iteration #198 loss: 0.0225\n",
            "Iteration #199 loss: 0.0399\n",
            "Iteration #200 loss: 0.0182\n",
            "Training Iteration #200 loss: 0.018246951904145926\n",
            "Iteration #201 loss: 0.0248\n",
            "Iteration #202 loss: 0.0353\n",
            "Iteration #203 loss: 0.0334\n",
            "Iteration #204 loss: 0.0273\n",
            "Iteration #205 loss: 0.0380\n",
            "Iteration #206 loss: 0.0209\n",
            "Iteration #207 loss: 0.0290\n",
            "Iteration #208 loss: 0.1226\n",
            "Iteration #209 loss: 0.0349\n",
            "Iteration #210 loss: 0.0153\n",
            "Iteration #211 loss: 0.0400\n",
            "Iteration #212 loss: 0.0378\n",
            "Iteration #213 loss: 0.0658\n",
            "Iteration #214 loss: 0.0205\n",
            "Iteration #215 loss: 0.0298\n",
            "Iteration #216 loss: 0.0246\n",
            "Iteration #217 loss: 0.0680\n",
            "Iteration #218 loss: 0.0601\n",
            "Iteration #219 loss: 0.0233\n",
            "Iteration #220 loss: 0.0269\n",
            "Iteration #221 loss: 0.0169\n",
            "Iteration #222 loss: 0.1050\n",
            "Iteration #223 loss: 0.0325\n",
            "Iteration #224 loss: 0.0359\n",
            "Iteration #225 loss: 0.0225\n",
            "Iteration #226 loss: 0.0448\n",
            "Iteration #227 loss: 0.0557\n",
            "Iteration #228 loss: 0.0154\n",
            "Iteration #229 loss: 0.0209\n",
            "Iteration #230 loss: 0.0931\n",
            "Iteration #231 loss: 0.0159\n",
            "Iteration #232 loss: 0.0638\n",
            "Iteration #233 loss: 0.0166\n",
            "Iteration #234 loss: 0.0179\n",
            "Iteration #235 loss: 0.0216\n",
            "Iteration #236 loss: 0.0203\n",
            "Iteration #237 loss: 0.0358\n",
            "Iteration #238 loss: 0.0202\n",
            "Iteration #239 loss: 0.0199\n",
            "Iteration #240 loss: 0.0399\n",
            "Iteration #241 loss: 0.0191\n",
            "Iteration #242 loss: 0.0208\n",
            "Iteration #243 loss: 0.0451\n",
            "Iteration #244 loss: 0.0430\n",
            "Iteration #245 loss: 0.0298\n",
            "Iteration #246 loss: 0.0213\n",
            "Iteration #247 loss: 0.0239\n",
            "Iteration #248 loss: 0.0948\n",
            "Iteration #249 loss: 0.0199\n",
            "Iteration #250 loss: 0.0395\n",
            "Training Iteration #250 loss: 0.03946415519784723\n",
            "Iteration #251 loss: 0.0454\n",
            "Iteration #252 loss: 0.0445\n",
            "Iteration #253 loss: 0.0428\n",
            "Epoch #9 Training loss: 0.03680483931881467\n",
            "Epoch #9 Training loss: 0.0368\n",
            "Start validation for Epoch #9...\n",
            "Starting validation for Epoch #9...\n",
            "Iteration #1 loss: 0.0799\n",
            "Iteration #2 loss: 0.0322\n",
            "Iteration #3 loss: 0.1033\n",
            "Iteration #4 loss: 0.0416\n",
            "Iteration #5 loss: 0.0506\n",
            "Iteration #6 loss: 0.0457\n",
            "Iteration #7 loss: 0.0631\n",
            "Iteration #8 loss: 0.0392\n",
            "Iteration #9 loss: 0.0463\n",
            "Iteration #10 loss: 0.0710\n",
            "Iteration #11 loss: 0.0332\n",
            "Iteration #12 loss: 0.0740\n",
            "Iteration #13 loss: 0.0807\n",
            "Iteration #14 loss: 0.0669\n",
            "Iteration #15 loss: 0.0858\n",
            "Iteration #16 loss: 0.0363\n",
            "Iteration #17 loss: 0.0652\n",
            "Epoch #9 Validation loss: 0.0597\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #10...\n",
            "Starting training for Epoch #10...\n",
            "Iteration #1 loss: 0.0157\n",
            "Iteration #2 loss: 0.0319\n",
            "Iteration #3 loss: 0.0263\n",
            "Iteration #4 loss: 0.0290\n",
            "Iteration #5 loss: 0.0392\n",
            "Iteration #6 loss: 0.0255\n",
            "Iteration #7 loss: 0.0252\n",
            "Iteration #8 loss: 0.0288\n",
            "Iteration #9 loss: 0.0200\n",
            "Iteration #10 loss: 0.0181\n",
            "Iteration #11 loss: 0.0159\n",
            "Iteration #12 loss: 0.0367\n",
            "Iteration #13 loss: 0.0232\n",
            "Iteration #14 loss: 0.0129\n",
            "Iteration #15 loss: 0.0278\n",
            "Iteration #16 loss: 0.0762\n",
            "Iteration #17 loss: 0.0227\n",
            "Iteration #18 loss: 0.0190\n",
            "Iteration #19 loss: 0.0239\n",
            "Iteration #20 loss: 0.0734\n",
            "Iteration #21 loss: 0.0212\n",
            "Iteration #22 loss: 0.0310\n",
            "Iteration #23 loss: 0.0232\n",
            "Iteration #24 loss: 0.0289\n",
            "Iteration #25 loss: 0.0338\n",
            "Iteration #26 loss: 0.0146\n",
            "Iteration #27 loss: 0.0235\n",
            "Iteration #28 loss: 0.0347\n",
            "Iteration #29 loss: 0.0218\n",
            "Iteration #30 loss: 0.0566\n",
            "Iteration #31 loss: 0.0176\n",
            "Iteration #32 loss: 0.1239\n",
            "Iteration #33 loss: 0.0206\n",
            "Iteration #34 loss: 0.0296\n",
            "Iteration #35 loss: 0.0243\n",
            "Iteration #36 loss: 0.0190\n",
            "Iteration #37 loss: 0.0125\n",
            "Iteration #38 loss: 0.0415\n",
            "Iteration #39 loss: 0.0215\n",
            "Iteration #40 loss: 0.0169\n",
            "Iteration #41 loss: 0.0298\n",
            "Iteration #42 loss: 0.0432\n",
            "Iteration #43 loss: 0.0737\n",
            "Iteration #44 loss: 0.0323\n",
            "Iteration #45 loss: 0.0757\n",
            "Iteration #46 loss: 0.0256\n",
            "Iteration #47 loss: 0.0310\n",
            "Iteration #48 loss: 0.0242\n",
            "Iteration #49 loss: 0.0326\n",
            "Iteration #50 loss: 0.0465\n",
            "Training Iteration #50 loss: 0.046476009520874016\n",
            "Iteration #51 loss: 0.0295\n",
            "Iteration #52 loss: 0.0946\n",
            "Iteration #53 loss: 0.0242\n",
            "Iteration #54 loss: 0.0302\n",
            "Iteration #55 loss: 0.0274\n",
            "Iteration #56 loss: 0.0266\n",
            "Iteration #57 loss: 0.0505\n",
            "Iteration #58 loss: 0.0370\n",
            "Iteration #59 loss: 0.0396\n",
            "Iteration #60 loss: 0.0363\n",
            "Iteration #61 loss: 0.0352\n",
            "Iteration #62 loss: 0.0417\n",
            "Iteration #63 loss: 0.0211\n",
            "Iteration #64 loss: 0.0221\n",
            "Iteration #65 loss: 0.0317\n",
            "Iteration #66 loss: 0.0415\n",
            "Iteration #67 loss: 0.0982\n",
            "Iteration #68 loss: 0.0584\n",
            "Iteration #69 loss: 0.0191\n",
            "Iteration #70 loss: 0.0224\n",
            "Iteration #71 loss: 0.0223\n",
            "Iteration #72 loss: 0.0549\n",
            "Iteration #73 loss: 0.0279\n",
            "Iteration #74 loss: 0.0278\n",
            "Iteration #75 loss: 0.0192\n",
            "Iteration #76 loss: 0.0171\n",
            "Iteration #77 loss: 0.0309\n",
            "Iteration #78 loss: 0.0813\n",
            "Iteration #79 loss: 0.0240\n",
            "Iteration #80 loss: 0.0501\n",
            "Iteration #81 loss: 0.0304\n",
            "Iteration #82 loss: 0.0274\n",
            "Iteration #83 loss: 0.0185\n",
            "Iteration #84 loss: 0.0341\n",
            "Iteration #85 loss: 0.0293\n",
            "Iteration #86 loss: 0.0407\n",
            "Iteration #87 loss: 0.0399\n",
            "Iteration #88 loss: 0.0421\n",
            "Iteration #89 loss: 0.0164\n",
            "Iteration #90 loss: 0.0490\n",
            "Iteration #91 loss: 0.0233\n",
            "Iteration #92 loss: 0.0210\n",
            "Iteration #93 loss: 0.0430\n",
            "Iteration #94 loss: 0.0187\n",
            "Iteration #95 loss: 0.0194\n",
            "Iteration #96 loss: 0.0359\n",
            "Iteration #97 loss: 0.0280\n",
            "Iteration #98 loss: 0.0199\n",
            "Iteration #99 loss: 0.0209\n",
            "Iteration #100 loss: 0.0298\n",
            "Training Iteration #100 loss: 0.029790486283352065\n",
            "Iteration #101 loss: 0.0519\n",
            "Iteration #102 loss: 0.0207\n",
            "Iteration #103 loss: 0.0861\n",
            "Iteration #104 loss: 0.0189\n",
            "Iteration #105 loss: 0.0235\n",
            "Iteration #106 loss: 0.0187\n",
            "Iteration #107 loss: 0.0146\n",
            "Iteration #108 loss: 0.0187\n",
            "Iteration #109 loss: 0.0254\n",
            "Iteration #110 loss: 0.0175\n",
            "Iteration #111 loss: 0.0204\n",
            "Iteration #112 loss: 0.0182\n",
            "Iteration #113 loss: 0.0218\n",
            "Iteration #114 loss: 0.0507\n",
            "Iteration #115 loss: 0.0105\n",
            "Iteration #116 loss: 0.0406\n",
            "Iteration #117 loss: 0.0279\n",
            "Iteration #118 loss: 0.0453\n",
            "Iteration #119 loss: 0.0227\n",
            "Iteration #120 loss: 0.0311\n",
            "Iteration #121 loss: 0.0874\n",
            "Iteration #122 loss: 0.0329\n",
            "Iteration #123 loss: 0.0272\n",
            "Iteration #124 loss: 0.0667\n",
            "Iteration #125 loss: 0.0976\n",
            "Iteration #126 loss: 0.0236\n",
            "Iteration #127 loss: 0.0268\n",
            "Iteration #128 loss: 0.0172\n",
            "Iteration #129 loss: 0.0299\n",
            "Iteration #130 loss: 0.0296\n",
            "Iteration #131 loss: 0.0344\n",
            "Iteration #132 loss: 0.0469\n",
            "Iteration #133 loss: 0.0523\n",
            "Iteration #134 loss: 0.0367\n",
            "Iteration #135 loss: 0.0350\n",
            "Iteration #136 loss: 0.0386\n",
            "Iteration #137 loss: 0.0594\n",
            "Iteration #138 loss: 0.0873\n",
            "Iteration #139 loss: 0.0269\n",
            "Iteration #140 loss: 0.1135\n",
            "Iteration #141 loss: 0.0205\n",
            "Iteration #142 loss: 0.0286\n",
            "Iteration #143 loss: 0.0277\n",
            "Iteration #144 loss: 0.0206\n",
            "Iteration #145 loss: 0.0192\n",
            "Iteration #146 loss: 0.0338\n",
            "Iteration #147 loss: 0.0619\n",
            "Iteration #148 loss: 0.0304\n",
            "Iteration #149 loss: 0.0294\n",
            "Iteration #150 loss: 0.0230\n",
            "Training Iteration #150 loss: 0.023031705635212407\n",
            "Iteration #151 loss: 0.0345\n",
            "Iteration #152 loss: 0.0152\n",
            "Iteration #153 loss: 0.0256\n",
            "Iteration #154 loss: 0.0207\n",
            "Iteration #155 loss: 0.0284\n",
            "Iteration #156 loss: 0.0181\n",
            "Iteration #157 loss: 0.0377\n",
            "Iteration #158 loss: 0.0272\n",
            "Iteration #159 loss: 0.0252\n",
            "Iteration #160 loss: 0.0432\n",
            "Iteration #161 loss: 0.0571\n",
            "Iteration #162 loss: 0.0738\n",
            "Iteration #163 loss: 0.0254\n",
            "Iteration #164 loss: 0.0284\n",
            "Iteration #165 loss: 0.0234\n",
            "Iteration #166 loss: 0.0139\n",
            "Iteration #167 loss: 0.0272\n",
            "Iteration #168 loss: 0.0404\n",
            "Iteration #169 loss: 0.0180\n",
            "Iteration #170 loss: 0.0532\n",
            "Iteration #171 loss: 0.0418\n",
            "Iteration #172 loss: 0.0228\n",
            "Iteration #173 loss: 0.0911\n",
            "Iteration #174 loss: 0.0373\n",
            "Iteration #175 loss: 0.0304\n",
            "Iteration #176 loss: 0.0433\n",
            "Iteration #177 loss: 0.0271\n",
            "Iteration #178 loss: 0.0323\n",
            "Iteration #179 loss: 0.0264\n",
            "Iteration #180 loss: 0.0241\n",
            "Iteration #181 loss: 0.0359\n",
            "Iteration #182 loss: 0.0322\n",
            "Iteration #183 loss: 0.0538\n",
            "Iteration #184 loss: 0.0303\n",
            "Iteration #185 loss: 0.0336\n",
            "Iteration #186 loss: 0.0198\n",
            "Iteration #187 loss: 0.0301\n",
            "Iteration #188 loss: 0.0191\n",
            "Iteration #189 loss: 0.0448\n",
            "Iteration #190 loss: 0.0163\n",
            "Iteration #191 loss: 0.0335\n",
            "Iteration #192 loss: 0.0285\n",
            "Iteration #193 loss: 0.0107\n",
            "Iteration #194 loss: 0.0152\n",
            "Iteration #195 loss: 0.0251\n",
            "Iteration #196 loss: 0.0263\n",
            "Iteration #197 loss: 0.0217\n",
            "Iteration #198 loss: 0.0377\n",
            "Iteration #199 loss: 0.0189\n",
            "Iteration #200 loss: 0.0208\n",
            "Training Iteration #200 loss: 0.02079591868829592\n",
            "Iteration #201 loss: 0.0224\n",
            "Iteration #202 loss: 0.0250\n",
            "Iteration #203 loss: 0.0179\n",
            "Iteration #204 loss: 0.0159\n",
            "Iteration #205 loss: 0.0257\n",
            "Iteration #206 loss: 0.0181\n",
            "Iteration #207 loss: 0.0927\n",
            "Iteration #208 loss: 0.0474\n",
            "Iteration #209 loss: 0.0201\n",
            "Iteration #210 loss: 0.0295\n",
            "Iteration #211 loss: 0.0257\n",
            "Iteration #212 loss: 0.0242\n",
            "Iteration #213 loss: 0.0216\n",
            "Iteration #214 loss: 0.0305\n",
            "Iteration #215 loss: 0.0430\n",
            "Iteration #216 loss: 0.0190\n",
            "Iteration #217 loss: 0.0300\n",
            "Iteration #218 loss: 0.0233\n",
            "Iteration #219 loss: 0.0321\n",
            "Iteration #220 loss: 0.0267\n",
            "Iteration #221 loss: 0.0316\n",
            "Iteration #222 loss: 0.0258\n",
            "Iteration #223 loss: 0.0298\n",
            "Iteration #224 loss: 0.0475\n",
            "Iteration #225 loss: 0.0355\n",
            "Iteration #226 loss: 0.1020\n",
            "Iteration #227 loss: 0.0496\n",
            "Iteration #228 loss: 0.0289\n",
            "Iteration #229 loss: 0.0189\n",
            "Iteration #230 loss: 0.1252\n",
            "Iteration #231 loss: 0.0676\n",
            "Iteration #232 loss: 0.0180\n",
            "Iteration #233 loss: 0.0210\n",
            "Iteration #234 loss: 0.0161\n",
            "Iteration #235 loss: 0.0144\n",
            "Iteration #236 loss: 0.0216\n",
            "Iteration #237 loss: 0.0263\n",
            "Iteration #238 loss: 0.0288\n",
            "Iteration #239 loss: 0.0315\n",
            "Iteration #240 loss: 0.0179\n",
            "Iteration #241 loss: 0.0212\n",
            "Iteration #242 loss: 0.0617\n",
            "Iteration #243 loss: 0.0260\n",
            "Iteration #244 loss: 0.0207\n",
            "Iteration #245 loss: 0.0216\n",
            "Iteration #246 loss: 0.0936\n",
            "Iteration #247 loss: 0.0248\n",
            "Iteration #248 loss: 0.0644\n",
            "Iteration #249 loss: 0.0341\n",
            "Iteration #250 loss: 0.0474\n",
            "Training Iteration #250 loss: 0.04738106687461639\n",
            "Iteration #251 loss: 0.0219\n",
            "Iteration #252 loss: 0.0183\n",
            "Iteration #253 loss: 0.0314\n",
            "Epoch #10 Training loss: 0.03401723706367803\n",
            "Epoch #10 Training loss: 0.0340\n",
            "Start validation for Epoch #10...\n",
            "Starting validation for Epoch #10...\n",
            "Iteration #1 loss: 0.0831\n",
            "Iteration #2 loss: 0.0320\n",
            "Iteration #3 loss: 0.1282\n",
            "Iteration #4 loss: 0.0452\n",
            "Iteration #5 loss: 0.0640\n",
            "Iteration #6 loss: 0.0431\n",
            "Iteration #7 loss: 0.0602\n",
            "Iteration #8 loss: 0.0457\n",
            "Iteration #9 loss: 0.0531\n",
            "Iteration #10 loss: 0.0830\n",
            "Iteration #11 loss: 0.0362\n",
            "Iteration #12 loss: 0.0866\n",
            "Iteration #13 loss: 0.0765\n",
            "Iteration #14 loss: 0.0709\n",
            "Iteration #15 loss: 0.1057\n",
            "Iteration #16 loss: 0.0340\n",
            "Iteration #17 loss: 0.0686\n",
            "Epoch #10 Validation loss: 0.0657\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #11...\n",
            "Starting training for Epoch #11...\n",
            "Iteration #1 loss: 0.0237\n",
            "Iteration #2 loss: 0.0212\n",
            "Iteration #3 loss: 0.0267\n",
            "Iteration #4 loss: 0.0286\n",
            "Iteration #5 loss: 0.0185\n",
            "Iteration #6 loss: 0.0204\n",
            "Iteration #7 loss: 0.1844\n",
            "Iteration #8 loss: 0.0278\n",
            "Iteration #9 loss: 0.0673\n",
            "Iteration #10 loss: 0.0151\n",
            "Iteration #11 loss: 0.0291\n",
            "Iteration #12 loss: 0.0167\n",
            "Iteration #13 loss: 0.0278\n",
            "Iteration #14 loss: 0.0201\n",
            "Iteration #15 loss: 0.0278\n",
            "Iteration #16 loss: 0.0330\n",
            "Iteration #17 loss: 0.0223\n",
            "Iteration #18 loss: 0.0164\n",
            "Iteration #19 loss: 0.0368\n",
            "Iteration #20 loss: 0.0244\n",
            "Iteration #21 loss: 0.0198\n",
            "Iteration #22 loss: 0.0853\n",
            "Iteration #23 loss: 0.0178\n",
            "Iteration #24 loss: 0.1113\n",
            "Iteration #25 loss: 0.0303\n",
            "Iteration #26 loss: 0.0243\n",
            "Iteration #27 loss: 0.0219\n",
            "Iteration #28 loss: 0.0333\n",
            "Iteration #29 loss: 0.0167\n",
            "Iteration #30 loss: 0.0367\n",
            "Iteration #31 loss: 0.0149\n",
            "Iteration #32 loss: 0.0275\n",
            "Iteration #33 loss: 0.0168\n",
            "Iteration #34 loss: 0.0293\n",
            "Iteration #35 loss: 0.0274\n",
            "Iteration #36 loss: 0.0319\n",
            "Iteration #37 loss: 0.0457\n",
            "Iteration #38 loss: 0.0187\n",
            "Iteration #39 loss: 0.0578\n",
            "Iteration #40 loss: 0.0616\n",
            "Iteration #41 loss: 0.0593\n",
            "Iteration #42 loss: 0.0473\n",
            "Iteration #43 loss: 0.0202\n",
            "Iteration #44 loss: 0.0271\n",
            "Iteration #45 loss: 0.0148\n",
            "Iteration #46 loss: 0.0205\n",
            "Iteration #47 loss: 0.0573\n",
            "Iteration #48 loss: 0.1732\n",
            "Iteration #49 loss: 0.0185\n",
            "Iteration #50 loss: 0.0630\n",
            "Training Iteration #50 loss: 0.06298844193213776\n",
            "Iteration #51 loss: 0.0470\n",
            "Iteration #52 loss: 0.0179\n",
            "Iteration #53 loss: 0.0322\n",
            "Iteration #54 loss: 0.0373\n",
            "Iteration #55 loss: 0.0592\n",
            "Iteration #56 loss: 0.0454\n",
            "Iteration #57 loss: 0.0260\n",
            "Iteration #58 loss: 0.1532\n",
            "Iteration #59 loss: 0.0268\n",
            "Iteration #60 loss: 0.0632\n",
            "Iteration #61 loss: 0.0287\n",
            "Iteration #62 loss: 0.0926\n",
            "Iteration #63 loss: 0.0452\n",
            "Iteration #64 loss: 0.0158\n",
            "Iteration #65 loss: 0.0182\n",
            "Iteration #66 loss: 0.0196\n",
            "Iteration #67 loss: 0.0399\n",
            "Iteration #68 loss: 0.0246\n",
            "Iteration #69 loss: 0.0307\n",
            "Iteration #70 loss: 0.0192\n",
            "Iteration #71 loss: 0.0626\n",
            "Iteration #72 loss: 0.0407\n",
            "Iteration #73 loss: 0.0377\n",
            "Iteration #74 loss: 0.0256\n",
            "Iteration #75 loss: 0.0376\n",
            "Iteration #76 loss: 0.0282\n",
            "Iteration #77 loss: 0.0224\n",
            "Iteration #78 loss: 0.0192\n",
            "Iteration #79 loss: 0.0198\n",
            "Iteration #80 loss: 0.0277\n",
            "Iteration #81 loss: 0.0200\n",
            "Iteration #82 loss: 0.0205\n",
            "Iteration #83 loss: 0.0425\n",
            "Iteration #84 loss: 0.0514\n",
            "Iteration #85 loss: 0.0357\n",
            "Iteration #86 loss: 0.0247\n",
            "Iteration #87 loss: 0.0313\n",
            "Iteration #88 loss: 0.0271\n",
            "Iteration #89 loss: 0.0211\n",
            "Iteration #90 loss: 0.0163\n",
            "Iteration #91 loss: 0.0346\n",
            "Iteration #92 loss: 0.0223\n",
            "Iteration #93 loss: 0.0214\n",
            "Iteration #94 loss: 0.0225\n",
            "Iteration #95 loss: 0.0762\n",
            "Iteration #96 loss: 0.0193\n",
            "Iteration #97 loss: 0.0235\n",
            "Iteration #98 loss: 0.0209\n",
            "Iteration #99 loss: 0.0181\n",
            "Iteration #100 loss: 0.0132\n",
            "Training Iteration #100 loss: 0.013203716183780644\n",
            "Iteration #101 loss: 0.0159\n",
            "Iteration #102 loss: 0.0658\n",
            "Iteration #103 loss: 0.0325\n",
            "Iteration #104 loss: 0.0218\n",
            "Iteration #105 loss: 0.0205\n",
            "Iteration #106 loss: 0.1432\n",
            "Iteration #107 loss: 0.0267\n",
            "Iteration #108 loss: 0.0164\n",
            "Iteration #109 loss: 0.0244\n",
            "Iteration #110 loss: 0.0229\n",
            "Iteration #111 loss: 0.0465\n",
            "Iteration #112 loss: 0.0209\n",
            "Iteration #113 loss: 0.0259\n",
            "Iteration #114 loss: 0.0127\n",
            "Iteration #115 loss: 0.0271\n",
            "Iteration #116 loss: 0.0302\n",
            "Iteration #117 loss: 0.0371\n",
            "Iteration #118 loss: 0.0238\n",
            "Iteration #119 loss: 0.0263\n",
            "Iteration #120 loss: 0.0301\n",
            "Iteration #121 loss: 0.0233\n",
            "Iteration #122 loss: 0.0199\n",
            "Iteration #123 loss: 0.0252\n",
            "Iteration #124 loss: 0.0256\n",
            "Iteration #125 loss: 0.0503\n",
            "Iteration #126 loss: 0.0234\n",
            "Iteration #127 loss: 0.0333\n",
            "Iteration #128 loss: 0.0300\n",
            "Iteration #129 loss: 0.0427\n",
            "Iteration #130 loss: 0.0351\n",
            "Iteration #131 loss: 0.0250\n",
            "Iteration #132 loss: 0.0644\n",
            "Iteration #133 loss: 0.0160\n",
            "Iteration #134 loss: 0.0224\n",
            "Iteration #135 loss: 0.0376\n",
            "Iteration #136 loss: 0.0168\n",
            "Iteration #137 loss: 0.0272\n",
            "Iteration #138 loss: 0.0313\n",
            "Iteration #139 loss: 0.0871\n",
            "Iteration #140 loss: 0.0205\n",
            "Iteration #141 loss: 0.0234\n",
            "Iteration #142 loss: 0.0219\n",
            "Iteration #143 loss: 0.0812\n",
            "Iteration #144 loss: 0.0436\n",
            "Iteration #145 loss: 0.0697\n",
            "Iteration #146 loss: 0.0201\n",
            "Iteration #147 loss: 0.0889\n",
            "Iteration #148 loss: 0.0236\n",
            "Iteration #149 loss: 0.0262\n",
            "Iteration #150 loss: 0.0157\n",
            "Training Iteration #150 loss: 0.01569195055836266\n",
            "Iteration #151 loss: 0.0161\n",
            "Iteration #152 loss: 0.0238\n",
            "Iteration #153 loss: 0.0160\n",
            "Iteration #154 loss: 0.0174\n",
            "Iteration #155 loss: 0.0309\n",
            "Iteration #156 loss: 0.0213\n",
            "Iteration #157 loss: 0.0159\n",
            "Iteration #158 loss: 0.0227\n",
            "Iteration #159 loss: 0.0243\n",
            "Iteration #160 loss: 0.0471\n",
            "Iteration #161 loss: 0.0282\n",
            "Iteration #162 loss: 0.0145\n",
            "Iteration #163 loss: 0.0257\n",
            "Iteration #164 loss: 0.0221\n",
            "Iteration #165 loss: 0.0374\n",
            "Iteration #166 loss: 0.0200\n",
            "Iteration #167 loss: 0.0284\n",
            "Iteration #168 loss: 0.0260\n",
            "Iteration #169 loss: 0.0200\n",
            "Iteration #170 loss: 0.0227\n",
            "Iteration #171 loss: 0.0137\n",
            "Iteration #172 loss: 0.0182\n",
            "Iteration #173 loss: 0.0214\n",
            "Iteration #174 loss: 0.0560\n",
            "Iteration #175 loss: 0.0162\n",
            "Iteration #176 loss: 0.0291\n",
            "Iteration #177 loss: 0.0544\n",
            "Iteration #178 loss: 0.0243\n",
            "Iteration #179 loss: 0.0279\n",
            "Iteration #180 loss: 0.0157\n",
            "Iteration #181 loss: 0.0215\n",
            "Iteration #182 loss: 0.0205\n",
            "Iteration #183 loss: 0.0195\n",
            "Iteration #184 loss: 0.0204\n",
            "Iteration #185 loss: 0.0323\n",
            "Iteration #186 loss: 0.0193\n",
            "Iteration #187 loss: 0.0263\n",
            "Iteration #188 loss: 0.0180\n",
            "Iteration #189 loss: 0.0252\n",
            "Iteration #190 loss: 0.0192\n",
            "Iteration #191 loss: 0.0483\n",
            "Iteration #192 loss: 0.0145\n",
            "Iteration #193 loss: 0.0206\n",
            "Iteration #194 loss: 0.0187\n",
            "Iteration #195 loss: 0.0288\n",
            "Iteration #196 loss: 0.0282\n",
            "Iteration #197 loss: 0.0217\n",
            "Iteration #198 loss: 0.0593\n",
            "Iteration #199 loss: 0.0768\n",
            "Iteration #200 loss: 0.0179\n",
            "Training Iteration #200 loss: 0.01788875599845097\n",
            "Iteration #201 loss: 0.0255\n",
            "Iteration #202 loss: 0.0309\n",
            "Iteration #203 loss: 0.0518\n",
            "Iteration #204 loss: 0.0126\n",
            "Iteration #205 loss: 0.0305\n",
            "Iteration #206 loss: 0.0178\n",
            "Iteration #207 loss: 0.0337\n",
            "Iteration #208 loss: 0.0200\n",
            "Iteration #209 loss: 0.0178\n",
            "Iteration #210 loss: 0.0225\n",
            "Iteration #211 loss: 0.0175\n",
            "Iteration #212 loss: 0.0417\n",
            "Iteration #213 loss: 0.0591\n",
            "Iteration #214 loss: 0.0551\n",
            "Iteration #215 loss: 0.0152\n",
            "Iteration #216 loss: 0.0124\n",
            "Iteration #217 loss: 0.0287\n",
            "Iteration #218 loss: 0.0213\n",
            "Iteration #219 loss: 0.0171\n",
            "Iteration #220 loss: 0.0540\n",
            "Iteration #221 loss: 0.0223\n",
            "Iteration #222 loss: 0.0113\n",
            "Iteration #223 loss: 0.0184\n",
            "Iteration #224 loss: 0.0195\n",
            "Iteration #225 loss: 0.0420\n",
            "Iteration #226 loss: 0.0296\n",
            "Iteration #227 loss: 0.0215\n",
            "Iteration #228 loss: 0.0396\n",
            "Iteration #229 loss: 0.0148\n",
            "Iteration #230 loss: 0.0207\n",
            "Iteration #231 loss: 0.0297\n",
            "Iteration #232 loss: 0.0233\n",
            "Iteration #233 loss: 0.0369\n",
            "Iteration #234 loss: 0.0334\n",
            "Iteration #235 loss: 0.0167\n",
            "Iteration #236 loss: 0.0331\n",
            "Iteration #237 loss: 0.0178\n",
            "Iteration #238 loss: 0.0191\n",
            "Iteration #239 loss: 0.0238\n",
            "Iteration #240 loss: 0.0207\n",
            "Iteration #241 loss: 0.0226\n",
            "Iteration #242 loss: 0.0122\n",
            "Iteration #243 loss: 0.0183\n",
            "Iteration #244 loss: 0.0260\n",
            "Iteration #245 loss: 0.0204\n",
            "Iteration #246 loss: 0.0288\n",
            "Iteration #247 loss: 0.0193\n",
            "Iteration #248 loss: 0.0233\n",
            "Iteration #249 loss: 0.0190\n",
            "Iteration #250 loss: 0.0303\n",
            "Training Iteration #250 loss: 0.03030261988334177\n",
            "Iteration #251 loss: 0.0222\n",
            "Iteration #252 loss: 0.0195\n",
            "Iteration #253 loss: 0.0194\n",
            "Epoch #11 Training loss: 0.03186656766371932\n",
            "Epoch #11 Training loss: 0.0319\n",
            "Start validation for Epoch #11...\n",
            "Starting validation for Epoch #11...\n",
            "Iteration #1 loss: 0.0751\n",
            "Iteration #2 loss: 0.0285\n",
            "Iteration #3 loss: 0.1020\n",
            "Iteration #4 loss: 0.0380\n",
            "Iteration #5 loss: 0.0470\n",
            "Iteration #6 loss: 0.0424\n",
            "Iteration #7 loss: 0.0596\n",
            "Iteration #8 loss: 0.0420\n",
            "Iteration #9 loss: 0.0432\n",
            "Iteration #10 loss: 0.0694\n",
            "Iteration #11 loss: 0.0329\n",
            "Iteration #12 loss: 0.0718\n",
            "Iteration #13 loss: 0.0770\n",
            "Iteration #14 loss: 0.0747\n",
            "Iteration #15 loss: 0.0898\n",
            "Iteration #16 loss: 0.0355\n",
            "Iteration #17 loss: 0.0644\n",
            "Epoch #11 Validation loss: 0.0584\n",
            "New best validation loss: 0.0584 at epoch #11. Saving model...\n",
            "New best validation loss: 0.0584 at epoch #11. Saving model...\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #12...\n",
            "Starting training for Epoch #12...\n",
            "Iteration #1 loss: 0.0307\n",
            "Iteration #2 loss: 0.0229\n",
            "Iteration #3 loss: 0.0213\n",
            "Iteration #4 loss: 0.0308\n",
            "Iteration #5 loss: 0.0149\n",
            "Iteration #6 loss: 0.0743\n",
            "Iteration #7 loss: 0.0239\n",
            "Iteration #8 loss: 0.0179\n",
            "Iteration #9 loss: 0.0542\n",
            "Iteration #10 loss: 0.0224\n",
            "Iteration #11 loss: 0.0242\n",
            "Iteration #12 loss: 0.0790\n",
            "Iteration #13 loss: 0.0172\n",
            "Iteration #14 loss: 0.0195\n",
            "Iteration #15 loss: 0.0336\n",
            "Iteration #16 loss: 0.0184\n",
            "Iteration #17 loss: 0.0219\n",
            "Iteration #18 loss: 0.0184\n",
            "Iteration #19 loss: 0.0420\n",
            "Iteration #20 loss: 0.0270\n",
            "Iteration #21 loss: 0.0356\n",
            "Iteration #22 loss: 0.0431\n",
            "Iteration #23 loss: 0.0206\n",
            "Iteration #24 loss: 0.0231\n",
            "Iteration #25 loss: 0.0360\n",
            "Iteration #26 loss: 0.0280\n",
            "Iteration #27 loss: 0.0189\n",
            "Iteration #28 loss: 0.0277\n",
            "Iteration #29 loss: 0.0207\n",
            "Iteration #30 loss: 0.0230\n",
            "Iteration #31 loss: 0.0097\n",
            "Iteration #32 loss: 0.0151\n",
            "Iteration #33 loss: 0.0113\n",
            "Iteration #34 loss: 0.1537\n",
            "Iteration #35 loss: 0.0361\n",
            "Iteration #36 loss: 0.0245\n",
            "Iteration #37 loss: 0.0159\n",
            "Iteration #38 loss: 0.0169\n",
            "Iteration #39 loss: 0.0196\n",
            "Iteration #40 loss: 0.0236\n",
            "Iteration #41 loss: 0.0609\n",
            "Iteration #42 loss: 0.0293\n",
            "Iteration #43 loss: 0.0160\n",
            "Iteration #44 loss: 0.0539\n",
            "Iteration #45 loss: 0.0108\n",
            "Iteration #46 loss: 0.0602\n",
            "Iteration #47 loss: 0.0399\n",
            "Iteration #48 loss: 0.0601\n",
            "Iteration #49 loss: 0.0258\n",
            "Iteration #50 loss: 0.0202\n",
            "Training Iteration #50 loss: 0.020247452316932365\n",
            "Iteration #51 loss: 0.0817\n",
            "Iteration #52 loss: 0.0302\n",
            "Iteration #53 loss: 0.0422\n",
            "Iteration #54 loss: 0.0316\n",
            "Iteration #55 loss: 0.0247\n",
            "Iteration #56 loss: 0.0343\n",
            "Iteration #57 loss: 0.0309\n",
            "Iteration #58 loss: 0.0245\n",
            "Iteration #59 loss: 0.0239\n",
            "Iteration #60 loss: 0.0140\n",
            "Iteration #61 loss: 0.0273\n",
            "Iteration #62 loss: 0.0126\n",
            "Iteration #63 loss: 0.0241\n",
            "Iteration #64 loss: 0.0161\n",
            "Iteration #65 loss: 0.0378\n",
            "Iteration #66 loss: 0.0274\n",
            "Iteration #67 loss: 0.0217\n",
            "Iteration #68 loss: 0.0311\n",
            "Iteration #69 loss: 0.0172\n",
            "Iteration #70 loss: 0.0320\n",
            "Iteration #71 loss: 0.0316\n",
            "Iteration #72 loss: 0.0179\n",
            "Iteration #73 loss: 0.0264\n",
            "Iteration #74 loss: 0.0430\n",
            "Iteration #75 loss: 0.0746\n",
            "Iteration #76 loss: 0.0176\n",
            "Iteration #77 loss: 0.0199\n",
            "Iteration #78 loss: 0.0179\n",
            "Iteration #79 loss: 0.0162\n",
            "Iteration #80 loss: 0.0224\n",
            "Iteration #81 loss: 0.0246\n",
            "Iteration #82 loss: 0.0687\n",
            "Iteration #83 loss: 0.0252\n",
            "Iteration #84 loss: 0.0197\n",
            "Iteration #85 loss: 0.0188\n",
            "Iteration #86 loss: 0.0257\n",
            "Iteration #87 loss: 0.0965\n",
            "Iteration #88 loss: 0.0194\n",
            "Iteration #89 loss: 0.0522\n",
            "Iteration #90 loss: 0.0242\n",
            "Iteration #91 loss: 0.0153\n",
            "Iteration #92 loss: 0.0229\n",
            "Iteration #93 loss: 0.0394\n",
            "Iteration #94 loss: 0.0191\n",
            "Iteration #95 loss: 0.0246\n",
            "Iteration #96 loss: 0.0222\n",
            "Iteration #97 loss: 0.0172\n",
            "Iteration #98 loss: 0.0155\n",
            "Iteration #99 loss: 0.0459\n",
            "Iteration #100 loss: 0.0195\n",
            "Training Iteration #100 loss: 0.019493372677456752\n",
            "Iteration #101 loss: 0.0158\n",
            "Iteration #102 loss: 0.0269\n",
            "Iteration #103 loss: 0.0180\n",
            "Iteration #104 loss: 0.0186\n",
            "Iteration #105 loss: 0.0184\n",
            "Iteration #106 loss: 0.0173\n",
            "Iteration #107 loss: 0.0515\n",
            "Iteration #108 loss: 0.0154\n",
            "Iteration #109 loss: 0.0451\n",
            "Iteration #110 loss: 0.0159\n",
            "Iteration #111 loss: 0.0196\n",
            "Iteration #112 loss: 0.0262\n",
            "Iteration #113 loss: 0.0212\n",
            "Iteration #114 loss: 0.0649\n",
            "Iteration #115 loss: 0.0395\n",
            "Iteration #116 loss: 0.0206\n",
            "Iteration #117 loss: 0.0146\n",
            "Iteration #118 loss: 0.0218\n",
            "Iteration #119 loss: 0.0395\n",
            "Iteration #120 loss: 0.0181\n",
            "Iteration #121 loss: 0.0233\n",
            "Iteration #122 loss: 0.0358\n",
            "Iteration #123 loss: 0.0272\n",
            "Iteration #124 loss: 0.0188\n",
            "Iteration #125 loss: 0.0206\n",
            "Iteration #126 loss: 0.0572\n",
            "Iteration #127 loss: 0.0386\n",
            "Iteration #128 loss: 0.0276\n",
            "Iteration #129 loss: 0.0205\n",
            "Iteration #130 loss: 0.0366\n",
            "Iteration #131 loss: 0.0261\n",
            "Iteration #132 loss: 0.0241\n",
            "Iteration #133 loss: 0.0413\n",
            "Iteration #134 loss: 0.0214\n",
            "Iteration #135 loss: 0.0391\n",
            "Iteration #136 loss: 0.0230\n",
            "Iteration #137 loss: 0.0298\n",
            "Iteration #138 loss: 0.0321\n",
            "Iteration #139 loss: 0.0191\n",
            "Iteration #140 loss: 0.0263\n",
            "Iteration #141 loss: 0.0542\n",
            "Iteration #142 loss: 0.0168\n",
            "Iteration #143 loss: 0.0307\n",
            "Iteration #144 loss: 0.0163\n",
            "Iteration #145 loss: 0.0154\n",
            "Iteration #146 loss: 0.0219\n",
            "Iteration #147 loss: 0.0200\n",
            "Iteration #148 loss: 0.0822\n",
            "Iteration #149 loss: 0.0182\n",
            "Iteration #150 loss: 0.0175\n",
            "Training Iteration #150 loss: 0.01753746131820721\n",
            "Iteration #151 loss: 0.0288\n",
            "Iteration #152 loss: 0.0233\n",
            "Iteration #153 loss: 0.0335\n",
            "Iteration #154 loss: 0.0243\n",
            "Iteration #155 loss: 0.0773\n",
            "Iteration #156 loss: 0.0212\n",
            "Iteration #157 loss: 0.0217\n",
            "Iteration #158 loss: 0.0209\n",
            "Iteration #159 loss: 0.0287\n",
            "Iteration #160 loss: 0.0219\n",
            "Iteration #161 loss: 0.0217\n",
            "Iteration #162 loss: 0.0161\n",
            "Iteration #163 loss: 0.0165\n",
            "Iteration #164 loss: 0.0392\n",
            "Iteration #165 loss: 0.0423\n",
            "Iteration #166 loss: 0.0291\n",
            "Iteration #167 loss: 0.0219\n",
            "Iteration #168 loss: 0.0234\n",
            "Iteration #169 loss: 0.0237\n",
            "Iteration #170 loss: 0.0148\n",
            "Iteration #171 loss: 0.0157\n",
            "Iteration #172 loss: 0.0208\n",
            "Iteration #173 loss: 0.0259\n",
            "Iteration #174 loss: 0.0290\n",
            "Iteration #175 loss: 0.0158\n",
            "Iteration #176 loss: 0.0262\n",
            "Iteration #177 loss: 0.0493\n",
            "Iteration #178 loss: 0.0311\n",
            "Iteration #179 loss: 0.0250\n",
            "Iteration #180 loss: 0.0385\n",
            "Iteration #181 loss: 0.0176\n",
            "Iteration #182 loss: 0.0459\n",
            "Iteration #183 loss: 0.0475\n",
            "Iteration #184 loss: 0.0216\n",
            "Iteration #185 loss: 0.0187\n",
            "Iteration #186 loss: 0.0172\n",
            "Iteration #187 loss: 0.0131\n",
            "Iteration #188 loss: 0.0245\n",
            "Iteration #189 loss: 0.0270\n",
            "Iteration #190 loss: 0.0203\n",
            "Iteration #191 loss: 0.0190\n",
            "Iteration #192 loss: 0.0229\n",
            "Iteration #193 loss: 0.0181\n",
            "Iteration #194 loss: 0.0175\n",
            "Iteration #195 loss: 0.0131\n",
            "Iteration #196 loss: 0.0155\n",
            "Iteration #197 loss: 0.0178\n",
            "Iteration #198 loss: 0.0278\n",
            "Iteration #199 loss: 0.0593\n",
            "Iteration #200 loss: 0.0340\n",
            "Training Iteration #200 loss: 0.033982225305566084\n",
            "Iteration #201 loss: 0.0216\n",
            "Iteration #202 loss: 0.0177\n",
            "Iteration #203 loss: 0.0493\n",
            "Iteration #204 loss: 0.0370\n",
            "Iteration #205 loss: 0.0357\n",
            "Iteration #206 loss: 0.0281\n",
            "Iteration #207 loss: 0.0193\n",
            "Iteration #208 loss: 0.0144\n",
            "Iteration #209 loss: 0.0148\n",
            "Iteration #210 loss: 0.0112\n",
            "Iteration #211 loss: 0.0172\n",
            "Iteration #212 loss: 0.0129\n",
            "Iteration #213 loss: 0.0241\n",
            "Iteration #214 loss: 0.0273\n",
            "Iteration #215 loss: 0.0221\n",
            "Iteration #216 loss: 0.0187\n",
            "Iteration #217 loss: 0.0370\n",
            "Iteration #218 loss: 0.0161\n",
            "Iteration #219 loss: 0.0679\n",
            "Iteration #220 loss: 0.1065\n",
            "Iteration #221 loss: 0.0603\n",
            "Iteration #222 loss: 0.0271\n",
            "Iteration #223 loss: 0.0307\n",
            "Iteration #224 loss: 0.0147\n",
            "Iteration #225 loss: 0.0577\n",
            "Iteration #226 loss: 0.0137\n",
            "Iteration #227 loss: 0.0167\n",
            "Iteration #228 loss: 0.0170\n",
            "Iteration #229 loss: 0.0257\n",
            "Iteration #230 loss: 0.0152\n",
            "Iteration #231 loss: 0.0214\n",
            "Iteration #232 loss: 0.0402\n",
            "Iteration #233 loss: 0.0114\n",
            "Iteration #234 loss: 0.0372\n",
            "Iteration #235 loss: 0.0137\n",
            "Iteration #236 loss: 0.0497\n",
            "Iteration #237 loss: 0.0440\n",
            "Iteration #238 loss: 0.0186\n",
            "Iteration #239 loss: 0.0212\n",
            "Iteration #240 loss: 0.0733\n",
            "Iteration #241 loss: 0.0570\n",
            "Iteration #242 loss: 0.0232\n",
            "Iteration #243 loss: 0.0294\n",
            "Iteration #244 loss: 0.0213\n",
            "Iteration #245 loss: 0.0130\n",
            "Iteration #246 loss: 0.0175\n",
            "Iteration #247 loss: 0.0490\n",
            "Iteration #248 loss: 0.0279\n",
            "Iteration #249 loss: 0.0285\n",
            "Iteration #250 loss: 0.0229\n",
            "Training Iteration #250 loss: 0.02289933280733569\n",
            "Iteration #251 loss: 0.0219\n",
            "Iteration #252 loss: 0.1183\n",
            "Iteration #253 loss: 0.1377\n",
            "Epoch #12 Training loss: 0.030026483839213643\n",
            "Epoch #12 Training loss: 0.0300\n",
            "Start validation for Epoch #12...\n",
            "Starting validation for Epoch #12...\n",
            "Iteration #1 loss: 0.0746\n",
            "Iteration #2 loss: 0.0290\n",
            "Iteration #3 loss: 0.1164\n",
            "Iteration #4 loss: 0.0417\n",
            "Iteration #5 loss: 0.0550\n",
            "Iteration #6 loss: 0.0397\n",
            "Iteration #7 loss: 0.0534\n",
            "Iteration #8 loss: 0.0427\n",
            "Iteration #9 loss: 0.0457\n",
            "Iteration #10 loss: 0.0790\n",
            "Iteration #11 loss: 0.0322\n",
            "Iteration #12 loss: 0.0666\n",
            "Iteration #13 loss: 0.0792\n",
            "Iteration #14 loss: 0.0756\n",
            "Iteration #15 loss: 0.0791\n",
            "Iteration #16 loss: 0.0357\n",
            "Iteration #17 loss: 0.0547\n",
            "Epoch #12 Validation loss: 0.0588\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #13...\n",
            "Starting training for Epoch #13...\n",
            "Iteration #1 loss: 0.0307\n",
            "Iteration #2 loss: 0.0219\n",
            "Iteration #3 loss: 0.0292\n",
            "Iteration #4 loss: 0.0666\n",
            "Iteration #5 loss: 0.0222\n",
            "Iteration #6 loss: 0.0260\n",
            "Iteration #7 loss: 0.0248\n",
            "Iteration #8 loss: 0.0223\n",
            "Iteration #9 loss: 0.0176\n",
            "Iteration #10 loss: 0.0323\n",
            "Iteration #11 loss: 0.0344\n",
            "Iteration #12 loss: 0.0242\n",
            "Iteration #13 loss: 0.0195\n",
            "Iteration #14 loss: 0.0335\n",
            "Iteration #15 loss: 0.0215\n",
            "Iteration #16 loss: 0.0811\n",
            "Iteration #17 loss: 0.0295\n",
            "Iteration #18 loss: 0.0219\n",
            "Iteration #19 loss: 0.0329\n",
            "Iteration #20 loss: 0.0229\n",
            "Iteration #21 loss: 0.0299\n",
            "Iteration #22 loss: 0.0189\n",
            "Iteration #23 loss: 0.0378\n",
            "Iteration #24 loss: 0.0282\n",
            "Iteration #25 loss: 0.1524\n",
            "Iteration #26 loss: 0.0203\n",
            "Iteration #27 loss: 0.0363\n",
            "Iteration #28 loss: 0.0839\n",
            "Iteration #29 loss: 0.0307\n",
            "Iteration #30 loss: 0.0349\n",
            "Iteration #31 loss: 0.0335\n",
            "Iteration #32 loss: 0.0285\n",
            "Iteration #33 loss: 0.0401\n",
            "Iteration #34 loss: 0.0232\n",
            "Iteration #35 loss: 0.0198\n",
            "Iteration #36 loss: 0.0196\n",
            "Iteration #37 loss: 0.0171\n",
            "Iteration #38 loss: 0.0308\n",
            "Iteration #39 loss: 0.0286\n",
            "Iteration #40 loss: 0.0537\n",
            "Iteration #41 loss: 0.0238\n",
            "Iteration #42 loss: 0.0165\n",
            "Iteration #43 loss: 0.0663\n",
            "Iteration #44 loss: 0.0279\n",
            "Iteration #45 loss: 0.0199\n",
            "Iteration #46 loss: 0.0097\n",
            "Iteration #47 loss: 0.0240\n",
            "Iteration #48 loss: 0.0250\n",
            "Iteration #49 loss: 0.0193\n",
            "Iteration #50 loss: 0.0207\n",
            "Training Iteration #50 loss: 0.02066793206229469\n",
            "Iteration #51 loss: 0.0625\n",
            "Iteration #52 loss: 0.0222\n",
            "Iteration #53 loss: 0.0147\n",
            "Iteration #54 loss: 0.0204\n",
            "Iteration #55 loss: 0.0198\n",
            "Iteration #56 loss: 0.0275\n",
            "Iteration #57 loss: 0.0211\n",
            "Iteration #58 loss: 0.0269\n",
            "Iteration #59 loss: 0.0395\n",
            "Iteration #60 loss: 0.0659\n",
            "Iteration #61 loss: 0.0132\n",
            "Iteration #62 loss: 0.0531\n",
            "Iteration #63 loss: 0.0200\n",
            "Iteration #64 loss: 0.0141\n",
            "Iteration #65 loss: 0.0243\n",
            "Iteration #66 loss: 0.0705\n",
            "Iteration #67 loss: 0.0200\n",
            "Iteration #68 loss: 0.0181\n",
            "Iteration #69 loss: 0.0260\n",
            "Iteration #70 loss: 0.0167\n",
            "Iteration #71 loss: 0.0182\n",
            "Iteration #72 loss: 0.0168\n",
            "Iteration #73 loss: 0.0182\n",
            "Iteration #74 loss: 0.0133\n",
            "Iteration #75 loss: 0.0109\n",
            "Iteration #76 loss: 0.0127\n",
            "Iteration #77 loss: 0.0749\n",
            "Iteration #78 loss: 0.0321\n",
            "Iteration #79 loss: 0.0261\n",
            "Iteration #80 loss: 0.0189\n",
            "Iteration #81 loss: 0.0237\n",
            "Iteration #82 loss: 0.0379\n",
            "Iteration #83 loss: 0.0213\n",
            "Iteration #84 loss: 0.0216\n",
            "Iteration #85 loss: 0.0211\n",
            "Iteration #86 loss: 0.0397\n",
            "Iteration #87 loss: 0.0348\n",
            "Iteration #88 loss: 0.0138\n",
            "Iteration #89 loss: 0.0193\n",
            "Iteration #90 loss: 0.0311\n",
            "Iteration #91 loss: 0.0239\n",
            "Iteration #92 loss: 0.0724\n",
            "Iteration #93 loss: 0.1014\n",
            "Iteration #94 loss: 0.0169\n",
            "Iteration #95 loss: 0.0825\n",
            "Iteration #96 loss: 0.0144\n",
            "Iteration #97 loss: 0.0223\n",
            "Iteration #98 loss: 0.0183\n",
            "Iteration #99 loss: 0.0151\n",
            "Iteration #100 loss: 0.0210\n",
            "Training Iteration #100 loss: 0.020968860281451243\n",
            "Iteration #101 loss: 0.0298\n",
            "Iteration #102 loss: 0.0289\n",
            "Iteration #103 loss: 0.0216\n",
            "Iteration #104 loss: 0.0189\n",
            "Iteration #105 loss: 0.0125\n",
            "Iteration #106 loss: 0.0242\n",
            "Iteration #107 loss: 0.0220\n",
            "Iteration #108 loss: 0.0147\n",
            "Iteration #109 loss: 0.0218\n",
            "Iteration #110 loss: 0.0295\n",
            "Iteration #111 loss: 0.0164\n",
            "Iteration #112 loss: 0.0180\n",
            "Iteration #113 loss: 0.0178\n",
            "Iteration #114 loss: 0.0238\n",
            "Iteration #115 loss: 0.0277\n",
            "Iteration #116 loss: 0.0125\n",
            "Iteration #117 loss: 0.0353\n",
            "Iteration #118 loss: 0.0443\n",
            "Iteration #119 loss: 0.0241\n",
            "Iteration #120 loss: 0.0218\n",
            "Iteration #121 loss: 0.0205\n",
            "Iteration #122 loss: 0.0258\n",
            "Iteration #123 loss: 0.0170\n",
            "Iteration #124 loss: 0.0243\n",
            "Iteration #125 loss: 0.0165\n",
            "Iteration #126 loss: 0.0123\n",
            "Iteration #127 loss: 0.0139\n",
            "Iteration #128 loss: 0.0965\n",
            "Iteration #129 loss: 0.0207\n",
            "Iteration #130 loss: 0.0366\n",
            "Iteration #131 loss: 0.0186\n",
            "Iteration #132 loss: 0.0349\n",
            "Iteration #133 loss: 0.0228\n",
            "Iteration #134 loss: 0.0794\n",
            "Iteration #135 loss: 0.0140\n",
            "Iteration #136 loss: 0.0204\n",
            "Iteration #137 loss: 0.0125\n",
            "Iteration #138 loss: 0.0228\n",
            "Iteration #139 loss: 0.0353\n",
            "Iteration #140 loss: 0.0634\n",
            "Iteration #141 loss: 0.0164\n",
            "Iteration #142 loss: 0.0209\n",
            "Iteration #143 loss: 0.0162\n",
            "Iteration #144 loss: 0.0493\n",
            "Iteration #145 loss: 0.0237\n",
            "Iteration #146 loss: 0.0547\n",
            "Iteration #147 loss: 0.0601\n",
            "Iteration #148 loss: 0.0346\n",
            "Iteration #149 loss: 0.0335\n",
            "Iteration #150 loss: 0.1048\n",
            "Training Iteration #150 loss: 0.10476831924079963\n",
            "Iteration #151 loss: 0.0230\n",
            "Iteration #152 loss: 0.0168\n",
            "Iteration #153 loss: 0.0222\n",
            "Iteration #154 loss: 0.0415\n",
            "Iteration #155 loss: 0.0205\n",
            "Iteration #156 loss: 0.0231\n",
            "Iteration #157 loss: 0.0159\n",
            "Iteration #158 loss: 0.0752\n",
            "Iteration #159 loss: 0.0289\n",
            "Iteration #160 loss: 0.0265\n",
            "Iteration #161 loss: 0.0211\n",
            "Iteration #162 loss: 0.0345\n",
            "Iteration #163 loss: 0.0161\n",
            "Iteration #164 loss: 0.0303\n",
            "Iteration #165 loss: 0.0387\n",
            "Iteration #166 loss: 0.0197\n",
            "Iteration #167 loss: 0.0231\n",
            "Iteration #168 loss: 0.0273\n",
            "Iteration #169 loss: 0.0308\n",
            "Iteration #170 loss: 0.0302\n",
            "Iteration #171 loss: 0.0152\n",
            "Iteration #172 loss: 0.0146\n",
            "Iteration #173 loss: 0.0304\n",
            "Iteration #174 loss: 0.0170\n",
            "Iteration #175 loss: 0.0184\n",
            "Iteration #176 loss: 0.0217\n",
            "Iteration #177 loss: 0.0175\n",
            "Iteration #178 loss: 0.0127\n",
            "Iteration #179 loss: 0.0207\n",
            "Iteration #180 loss: 0.0208\n",
            "Iteration #181 loss: 0.0204\n",
            "Iteration #182 loss: 0.0240\n",
            "Iteration #183 loss: 0.0174\n",
            "Iteration #184 loss: 0.0138\n",
            "Iteration #185 loss: 0.0320\n",
            "Iteration #186 loss: 0.0133\n",
            "Iteration #187 loss: 0.0235\n",
            "Iteration #188 loss: 0.0177\n",
            "Iteration #189 loss: 0.0277\n",
            "Iteration #190 loss: 0.0191\n",
            "Iteration #191 loss: 0.0207\n",
            "Iteration #192 loss: 0.0147\n",
            "Iteration #193 loss: 0.0211\n",
            "Iteration #194 loss: 0.0211\n",
            "Iteration #195 loss: 0.0194\n",
            "Iteration #196 loss: 0.0255\n",
            "Iteration #197 loss: 0.0214\n",
            "Iteration #198 loss: 0.0164\n",
            "Iteration #199 loss: 0.0571\n",
            "Iteration #200 loss: 0.0282\n",
            "Training Iteration #200 loss: 0.028169945540742744\n",
            "Iteration #201 loss: 0.0189\n",
            "Iteration #202 loss: 0.0319\n",
            "Iteration #203 loss: 0.0342\n",
            "Iteration #204 loss: 0.0216\n",
            "Iteration #205 loss: 0.0198\n",
            "Iteration #206 loss: 0.0160\n",
            "Iteration #207 loss: 0.0141\n",
            "Iteration #208 loss: 0.0963\n",
            "Iteration #209 loss: 0.0309\n",
            "Iteration #210 loss: 0.0165\n",
            "Iteration #211 loss: 0.0294\n",
            "Iteration #212 loss: 0.0154\n",
            "Iteration #213 loss: 0.0262\n",
            "Iteration #214 loss: 0.0133\n",
            "Iteration #215 loss: 0.0277\n",
            "Iteration #216 loss: 0.0872\n",
            "Iteration #217 loss: 0.0251\n",
            "Iteration #218 loss: 0.0383\n",
            "Iteration #219 loss: 0.0528\n",
            "Iteration #220 loss: 0.0466\n",
            "Iteration #221 loss: 0.0278\n",
            "Iteration #222 loss: 0.0373\n",
            "Iteration #223 loss: 0.0159\n",
            "Iteration #224 loss: 0.0333\n",
            "Iteration #225 loss: 0.0160\n",
            "Iteration #226 loss: 0.0240\n",
            "Iteration #227 loss: 0.0216\n",
            "Iteration #228 loss: 0.0381\n",
            "Iteration #229 loss: 0.0270\n",
            "Iteration #230 loss: 0.0266\n",
            "Iteration #231 loss: 0.0241\n",
            "Iteration #232 loss: 0.0162\n",
            "Iteration #233 loss: 0.0258\n",
            "Iteration #234 loss: 0.0165\n",
            "Iteration #235 loss: 0.0207\n",
            "Iteration #236 loss: 0.0157\n",
            "Iteration #237 loss: 0.0177\n",
            "Iteration #238 loss: 0.0197\n",
            "Iteration #239 loss: 0.0195\n",
            "Iteration #240 loss: 0.0164\n",
            "Iteration #241 loss: 0.0552\n",
            "Iteration #242 loss: 0.0172\n",
            "Iteration #243 loss: 0.0174\n",
            "Iteration #244 loss: 0.0957\n",
            "Iteration #245 loss: 0.0180\n",
            "Iteration #246 loss: 0.0127\n",
            "Iteration #247 loss: 0.0121\n",
            "Iteration #248 loss: 0.0197\n",
            "Iteration #249 loss: 0.0650\n",
            "Iteration #250 loss: 0.0206\n",
            "Training Iteration #250 loss: 0.020557986427606124\n",
            "Iteration #251 loss: 0.0473\n",
            "Iteration #252 loss: 0.0128\n",
            "Iteration #253 loss: 0.0245\n",
            "Epoch #13 Training loss: 0.02910678969100652\n",
            "Epoch #13 Training loss: 0.0291\n",
            "Start validation for Epoch #13...\n",
            "Starting validation for Epoch #13...\n",
            "Iteration #1 loss: 0.0782\n",
            "Iteration #2 loss: 0.0276\n",
            "Iteration #3 loss: 0.1064\n",
            "Iteration #4 loss: 0.0452\n",
            "Iteration #5 loss: 0.0470\n",
            "Iteration #6 loss: 0.0417\n",
            "Iteration #7 loss: 0.0574\n",
            "Iteration #8 loss: 0.0419\n",
            "Iteration #9 loss: 0.0407\n",
            "Iteration #10 loss: 0.0778\n",
            "Iteration #11 loss: 0.0377\n",
            "Iteration #12 loss: 0.0677\n",
            "Iteration #13 loss: 0.0983\n",
            "Iteration #14 loss: 0.0787\n",
            "Iteration #15 loss: 0.0791\n",
            "Iteration #16 loss: 0.0352\n",
            "Iteration #17 loss: 0.0711\n",
            "Epoch #13 Validation loss: 0.0607\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #14...\n",
            "Starting training for Epoch #14...\n",
            "Iteration #1 loss: 0.0220\n",
            "Iteration #2 loss: 0.0354\n",
            "Iteration #3 loss: 0.0167\n",
            "Iteration #4 loss: 0.0165\n",
            "Iteration #5 loss: 0.0123\n",
            "Iteration #6 loss: 0.0152\n",
            "Iteration #7 loss: 0.0269\n",
            "Iteration #8 loss: 0.0192\n",
            "Iteration #9 loss: 0.0236\n",
            "Iteration #10 loss: 0.0190\n",
            "Iteration #11 loss: 0.0104\n",
            "Iteration #12 loss: 0.0104\n",
            "Iteration #13 loss: 0.0218\n",
            "Iteration #14 loss: 0.0096\n",
            "Iteration #15 loss: 0.0384\n",
            "Iteration #16 loss: 0.0300\n",
            "Iteration #17 loss: 0.0124\n",
            "Iteration #18 loss: 0.0326\n",
            "Iteration #19 loss: 0.0275\n",
            "Iteration #20 loss: 0.0242\n",
            "Iteration #21 loss: 0.0204\n",
            "Iteration #22 loss: 0.0158\n",
            "Iteration #23 loss: 0.0238\n",
            "Iteration #24 loss: 0.0669\n",
            "Iteration #25 loss: 0.0190\n",
            "Iteration #26 loss: 0.0183\n",
            "Iteration #27 loss: 0.0251\n",
            "Iteration #28 loss: 0.0181\n",
            "Iteration #29 loss: 0.0217\n",
            "Iteration #30 loss: 0.0269\n",
            "Iteration #31 loss: 0.0306\n",
            "Iteration #32 loss: 0.0754\n",
            "Iteration #33 loss: 0.0235\n",
            "Iteration #34 loss: 0.0207\n",
            "Iteration #35 loss: 0.0248\n",
            "Iteration #36 loss: 0.0171\n",
            "Iteration #37 loss: 0.0265\n",
            "Iteration #38 loss: 0.0159\n",
            "Iteration #39 loss: 0.0181\n",
            "Iteration #40 loss: 0.0186\n",
            "Iteration #41 loss: 0.0133\n",
            "Iteration #42 loss: 0.0171\n",
            "Iteration #43 loss: 0.0141\n",
            "Iteration #44 loss: 0.0201\n",
            "Iteration #45 loss: 0.0131\n",
            "Iteration #46 loss: 0.0262\n",
            "Iteration #47 loss: 0.0139\n",
            "Iteration #48 loss: 0.0340\n",
            "Iteration #49 loss: 0.0182\n",
            "Iteration #50 loss: 0.0295\n",
            "Training Iteration #50 loss: 0.029451237104354176\n",
            "Iteration #51 loss: 0.0225\n",
            "Iteration #52 loss: 0.0207\n",
            "Iteration #53 loss: 0.0277\n",
            "Iteration #54 loss: 0.0186\n",
            "Iteration #55 loss: 0.0112\n",
            "Iteration #56 loss: 0.0220\n",
            "Iteration #57 loss: 0.0353\n",
            "Iteration #58 loss: 0.0169\n",
            "Iteration #59 loss: 0.0177\n",
            "Iteration #60 loss: 0.0161\n",
            "Iteration #61 loss: 0.0118\n",
            "Iteration #62 loss: 0.0165\n",
            "Iteration #63 loss: 0.0175\n",
            "Iteration #64 loss: 0.0667\n",
            "Iteration #65 loss: 0.0124\n",
            "Iteration #66 loss: 0.0175\n",
            "Iteration #67 loss: 0.0153\n",
            "Iteration #68 loss: 0.0361\n",
            "Iteration #69 loss: 0.0220\n",
            "Iteration #70 loss: 0.0140\n",
            "Iteration #71 loss: 0.0281\n",
            "Iteration #72 loss: 0.0161\n",
            "Iteration #73 loss: 0.0168\n",
            "Iteration #74 loss: 0.0134\n",
            "Iteration #75 loss: 0.0156\n",
            "Iteration #76 loss: 0.0137\n",
            "Iteration #77 loss: 0.0110\n",
            "Iteration #78 loss: 0.0215\n",
            "Iteration #79 loss: 0.0179\n",
            "Iteration #80 loss: 0.1108\n",
            "Iteration #81 loss: 0.0182\n",
            "Iteration #82 loss: 0.0144\n",
            "Iteration #83 loss: 0.0140\n",
            "Iteration #84 loss: 0.0595\n",
            "Iteration #85 loss: 0.0321\n",
            "Iteration #86 loss: 0.0169\n",
            "Iteration #87 loss: 0.0171\n",
            "Iteration #88 loss: 0.0214\n",
            "Iteration #89 loss: 0.0262\n",
            "Iteration #90 loss: 0.0175\n",
            "Iteration #91 loss: 0.0325\n",
            "Iteration #92 loss: 0.0302\n",
            "Iteration #93 loss: 0.0208\n",
            "Iteration #94 loss: 0.0384\n",
            "Iteration #95 loss: 0.0228\n",
            "Iteration #96 loss: 0.0178\n",
            "Iteration #97 loss: 0.0253\n",
            "Iteration #98 loss: 0.0123\n",
            "Iteration #99 loss: 0.0200\n",
            "Iteration #100 loss: 0.0175\n",
            "Training Iteration #100 loss: 0.017530822554919084\n",
            "Iteration #101 loss: 0.0157\n",
            "Iteration #102 loss: 0.0313\n",
            "Iteration #103 loss: 0.0193\n",
            "Iteration #104 loss: 0.0273\n",
            "Iteration #105 loss: 0.0177\n",
            "Iteration #106 loss: 0.0258\n",
            "Iteration #107 loss: 0.0156\n",
            "Iteration #108 loss: 0.0895\n",
            "Iteration #109 loss: 0.0146\n",
            "Iteration #110 loss: 0.0291\n",
            "Iteration #111 loss: 0.0249\n",
            "Iteration #112 loss: 0.0363\n",
            "Iteration #113 loss: 0.0164\n",
            "Iteration #114 loss: 0.0386\n",
            "Iteration #115 loss: 0.0839\n",
            "Iteration #116 loss: 0.0299\n",
            "Iteration #117 loss: 0.0516\n",
            "Iteration #118 loss: 0.0249\n",
            "Iteration #119 loss: 0.0231\n",
            "Iteration #120 loss: 0.0285\n",
            "Iteration #121 loss: 0.0178\n",
            "Iteration #122 loss: 0.0206\n",
            "Iteration #123 loss: 0.0170\n",
            "Iteration #124 loss: 0.0159\n",
            "Iteration #125 loss: 0.0181\n",
            "Iteration #126 loss: 0.0383\n",
            "Iteration #127 loss: 0.0173\n",
            "Iteration #128 loss: 0.0179\n",
            "Iteration #129 loss: 0.0164\n",
            "Iteration #130 loss: 0.0745\n",
            "Iteration #131 loss: 0.0199\n",
            "Iteration #132 loss: 0.0192\n",
            "Iteration #133 loss: 0.0643\n",
            "Iteration #134 loss: 0.0298\n",
            "Iteration #135 loss: 0.0192\n",
            "Iteration #136 loss: 0.0834\n",
            "Iteration #137 loss: 0.0205\n",
            "Iteration #138 loss: 0.0227\n",
            "Iteration #139 loss: 0.0159\n",
            "Iteration #140 loss: 0.0173\n",
            "Iteration #141 loss: 0.0419\n",
            "Iteration #142 loss: 0.0220\n",
            "Iteration #143 loss: 0.0190\n",
            "Iteration #144 loss: 0.0600\n",
            "Iteration #145 loss: 0.0216\n",
            "Iteration #146 loss: 0.0206\n",
            "Iteration #147 loss: 0.0196\n",
            "Iteration #148 loss: 0.0198\n",
            "Iteration #149 loss: 0.0194\n",
            "Iteration #150 loss: 0.0229\n",
            "Training Iteration #150 loss: 0.022877600961974717\n",
            "Iteration #151 loss: 0.0285\n",
            "Iteration #152 loss: 0.0200\n",
            "Iteration #153 loss: 0.0882\n",
            "Iteration #154 loss: 0.0140\n",
            "Iteration #155 loss: 0.0146\n",
            "Iteration #156 loss: 0.0183\n",
            "Iteration #157 loss: 0.0206\n",
            "Iteration #158 loss: 0.0213\n",
            "Iteration #159 loss: 0.0264\n",
            "Iteration #160 loss: 0.0371\n",
            "Iteration #161 loss: 0.0903\n",
            "Iteration #162 loss: 0.0434\n",
            "Iteration #163 loss: 0.0695\n",
            "Iteration #164 loss: 0.0291\n",
            "Iteration #165 loss: 0.0301\n",
            "Iteration #166 loss: 0.0316\n",
            "Iteration #167 loss: 0.0242\n",
            "Iteration #168 loss: 0.0158\n",
            "Iteration #169 loss: 0.1029\n",
            "Iteration #170 loss: 0.0543\n",
            "Iteration #171 loss: 0.0193\n",
            "Iteration #172 loss: 0.0271\n",
            "Iteration #173 loss: 0.0502\n",
            "Iteration #174 loss: 0.0144\n",
            "Iteration #175 loss: 0.0144\n",
            "Iteration #176 loss: 0.0394\n",
            "Iteration #177 loss: 0.0175\n",
            "Iteration #178 loss: 0.0305\n",
            "Iteration #179 loss: 0.0213\n",
            "Iteration #180 loss: 0.0518\n",
            "Iteration #181 loss: 0.0192\n",
            "Iteration #182 loss: 0.0160\n",
            "Iteration #183 loss: 0.0223\n",
            "Iteration #184 loss: 0.0431\n",
            "Iteration #185 loss: 0.0220\n",
            "Iteration #186 loss: 0.0193\n",
            "Iteration #187 loss: 0.0208\n",
            "Iteration #188 loss: 0.0242\n",
            "Iteration #189 loss: 0.0132\n",
            "Iteration #190 loss: 0.0147\n",
            "Iteration #191 loss: 0.0190\n",
            "Iteration #192 loss: 0.0259\n",
            "Iteration #193 loss: 0.0521\n",
            "Iteration #194 loss: 0.0383\n",
            "Iteration #195 loss: 0.0415\n",
            "Iteration #196 loss: 0.0164\n",
            "Iteration #197 loss: 0.0413\n",
            "Iteration #198 loss: 0.0654\n",
            "Iteration #199 loss: 0.0370\n",
            "Iteration #200 loss: 0.0171\n",
            "Training Iteration #200 loss: 0.017096266251538993\n",
            "Iteration #201 loss: 0.0101\n",
            "Iteration #202 loss: 0.0193\n",
            "Iteration #203 loss: 0.0771\n",
            "Iteration #204 loss: 0.0705\n",
            "Iteration #205 loss: 0.0164\n",
            "Iteration #206 loss: 0.0169\n",
            "Iteration #207 loss: 0.0528\n",
            "Iteration #208 loss: 0.0263\n",
            "Iteration #209 loss: 0.0146\n",
            "Iteration #210 loss: 0.0108\n",
            "Iteration #211 loss: 0.0208\n",
            "Iteration #212 loss: 0.0191\n",
            "Iteration #213 loss: 0.0191\n",
            "Iteration #214 loss: 0.0150\n",
            "Iteration #215 loss: 0.0200\n",
            "Iteration #216 loss: 0.0240\n",
            "Iteration #217 loss: 0.0193\n",
            "Iteration #218 loss: 0.0191\n",
            "Iteration #219 loss: 0.0328\n",
            "Iteration #220 loss: 0.0173\n",
            "Iteration #221 loss: 0.0333\n",
            "Iteration #222 loss: 0.0143\n",
            "Iteration #223 loss: 0.0176\n",
            "Iteration #224 loss: 0.0223\n",
            "Iteration #225 loss: 0.0142\n",
            "Iteration #226 loss: 0.0173\n",
            "Iteration #227 loss: 0.0153\n",
            "Iteration #228 loss: 0.0138\n",
            "Iteration #229 loss: 0.0180\n",
            "Iteration #230 loss: 0.0272\n",
            "Iteration #231 loss: 0.0309\n",
            "Iteration #232 loss: 0.0323\n",
            "Iteration #233 loss: 0.0150\n",
            "Iteration #234 loss: 0.0313\n",
            "Iteration #235 loss: 0.0168\n",
            "Iteration #236 loss: 0.0133\n",
            "Iteration #237 loss: 0.0314\n",
            "Iteration #238 loss: 0.0133\n",
            "Iteration #239 loss: 0.0139\n",
            "Iteration #240 loss: 0.0208\n",
            "Iteration #241 loss: 0.0136\n",
            "Iteration #242 loss: 0.0223\n",
            "Iteration #243 loss: 0.0373\n",
            "Iteration #244 loss: 0.0238\n",
            "Iteration #245 loss: 0.0191\n",
            "Iteration #246 loss: 0.0135\n",
            "Iteration #247 loss: 0.0157\n",
            "Iteration #248 loss: 0.0276\n",
            "Iteration #249 loss: 0.0146\n",
            "Iteration #250 loss: 0.0799\n",
            "Training Iteration #250 loss: 0.07987319905268113\n",
            "Iteration #251 loss: 0.0242\n",
            "Iteration #252 loss: 0.0374\n",
            "Iteration #253 loss: 0.0483\n",
            "Epoch #14 Training loss: 0.026682654530157543\n",
            "Epoch #14 Training loss: 0.0267\n",
            "Start validation for Epoch #14...\n",
            "Starting validation for Epoch #14...\n",
            "Iteration #1 loss: 0.0795\n",
            "Iteration #2 loss: 0.0318\n",
            "Iteration #3 loss: 0.1119\n",
            "Iteration #4 loss: 0.0369\n",
            "Iteration #5 loss: 0.0506\n",
            "Iteration #6 loss: 0.0386\n",
            "Iteration #7 loss: 0.0494\n",
            "Iteration #8 loss: 0.0422\n",
            "Iteration #9 loss: 0.0438\n",
            "Iteration #10 loss: 0.0733\n",
            "Iteration #11 loss: 0.0356\n",
            "Iteration #12 loss: 0.0709\n",
            "Iteration #13 loss: 0.0781\n",
            "Iteration #14 loss: 0.0713\n",
            "Iteration #15 loss: 0.0824\n",
            "Iteration #16 loss: 0.0344\n",
            "Iteration #17 loss: 0.0557\n",
            "Epoch #14 Validation loss: 0.0580\n",
            "New best validation loss: 0.0580 at epoch #14. Saving model...\n",
            "New best validation loss: 0.0580 at epoch #14. Saving model...\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Model weights saved to /content/logs/detection/best_model/best_model_weights.pth\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #15...\n",
            "Starting training for Epoch #15...\n",
            "Iteration #1 loss: 0.0325\n",
            "Iteration #2 loss: 0.0122\n",
            "Iteration #3 loss: 0.0159\n",
            "Iteration #4 loss: 0.0401\n",
            "Iteration #5 loss: 0.0154\n",
            "Iteration #6 loss: 0.0239\n",
            "Iteration #7 loss: 0.0121\n",
            "Iteration #8 loss: 0.0102\n",
            "Iteration #9 loss: 0.0253\n",
            "Iteration #10 loss: 0.0122\n",
            "Iteration #11 loss: 0.0161\n",
            "Iteration #12 loss: 0.0204\n",
            "Iteration #13 loss: 0.0120\n",
            "Iteration #14 loss: 0.0104\n",
            "Iteration #15 loss: 0.0346\n",
            "Iteration #16 loss: 0.0144\n",
            "Iteration #17 loss: 0.0954\n",
            "Iteration #18 loss: 0.0094\n",
            "Iteration #19 loss: 0.0242\n",
            "Iteration #20 loss: 0.0285\n",
            "Iteration #21 loss: 0.1032\n",
            "Iteration #22 loss: 0.1061\n",
            "Iteration #23 loss: 0.0155\n",
            "Iteration #24 loss: 0.0155\n",
            "Iteration #25 loss: 0.0330\n",
            "Iteration #26 loss: 0.0282\n",
            "Iteration #27 loss: 0.0307\n",
            "Iteration #28 loss: 0.0126\n",
            "Iteration #29 loss: 0.1324\n",
            "Iteration #30 loss: 0.0207\n",
            "Iteration #31 loss: 0.0546\n",
            "Iteration #32 loss: 0.0283\n",
            "Iteration #33 loss: 0.0425\n",
            "Iteration #34 loss: 0.0146\n",
            "Iteration #35 loss: 0.0191\n",
            "Iteration #36 loss: 0.0268\n",
            "Iteration #37 loss: 0.0311\n",
            "Iteration #38 loss: 0.0961\n",
            "Iteration #39 loss: 0.0136\n",
            "Iteration #40 loss: 0.0168\n",
            "Iteration #41 loss: 0.0143\n",
            "Iteration #42 loss: 0.0324\n",
            "Iteration #43 loss: 0.0314\n",
            "Iteration #44 loss: 0.0212\n",
            "Iteration #45 loss: 0.0371\n",
            "Iteration #46 loss: 0.0158\n",
            "Iteration #47 loss: 0.0144\n",
            "Iteration #48 loss: 0.0238\n",
            "Iteration #49 loss: 0.0151\n",
            "Iteration #50 loss: 0.0436\n",
            "Training Iteration #50 loss: 0.0436460541324239\n",
            "Iteration #51 loss: 0.0163\n",
            "Iteration #52 loss: 0.0300\n",
            "Iteration #53 loss: 0.0354\n",
            "Iteration #54 loss: 0.0187\n",
            "Iteration #55 loss: 0.0178\n",
            "Iteration #56 loss: 0.0127\n",
            "Iteration #57 loss: 0.0228\n",
            "Iteration #58 loss: 0.0188\n",
            "Iteration #59 loss: 0.0558\n",
            "Iteration #60 loss: 0.0248\n",
            "Iteration #61 loss: 0.0148\n",
            "Iteration #62 loss: 0.0293\n",
            "Iteration #63 loss: 0.0139\n",
            "Iteration #64 loss: 0.0204\n",
            "Iteration #65 loss: 0.0266\n",
            "Iteration #66 loss: 0.0629\n",
            "Iteration #67 loss: 0.0157\n",
            "Iteration #68 loss: 0.0288\n",
            "Iteration #69 loss: 0.0313\n",
            "Iteration #70 loss: 0.0172\n",
            "Iteration #71 loss: 0.0195\n",
            "Iteration #72 loss: 0.0149\n",
            "Iteration #73 loss: 0.0194\n",
            "Iteration #74 loss: 0.0133\n",
            "Iteration #75 loss: 0.0196\n",
            "Iteration #76 loss: 0.0369\n",
            "Iteration #77 loss: 0.0124\n",
            "Iteration #78 loss: 0.0140\n",
            "Iteration #79 loss: 0.0204\n",
            "Iteration #80 loss: 0.0651\n",
            "Iteration #81 loss: 0.0305\n",
            "Iteration #82 loss: 0.0238\n",
            "Iteration #83 loss: 0.0303\n",
            "Iteration #84 loss: 0.0373\n",
            "Iteration #85 loss: 0.0178\n",
            "Iteration #86 loss: 0.0508\n",
            "Iteration #87 loss: 0.0156\n",
            "Iteration #88 loss: 0.0088\n",
            "Iteration #89 loss: 0.0226\n",
            "Iteration #90 loss: 0.0187\n",
            "Iteration #91 loss: 0.0207\n",
            "Iteration #92 loss: 0.0165\n",
            "Iteration #93 loss: 0.0238\n",
            "Iteration #94 loss: 0.0185\n",
            "Iteration #95 loss: 0.0173\n",
            "Iteration #96 loss: 0.0186\n",
            "Iteration #97 loss: 0.0137\n",
            "Iteration #98 loss: 0.0140\n",
            "Iteration #99 loss: 0.0192\n",
            "Iteration #100 loss: 0.0231\n",
            "Training Iteration #100 loss: 0.02312843956068443\n",
            "Iteration #101 loss: 0.0297\n",
            "Iteration #102 loss: 0.0116\n",
            "Iteration #103 loss: 0.0137\n",
            "Iteration #104 loss: 0.0508\n",
            "Iteration #105 loss: 0.0187\n",
            "Iteration #106 loss: 0.0422\n",
            "Iteration #107 loss: 0.0178\n",
            "Iteration #108 loss: 0.0132\n",
            "Iteration #109 loss: 0.0223\n",
            "Iteration #110 loss: 0.0132\n",
            "Iteration #111 loss: 0.0308\n",
            "Iteration #112 loss: 0.0513\n",
            "Iteration #113 loss: 0.0173\n",
            "Iteration #114 loss: 0.0144\n",
            "Iteration #115 loss: 0.0230\n",
            "Iteration #116 loss: 0.0155\n",
            "Iteration #117 loss: 0.0186\n",
            "Iteration #118 loss: 0.0114\n",
            "Iteration #119 loss: 0.0799\n",
            "Iteration #120 loss: 0.0549\n",
            "Iteration #121 loss: 0.0314\n",
            "Iteration #122 loss: 0.0201\n",
            "Iteration #123 loss: 0.0292\n",
            "Iteration #124 loss: 0.0218\n",
            "Iteration #125 loss: 0.0309\n",
            "Iteration #126 loss: 0.0616\n",
            "Iteration #127 loss: 0.0167\n",
            "Iteration #128 loss: 0.0192\n",
            "Iteration #129 loss: 0.0117\n",
            "Iteration #130 loss: 0.0202\n",
            "Iteration #131 loss: 0.0354\n",
            "Iteration #132 loss: 0.0203\n",
            "Iteration #133 loss: 0.0144\n",
            "Iteration #134 loss: 0.0172\n",
            "Iteration #135 loss: 0.0191\n",
            "Iteration #136 loss: 0.0217\n",
            "Iteration #137 loss: 0.0317\n",
            "Iteration #138 loss: 0.0399\n",
            "Iteration #139 loss: 0.0179\n",
            "Iteration #140 loss: 0.0296\n",
            "Iteration #141 loss: 0.0267\n",
            "Iteration #142 loss: 0.0562\n",
            "Iteration #143 loss: 0.0087\n",
            "Iteration #144 loss: 0.0192\n",
            "Iteration #145 loss: 0.0837\n",
            "Iteration #146 loss: 0.0210\n",
            "Iteration #147 loss: 0.0221\n",
            "Iteration #148 loss: 0.0236\n",
            "Iteration #149 loss: 0.0208\n",
            "Iteration #150 loss: 0.0329\n",
            "Training Iteration #150 loss: 0.032911535038001476\n",
            "Iteration #151 loss: 0.0200\n",
            "Iteration #152 loss: 0.0155\n",
            "Iteration #153 loss: 0.0189\n",
            "Iteration #154 loss: 0.0151\n",
            "Iteration #155 loss: 0.0093\n",
            "Iteration #156 loss: 0.0175\n",
            "Iteration #157 loss: 0.0131\n",
            "Iteration #158 loss: 0.0167\n",
            "Iteration #159 loss: 0.0166\n",
            "Iteration #160 loss: 0.0120\n",
            "Iteration #161 loss: 0.0348\n",
            "Iteration #162 loss: 0.0124\n",
            "Iteration #163 loss: 0.0196\n",
            "Iteration #164 loss: 0.0185\n",
            "Iteration #165 loss: 0.0792\n",
            "Iteration #166 loss: 0.0161\n",
            "Iteration #167 loss: 0.0632\n",
            "Iteration #168 loss: 0.0259\n",
            "Iteration #169 loss: 0.0216\n",
            "Iteration #170 loss: 0.0138\n",
            "Iteration #171 loss: 0.0320\n",
            "Iteration #172 loss: 0.0293\n",
            "Iteration #173 loss: 0.0186\n",
            "Iteration #174 loss: 0.0132\n",
            "Iteration #175 loss: 0.0253\n",
            "Iteration #176 loss: 0.0182\n",
            "Iteration #177 loss: 0.0251\n",
            "Iteration #178 loss: 0.0537\n",
            "Iteration #179 loss: 0.0149\n",
            "Iteration #180 loss: 0.0144\n",
            "Iteration #181 loss: 0.0413\n",
            "Iteration #182 loss: 0.0145\n",
            "Iteration #183 loss: 0.0269\n",
            "Iteration #184 loss: 0.0138\n",
            "Iteration #185 loss: 0.0110\n",
            "Iteration #186 loss: 0.0191\n",
            "Iteration #187 loss: 0.0119\n",
            "Iteration #188 loss: 0.0308\n",
            "Iteration #189 loss: 0.0156\n",
            "Iteration #190 loss: 0.0183\n",
            "Iteration #191 loss: 0.0273\n",
            "Iteration #192 loss: 0.0209\n",
            "Iteration #193 loss: 0.0404\n",
            "Iteration #194 loss: 0.0217\n",
            "Iteration #195 loss: 0.0112\n",
            "Iteration #196 loss: 0.0205\n",
            "Iteration #197 loss: 0.0468\n",
            "Iteration #198 loss: 0.0221\n",
            "Iteration #199 loss: 0.0425\n",
            "Iteration #200 loss: 0.0120\n",
            "Training Iteration #200 loss: 0.012018123062275227\n",
            "Iteration #201 loss: 0.0321\n",
            "Iteration #202 loss: 0.0166\n",
            "Iteration #203 loss: 0.0172\n",
            "Iteration #204 loss: 0.0173\n",
            "Iteration #205 loss: 0.0130\n",
            "Iteration #206 loss: 0.0137\n",
            "Iteration #207 loss: 0.0327\n",
            "Iteration #208 loss: 0.0203\n",
            "Iteration #209 loss: 0.0117\n",
            "Iteration #210 loss: 0.0410\n",
            "Iteration #211 loss: 0.0479\n",
            "Iteration #212 loss: 0.0167\n",
            "Iteration #213 loss: 0.0136\n",
            "Iteration #214 loss: 0.0184\n",
            "Iteration #215 loss: 0.0154\n",
            "Iteration #216 loss: 0.0232\n",
            "Iteration #217 loss: 0.0224\n",
            "Iteration #218 loss: 0.0139\n",
            "Iteration #219 loss: 0.0121\n",
            "Iteration #220 loss: 0.0108\n",
            "Iteration #221 loss: 0.0169\n",
            "Iteration #222 loss: 0.0134\n",
            "Iteration #223 loss: 0.0174\n",
            "Iteration #224 loss: 0.0349\n",
            "Iteration #225 loss: 0.0164\n",
            "Iteration #226 loss: 0.0180\n",
            "Iteration #227 loss: 0.0132\n",
            "Iteration #228 loss: 0.0103\n",
            "Iteration #229 loss: 0.0566\n",
            "Iteration #230 loss: 0.0135\n",
            "Iteration #231 loss: 0.0313\n",
            "Iteration #232 loss: 0.0178\n",
            "Iteration #233 loss: 0.0191\n",
            "Iteration #234 loss: 0.0554\n",
            "Iteration #235 loss: 0.0214\n",
            "Iteration #236 loss: 0.0176\n",
            "Iteration #237 loss: 0.0175\n",
            "Iteration #238 loss: 0.0184\n",
            "Iteration #239 loss: 0.0233\n",
            "Iteration #240 loss: 0.0118\n",
            "Iteration #241 loss: 0.0088\n",
            "Iteration #242 loss: 0.0117\n",
            "Iteration #243 loss: 0.0152\n",
            "Iteration #244 loss: 0.0218\n",
            "Iteration #245 loss: 0.0143\n",
            "Iteration #246 loss: 0.0300\n",
            "Iteration #247 loss: 0.0732\n",
            "Iteration #248 loss: 0.0228\n",
            "Iteration #249 loss: 0.0231\n",
            "Iteration #250 loss: 0.0174\n",
            "Training Iteration #250 loss: 0.017350972944220413\n",
            "Iteration #251 loss: 0.0142\n",
            "Iteration #252 loss: 0.0200\n",
            "Iteration #253 loss: 0.0185\n",
            "Epoch #15 Training loss: 0.025452941907390232\n",
            "Epoch #15 Training loss: 0.0255\n",
            "Start validation for Epoch #15...\n",
            "Starting validation for Epoch #15...\n",
            "Iteration #1 loss: 0.0837\n",
            "Iteration #2 loss: 0.0352\n",
            "Iteration #3 loss: 0.1278\n",
            "Iteration #4 loss: 0.0351\n",
            "Iteration #5 loss: 0.0561\n",
            "Iteration #6 loss: 0.0394\n",
            "Iteration #7 loss: 0.0605\n",
            "Iteration #8 loss: 0.0428\n",
            "Iteration #9 loss: 0.0467\n",
            "Iteration #10 loss: 0.0893\n",
            "Iteration #11 loss: 0.0362\n",
            "Iteration #12 loss: 0.0850\n",
            "Iteration #13 loss: 0.0846\n",
            "Iteration #14 loss: 0.0706\n",
            "Iteration #15 loss: 0.0847\n",
            "Iteration #16 loss: 0.0379\n",
            "Iteration #17 loss: 0.1091\n",
            "Epoch #15 Validation loss: 0.0662\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #16...\n",
            "Starting training for Epoch #16...\n",
            "Iteration #1 loss: 0.0204\n",
            "Iteration #2 loss: 0.0187\n",
            "Iteration #3 loss: 0.0155\n",
            "Iteration #4 loss: 0.0221\n",
            "Iteration #5 loss: 0.0128\n",
            "Iteration #6 loss: 0.0318\n",
            "Iteration #7 loss: 0.0180\n",
            "Iteration #8 loss: 0.0158\n",
            "Iteration #9 loss: 0.0244\n",
            "Iteration #10 loss: 0.0136\n",
            "Iteration #11 loss: 0.0258\n",
            "Iteration #12 loss: 0.0123\n",
            "Iteration #13 loss: 0.0145\n",
            "Iteration #14 loss: 0.0150\n",
            "Iteration #15 loss: 0.0194\n",
            "Iteration #16 loss: 0.0325\n",
            "Iteration #17 loss: 0.0152\n",
            "Iteration #18 loss: 0.0113\n",
            "Iteration #19 loss: 0.0217\n",
            "Iteration #20 loss: 0.0275\n",
            "Iteration #21 loss: 0.0461\n",
            "Iteration #22 loss: 0.0228\n",
            "Iteration #23 loss: 0.0234\n",
            "Iteration #24 loss: 0.0134\n",
            "Iteration #25 loss: 0.0169\n",
            "Iteration #26 loss: 0.0172\n",
            "Iteration #27 loss: 0.0121\n",
            "Iteration #28 loss: 0.0220\n",
            "Iteration #29 loss: 0.0602\n",
            "Iteration #30 loss: 0.0247\n",
            "Iteration #31 loss: 0.0173\n",
            "Iteration #32 loss: 0.0158\n",
            "Iteration #33 loss: 0.0240\n",
            "Iteration #34 loss: 0.0148\n",
            "Iteration #35 loss: 0.0105\n",
            "Iteration #36 loss: 0.0205\n",
            "Iteration #37 loss: 0.0153\n",
            "Iteration #38 loss: 0.0208\n",
            "Iteration #39 loss: 0.0116\n",
            "Iteration #40 loss: 0.0216\n",
            "Iteration #41 loss: 0.0567\n",
            "Iteration #42 loss: 0.0149\n",
            "Iteration #43 loss: 0.0151\n",
            "Iteration #44 loss: 0.0165\n",
            "Iteration #45 loss: 0.0146\n",
            "Iteration #46 loss: 0.0116\n",
            "Iteration #47 loss: 0.0338\n",
            "Iteration #48 loss: 0.0146\n",
            "Iteration #49 loss: 0.0170\n",
            "Iteration #50 loss: 0.0185\n",
            "Training Iteration #50 loss: 0.018506951158087558\n",
            "Iteration #51 loss: 0.0124\n",
            "Iteration #52 loss: 0.0202\n",
            "Iteration #53 loss: 0.0146\n",
            "Iteration #54 loss: 0.0142\n",
            "Iteration #55 loss: 0.0240\n",
            "Iteration #56 loss: 0.0250\n",
            "Iteration #57 loss: 0.0219\n",
            "Iteration #58 loss: 0.0520\n",
            "Iteration #59 loss: 0.0157\n",
            "Iteration #60 loss: 0.0594\n",
            "Iteration #61 loss: 0.0110\n",
            "Iteration #62 loss: 0.0191\n",
            "Iteration #63 loss: 0.0197\n",
            "Iteration #64 loss: 0.0261\n",
            "Iteration #65 loss: 0.0198\n",
            "Iteration #66 loss: 0.0218\n",
            "Iteration #67 loss: 0.0174\n",
            "Iteration #68 loss: 0.0159\n",
            "Iteration #69 loss: 0.0202\n",
            "Iteration #70 loss: 0.0817\n",
            "Iteration #71 loss: 0.0226\n",
            "Iteration #72 loss: 0.0262\n",
            "Iteration #73 loss: 0.0253\n",
            "Iteration #74 loss: 0.0200\n",
            "Iteration #75 loss: 0.0159\n",
            "Iteration #76 loss: 0.0164\n",
            "Iteration #77 loss: 0.0442\n",
            "Iteration #78 loss: 0.0313\n",
            "Iteration #79 loss: 0.0171\n",
            "Iteration #80 loss: 0.0193\n",
            "Iteration #81 loss: 0.0157\n",
            "Iteration #82 loss: 0.0317\n",
            "Iteration #83 loss: 0.0209\n",
            "Iteration #84 loss: 0.0281\n",
            "Iteration #85 loss: 0.0358\n",
            "Iteration #86 loss: 0.0534\n",
            "Iteration #87 loss: 0.0177\n",
            "Iteration #88 loss: 0.0211\n",
            "Iteration #89 loss: 0.0221\n",
            "Iteration #90 loss: 0.0487\n",
            "Iteration #91 loss: 0.0147\n",
            "Iteration #92 loss: 0.0736\n",
            "Iteration #93 loss: 0.0169\n",
            "Iteration #94 loss: 0.0190\n",
            "Iteration #95 loss: 0.0322\n",
            "Iteration #96 loss: 0.0165\n",
            "Iteration #97 loss: 0.0146\n",
            "Iteration #98 loss: 0.0295\n",
            "Iteration #99 loss: 0.0227\n",
            "Iteration #100 loss: 0.0186\n",
            "Training Iteration #100 loss: 0.01861977067718154\n",
            "Iteration #101 loss: 0.0247\n",
            "Iteration #102 loss: 0.0227\n",
            "Iteration #103 loss: 0.0238\n",
            "Iteration #104 loss: 0.0286\n",
            "Iteration #105 loss: 0.0396\n",
            "Iteration #106 loss: 0.0166\n",
            "Iteration #107 loss: 0.0329\n",
            "Iteration #108 loss: 0.0353\n",
            "Iteration #109 loss: 0.0151\n",
            "Iteration #110 loss: 0.0274\n",
            "Iteration #111 loss: 0.0206\n",
            "Iteration #112 loss: 0.0125\n",
            "Iteration #113 loss: 0.0129\n",
            "Iteration #114 loss: 0.0182\n",
            "Iteration #115 loss: 0.0681\n",
            "Iteration #116 loss: 0.0197\n",
            "Iteration #117 loss: 0.0136\n",
            "Iteration #118 loss: 0.0161\n",
            "Iteration #119 loss: 0.0158\n",
            "Iteration #120 loss: 0.0314\n",
            "Iteration #121 loss: 0.0139\n",
            "Iteration #122 loss: 0.0333\n",
            "Iteration #123 loss: 0.0161\n",
            "Iteration #124 loss: 0.0191\n",
            "Iteration #125 loss: 0.0164\n",
            "Iteration #126 loss: 0.0142\n",
            "Iteration #127 loss: 0.0203\n",
            "Iteration #128 loss: 0.0210\n",
            "Iteration #129 loss: 0.0170\n",
            "Iteration #130 loss: 0.0217\n",
            "Iteration #131 loss: 0.0174\n",
            "Iteration #132 loss: 0.0194\n",
            "Iteration #133 loss: 0.0259\n",
            "Iteration #134 loss: 0.0202\n",
            "Iteration #135 loss: 0.0086\n",
            "Iteration #136 loss: 0.0297\n",
            "Iteration #137 loss: 0.0148\n",
            "Iteration #138 loss: 0.0423\n",
            "Iteration #139 loss: 0.0194\n",
            "Iteration #140 loss: 0.0099\n",
            "Iteration #141 loss: 0.0136\n",
            "Iteration #142 loss: 0.0240\n",
            "Iteration #143 loss: 0.0195\n",
            "Iteration #144 loss: 0.0328\n",
            "Iteration #145 loss: 0.0220\n",
            "Iteration #146 loss: 0.0145\n",
            "Iteration #147 loss: 0.0166\n",
            "Iteration #148 loss: 0.0626\n",
            "Iteration #149 loss: 0.0323\n",
            "Iteration #150 loss: 0.0197\n",
            "Training Iteration #150 loss: 0.019675746862937667\n",
            "Iteration #151 loss: 0.0198\n",
            "Iteration #152 loss: 0.0172\n",
            "Iteration #153 loss: 0.0196\n",
            "Iteration #154 loss: 0.0171\n",
            "Iteration #155 loss: 0.0240\n",
            "Iteration #156 loss: 0.0199\n",
            "Iteration #157 loss: 0.0218\n",
            "Iteration #158 loss: 0.0106\n",
            "Iteration #159 loss: 0.0124\n",
            "Iteration #160 loss: 0.0126\n",
            "Iteration #161 loss: 0.0153\n",
            "Iteration #162 loss: 0.0182\n",
            "Iteration #163 loss: 0.0139\n",
            "Iteration #164 loss: 0.0299\n",
            "Iteration #165 loss: 0.0897\n",
            "Iteration #166 loss: 0.0517\n",
            "Iteration #167 loss: 0.0227\n",
            "Iteration #168 loss: 0.0501\n",
            "Iteration #169 loss: 0.0361\n",
            "Iteration #170 loss: 0.0220\n",
            "Iteration #171 loss: 0.0662\n",
            "Iteration #172 loss: 0.0229\n",
            "Iteration #173 loss: 0.0256\n",
            "Iteration #174 loss: 0.0690\n",
            "Iteration #175 loss: 0.0203\n",
            "Iteration #176 loss: 0.0145\n",
            "Iteration #177 loss: 0.0249\n",
            "Iteration #178 loss: 0.0159\n",
            "Iteration #179 loss: 0.0316\n",
            "Iteration #180 loss: 0.0150\n",
            "Iteration #181 loss: 0.0152\n",
            "Iteration #182 loss: 0.0178\n",
            "Iteration #183 loss: 0.0113\n",
            "Iteration #184 loss: 0.0383\n",
            "Iteration #185 loss: 0.0495\n",
            "Iteration #186 loss: 0.0126\n",
            "Iteration #187 loss: 0.0354\n",
            "Iteration #188 loss: 0.0174\n",
            "Iteration #189 loss: 0.0449\n",
            "Iteration #190 loss: 0.0196\n",
            "Iteration #191 loss: 0.0155\n",
            "Iteration #192 loss: 0.0183\n",
            "Iteration #193 loss: 0.0311\n",
            "Iteration #194 loss: 0.0154\n",
            "Iteration #195 loss: 0.0218\n",
            "Iteration #196 loss: 0.0529\n",
            "Iteration #197 loss: 0.0361\n",
            "Iteration #198 loss: 0.0222\n",
            "Iteration #199 loss: 0.0429\n",
            "Iteration #200 loss: 0.0233\n",
            "Training Iteration #200 loss: 0.02330107009519014\n",
            "Iteration #201 loss: 0.0129\n",
            "Iteration #202 loss: 0.0217\n",
            "Iteration #203 loss: 0.0248\n",
            "Iteration #204 loss: 0.0180\n",
            "Iteration #205 loss: 0.0173\n",
            "Iteration #206 loss: 0.0461\n",
            "Iteration #207 loss: 0.0114\n",
            "Iteration #208 loss: 0.0117\n",
            "Iteration #209 loss: 0.0311\n",
            "Iteration #210 loss: 0.0652\n",
            "Iteration #211 loss: 0.0169\n",
            "Iteration #212 loss: 0.0166\n",
            "Iteration #213 loss: 0.1144\n",
            "Iteration #214 loss: 0.0142\n",
            "Iteration #215 loss: 0.0325\n",
            "Iteration #216 loss: 0.0231\n",
            "Iteration #217 loss: 0.0124\n",
            "Iteration #218 loss: 0.0155\n",
            "Iteration #219 loss: 0.0154\n",
            "Iteration #220 loss: 0.0230\n",
            "Iteration #221 loss: 0.0220\n",
            "Iteration #222 loss: 0.0203\n",
            "Iteration #223 loss: 0.0230\n",
            "Iteration #224 loss: 0.0523\n",
            "Iteration #225 loss: 0.0163\n",
            "Iteration #226 loss: 0.0857\n",
            "Iteration #227 loss: 0.0176\n",
            "Iteration #228 loss: 0.0214\n",
            "Iteration #229 loss: 0.0205\n",
            "Iteration #230 loss: 0.0141\n",
            "Iteration #231 loss: 0.0244\n",
            "Iteration #232 loss: 0.0308\n",
            "Iteration #233 loss: 0.0362\n",
            "Iteration #234 loss: 0.0196\n",
            "Iteration #235 loss: 0.0128\n",
            "Iteration #236 loss: 0.0212\n",
            "Iteration #237 loss: 0.0278\n",
            "Iteration #238 loss: 0.0196\n",
            "Iteration #239 loss: 0.0237\n",
            "Iteration #240 loss: 0.0171\n",
            "Iteration #241 loss: 0.0335\n",
            "Iteration #242 loss: 0.0197\n",
            "Iteration #243 loss: 0.0187\n",
            "Iteration #244 loss: 0.0212\n",
            "Iteration #245 loss: 0.0169\n",
            "Iteration #246 loss: 0.0708\n",
            "Iteration #247 loss: 0.0186\n",
            "Iteration #248 loss: 0.0168\n",
            "Iteration #249 loss: 0.0174\n",
            "Iteration #250 loss: 0.0158\n",
            "Training Iteration #250 loss: 0.015842607452779548\n",
            "Iteration #251 loss: 0.0269\n",
            "Iteration #252 loss: 0.0210\n",
            "Iteration #253 loss: 0.0168\n",
            "Epoch #16 Training loss: 0.02468714160406821\n",
            "Epoch #16 Training loss: 0.0247\n",
            "Start validation for Epoch #16...\n",
            "Starting validation for Epoch #16...\n",
            "Iteration #1 loss: 0.0809\n",
            "Iteration #2 loss: 0.0288\n",
            "Iteration #3 loss: 0.1305\n",
            "Iteration #4 loss: 0.0413\n",
            "Iteration #5 loss: 0.0479\n",
            "Iteration #6 loss: 0.0427\n",
            "Iteration #7 loss: 0.0557\n",
            "Iteration #8 loss: 0.0386\n",
            "Iteration #9 loss: 0.0438\n",
            "Iteration #10 loss: 0.0791\n",
            "Iteration #11 loss: 0.0348\n",
            "Iteration #12 loss: 0.0624\n",
            "Iteration #13 loss: 0.0875\n",
            "Iteration #14 loss: 0.0729\n",
            "Iteration #15 loss: 0.0791\n",
            "Iteration #16 loss: 0.0350\n",
            "Iteration #17 loss: 0.0597\n",
            "Epoch #16 Validation loss: 0.0600\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #17...\n",
            "Starting training for Epoch #17...\n",
            "Iteration #1 loss: 0.0197\n",
            "Iteration #2 loss: 0.0192\n",
            "Iteration #3 loss: 0.0144\n",
            "Iteration #4 loss: 0.0144\n",
            "Iteration #5 loss: 0.0130\n",
            "Iteration #6 loss: 0.0129\n",
            "Iteration #7 loss: 0.0265\n",
            "Iteration #8 loss: 0.0205\n",
            "Iteration #9 loss: 0.0286\n",
            "Iteration #10 loss: 0.0131\n",
            "Iteration #11 loss: 0.0151\n",
            "Iteration #12 loss: 0.0341\n",
            "Iteration #13 loss: 0.0159\n",
            "Iteration #14 loss: 0.0110\n",
            "Iteration #15 loss: 0.0246\n",
            "Iteration #16 loss: 0.0573\n",
            "Iteration #17 loss: 0.0154\n",
            "Iteration #18 loss: 0.0161\n",
            "Iteration #19 loss: 0.0111\n",
            "Iteration #20 loss: 0.0147\n",
            "Iteration #21 loss: 0.0228\n",
            "Iteration #22 loss: 0.0545\n",
            "Iteration #23 loss: 0.0196\n",
            "Iteration #24 loss: 0.0153\n",
            "Iteration #25 loss: 0.0380\n",
            "Iteration #26 loss: 0.0198\n",
            "Iteration #27 loss: 0.0689\n",
            "Iteration #28 loss: 0.0148\n",
            "Iteration #29 loss: 0.0202\n",
            "Iteration #30 loss: 0.0196\n",
            "Iteration #31 loss: 0.0426\n",
            "Iteration #32 loss: 0.0332\n",
            "Iteration #33 loss: 0.0198\n",
            "Iteration #34 loss: 0.0187\n",
            "Iteration #35 loss: 0.0195\n",
            "Iteration #36 loss: 0.0170\n",
            "Iteration #37 loss: 0.0163\n",
            "Iteration #38 loss: 0.0313\n",
            "Iteration #39 loss: 0.0179\n",
            "Iteration #40 loss: 0.0239\n",
            "Iteration #41 loss: 0.0097\n",
            "Iteration #42 loss: 0.0230\n",
            "Iteration #43 loss: 0.0117\n",
            "Iteration #44 loss: 0.0337\n",
            "Iteration #45 loss: 0.0155\n",
            "Iteration #46 loss: 0.0399\n",
            "Iteration #47 loss: 0.0206\n",
            "Iteration #48 loss: 0.0212\n",
            "Iteration #49 loss: 0.0538\n",
            "Iteration #50 loss: 0.0162\n",
            "Training Iteration #50 loss: 0.016209248210227192\n",
            "Iteration #51 loss: 0.0241\n",
            "Iteration #52 loss: 0.0155\n",
            "Iteration #53 loss: 0.0328\n",
            "Iteration #54 loss: 0.0116\n",
            "Iteration #55 loss: 0.0084\n",
            "Iteration #56 loss: 0.0121\n",
            "Iteration #57 loss: 0.0188\n",
            "Iteration #58 loss: 0.0120\n",
            "Iteration #59 loss: 0.0162\n",
            "Iteration #60 loss: 0.0186\n",
            "Iteration #61 loss: 0.0702\n",
            "Iteration #62 loss: 0.0150\n",
            "Iteration #63 loss: 0.0151\n",
            "Iteration #64 loss: 0.0154\n",
            "Iteration #65 loss: 0.0250\n",
            "Iteration #66 loss: 0.0189\n",
            "Iteration #67 loss: 0.0153\n",
            "Iteration #68 loss: 0.0202\n",
            "Iteration #69 loss: 0.0175\n",
            "Iteration #70 loss: 0.0220\n",
            "Iteration #71 loss: 0.0337\n",
            "Iteration #72 loss: 0.0183\n",
            "Iteration #73 loss: 0.0250\n",
            "Iteration #74 loss: 0.0156\n",
            "Iteration #75 loss: 0.0384\n",
            "Iteration #76 loss: 0.0177\n",
            "Iteration #77 loss: 0.0233\n",
            "Iteration #78 loss: 0.0203\n",
            "Iteration #79 loss: 0.0253\n",
            "Iteration #80 loss: 0.0113\n",
            "Iteration #81 loss: 0.0242\n",
            "Iteration #82 loss: 0.0159\n",
            "Iteration #83 loss: 0.0163\n",
            "Iteration #84 loss: 0.0153\n",
            "Iteration #85 loss: 0.0157\n",
            "Iteration #86 loss: 0.0439\n",
            "Iteration #87 loss: 0.0208\n",
            "Iteration #88 loss: 0.0194\n",
            "Iteration #89 loss: 0.0179\n",
            "Iteration #90 loss: 0.0146\n",
            "Iteration #91 loss: 0.0134\n",
            "Iteration #92 loss: 0.0455\n",
            "Iteration #93 loss: 0.0226\n",
            "Iteration #94 loss: 0.0638\n",
            "Iteration #95 loss: 0.0142\n",
            "Iteration #96 loss: 0.0093\n",
            "Iteration #97 loss: 0.0464\n",
            "Iteration #98 loss: 0.0130\n",
            "Iteration #99 loss: 0.0250\n",
            "Iteration #100 loss: 0.0248\n",
            "Training Iteration #100 loss: 0.024826550402810676\n",
            "Iteration #101 loss: 0.0667\n",
            "Iteration #102 loss: 0.0219\n",
            "Iteration #103 loss: 0.0126\n",
            "Iteration #104 loss: 0.0189\n",
            "Iteration #105 loss: 0.0124\n",
            "Iteration #106 loss: 0.0186\n",
            "Iteration #107 loss: 0.0229\n",
            "Iteration #108 loss: 0.0468\n",
            "Iteration #109 loss: 0.0160\n",
            "Iteration #110 loss: 0.0143\n",
            "Iteration #111 loss: 0.0170\n",
            "Iteration #112 loss: 0.0357\n",
            "Iteration #113 loss: 0.0179\n",
            "Iteration #114 loss: 0.0294\n",
            "Iteration #115 loss: 0.0197\n",
            "Iteration #116 loss: 0.0377\n",
            "Iteration #117 loss: 0.0132\n",
            "Iteration #118 loss: 0.0170\n",
            "Iteration #119 loss: 0.0242\n",
            "Iteration #120 loss: 0.0153\n",
            "Iteration #121 loss: 0.0108\n",
            "Iteration #122 loss: 0.0164\n",
            "Iteration #123 loss: 0.0110\n",
            "Iteration #124 loss: 0.0131\n",
            "Iteration #125 loss: 0.0110\n",
            "Iteration #126 loss: 0.0147\n",
            "Iteration #127 loss: 0.0154\n",
            "Iteration #128 loss: 0.0219\n",
            "Iteration #129 loss: 0.0108\n",
            "Iteration #130 loss: 0.0095\n",
            "Iteration #131 loss: 0.0245\n",
            "Iteration #132 loss: 0.0119\n",
            "Iteration #133 loss: 0.0114\n",
            "Iteration #134 loss: 0.0155\n",
            "Iteration #135 loss: 0.0197\n",
            "Iteration #136 loss: 0.0902\n",
            "Iteration #137 loss: 0.0226\n",
            "Iteration #138 loss: 0.0230\n",
            "Iteration #139 loss: 0.0138\n",
            "Iteration #140 loss: 0.0406\n",
            "Iteration #141 loss: 0.0145\n",
            "Iteration #142 loss: 0.0200\n",
            "Iteration #143 loss: 0.0300\n",
            "Iteration #144 loss: 0.0468\n",
            "Iteration #145 loss: 0.0499\n",
            "Iteration #146 loss: 0.0108\n",
            "Iteration #147 loss: 0.0140\n",
            "Iteration #148 loss: 0.0112\n",
            "Iteration #149 loss: 0.0257\n",
            "Iteration #150 loss: 0.0206\n",
            "Training Iteration #150 loss: 0.0205799124156896\n",
            "Iteration #151 loss: 0.0193\n",
            "Iteration #152 loss: 0.0192\n",
            "Iteration #153 loss: 0.0185\n",
            "Iteration #154 loss: 0.0747\n",
            "Iteration #155 loss: 0.0280\n",
            "Iteration #156 loss: 0.0159\n",
            "Iteration #157 loss: 0.0275\n",
            "Iteration #158 loss: 0.0275\n",
            "Iteration #159 loss: 0.0197\n",
            "Iteration #160 loss: 0.0234\n",
            "Iteration #161 loss: 0.0445\n",
            "Iteration #162 loss: 0.0167\n",
            "Iteration #163 loss: 0.0105\n",
            "Iteration #164 loss: 0.0514\n",
            "Iteration #165 loss: 0.0174\n",
            "Iteration #166 loss: 0.0302\n",
            "Iteration #167 loss: 0.0223\n",
            "Iteration #168 loss: 0.0137\n",
            "Iteration #169 loss: 0.0204\n",
            "Iteration #170 loss: 0.0177\n",
            "Iteration #171 loss: 0.0520\n",
            "Iteration #172 loss: 0.0230\n",
            "Iteration #173 loss: 0.0241\n",
            "Iteration #174 loss: 0.0240\n",
            "Iteration #175 loss: 0.0116\n",
            "Iteration #176 loss: 0.0167\n",
            "Iteration #177 loss: 0.0176\n",
            "Iteration #178 loss: 0.0493\n",
            "Iteration #179 loss: 0.0128\n",
            "Iteration #180 loss: 0.0186\n",
            "Iteration #181 loss: 0.0350\n",
            "Iteration #182 loss: 0.0524\n",
            "Iteration #183 loss: 0.0156\n",
            "Iteration #184 loss: 0.0225\n",
            "Iteration #185 loss: 0.0182\n",
            "Iteration #186 loss: 0.0289\n",
            "Iteration #187 loss: 0.0133\n",
            "Iteration #188 loss: 0.0273\n",
            "Iteration #189 loss: 0.0171\n",
            "Iteration #190 loss: 0.0149\n",
            "Iteration #191 loss: 0.0700\n",
            "Iteration #192 loss: 0.0498\n",
            "Iteration #193 loss: 0.0184\n",
            "Iteration #194 loss: 0.0584\n",
            "Iteration #195 loss: 0.0147\n",
            "Iteration #196 loss: 0.0171\n",
            "Iteration #197 loss: 0.0250\n",
            "Iteration #198 loss: 0.0211\n",
            "Iteration #199 loss: 0.0271\n",
            "Iteration #200 loss: 0.0164\n",
            "Training Iteration #200 loss: 0.01636494355128753\n",
            "Iteration #201 loss: 0.0192\n",
            "Iteration #202 loss: 0.0218\n",
            "Iteration #203 loss: 0.0148\n",
            "Iteration #204 loss: 0.0241\n",
            "Iteration #205 loss: 0.0395\n",
            "Iteration #206 loss: 0.0173\n",
            "Iteration #207 loss: 0.0117\n",
            "Iteration #208 loss: 0.0222\n",
            "Iteration #209 loss: 0.0284\n",
            "Iteration #210 loss: 0.0225\n",
            "Iteration #211 loss: 0.0111\n",
            "Iteration #212 loss: 0.0275\n",
            "Iteration #213 loss: 0.0213\n",
            "Iteration #214 loss: 0.0556\n",
            "Iteration #215 loss: 0.0144\n",
            "Iteration #216 loss: 0.0376\n",
            "Iteration #217 loss: 0.0255\n",
            "Iteration #218 loss: 0.0226\n",
            "Iteration #219 loss: 0.0163\n",
            "Iteration #220 loss: 0.0179\n",
            "Iteration #221 loss: 0.0157\n",
            "Iteration #222 loss: 0.0112\n",
            "Iteration #223 loss: 0.0210\n",
            "Iteration #224 loss: 0.0597\n",
            "Iteration #225 loss: 0.0207\n",
            "Iteration #226 loss: 0.0148\n",
            "Iteration #227 loss: 0.0222\n",
            "Iteration #228 loss: 0.0658\n",
            "Iteration #229 loss: 0.0178\n",
            "Iteration #230 loss: 0.0179\n",
            "Iteration #231 loss: 0.0778\n",
            "Iteration #232 loss: 0.0377\n",
            "Iteration #233 loss: 0.0213\n",
            "Iteration #234 loss: 0.0262\n",
            "Iteration #235 loss: 0.0225\n",
            "Iteration #236 loss: 0.0160\n",
            "Iteration #237 loss: 0.0401\n",
            "Iteration #238 loss: 0.0159\n",
            "Iteration #239 loss: 0.0132\n",
            "Iteration #240 loss: 0.0161\n",
            "Iteration #241 loss: 0.0157\n",
            "Iteration #242 loss: 0.0157\n",
            "Iteration #243 loss: 0.0138\n",
            "Iteration #244 loss: 0.0182\n",
            "Iteration #245 loss: 0.0249\n",
            "Iteration #246 loss: 0.0185\n",
            "Iteration #247 loss: 0.0169\n",
            "Iteration #248 loss: 0.0124\n",
            "Iteration #249 loss: 0.0182\n",
            "Iteration #250 loss: 0.0197\n",
            "Training Iteration #250 loss: 0.019668784492209997\n",
            "Iteration #251 loss: 0.0161\n",
            "Iteration #252 loss: 0.0535\n",
            "Iteration #253 loss: 0.0133\n",
            "Epoch #17 Training loss: 0.02385521190104348\n",
            "Epoch #17 Training loss: 0.0239\n",
            "Start validation for Epoch #17...\n",
            "Starting validation for Epoch #17...\n",
            "Iteration #1 loss: 0.0827\n",
            "Iteration #2 loss: 0.0257\n",
            "Iteration #3 loss: 0.1153\n",
            "Iteration #4 loss: 0.0354\n",
            "Iteration #5 loss: 0.0504\n",
            "Iteration #6 loss: 0.0400\n",
            "Iteration #7 loss: 0.0533\n",
            "Iteration #8 loss: 0.0379\n",
            "Iteration #9 loss: 0.0477\n",
            "Iteration #10 loss: 0.0765\n",
            "Iteration #11 loss: 0.0311\n",
            "Iteration #12 loss: 0.0563\n",
            "Iteration #13 loss: 0.0968\n",
            "Iteration #14 loss: 0.0753\n",
            "Iteration #15 loss: 0.0793\n",
            "Iteration #16 loss: 0.0347\n",
            "Iteration #17 loss: 0.0613\n",
            "Epoch #17 Validation loss: 0.0588\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #18...\n",
            "Starting training for Epoch #18...\n",
            "Iteration #1 loss: 0.0673\n",
            "Iteration #2 loss: 0.0149\n",
            "Iteration #3 loss: 0.0118\n",
            "Iteration #4 loss: 0.0144\n",
            "Iteration #5 loss: 0.0168\n",
            "Iteration #6 loss: 0.0196\n",
            "Iteration #7 loss: 0.0167\n",
            "Iteration #8 loss: 0.0127\n",
            "Iteration #9 loss: 0.0153\n",
            "Iteration #10 loss: 0.0303\n",
            "Iteration #11 loss: 0.0247\n",
            "Iteration #12 loss: 0.0176\n",
            "Iteration #13 loss: 0.0395\n",
            "Iteration #14 loss: 0.0173\n",
            "Iteration #15 loss: 0.0196\n",
            "Iteration #16 loss: 0.0135\n",
            "Iteration #17 loss: 0.0133\n",
            "Iteration #18 loss: 0.0158\n",
            "Iteration #19 loss: 0.0232\n",
            "Iteration #20 loss: 0.0133\n",
            "Iteration #21 loss: 0.0174\n",
            "Iteration #22 loss: 0.0139\n",
            "Iteration #23 loss: 0.0117\n",
            "Iteration #24 loss: 0.0226\n",
            "Iteration #25 loss: 0.0197\n",
            "Iteration #26 loss: 0.0206\n",
            "Iteration #27 loss: 0.0231\n",
            "Iteration #28 loss: 0.0109\n",
            "Iteration #29 loss: 0.0194\n",
            "Iteration #30 loss: 0.0455\n",
            "Iteration #31 loss: 0.0102\n",
            "Iteration #32 loss: 0.0185\n",
            "Iteration #33 loss: 0.0148\n",
            "Iteration #34 loss: 0.0224\n",
            "Iteration #35 loss: 0.0397\n",
            "Iteration #36 loss: 0.0132\n",
            "Iteration #37 loss: 0.0114\n",
            "Iteration #38 loss: 0.0125\n",
            "Iteration #39 loss: 0.0155\n",
            "Iteration #40 loss: 0.0173\n",
            "Iteration #41 loss: 0.0153\n",
            "Iteration #42 loss: 0.0170\n",
            "Iteration #43 loss: 0.0217\n",
            "Iteration #44 loss: 0.0189\n",
            "Iteration #45 loss: 0.0108\n",
            "Iteration #46 loss: 0.0124\n",
            "Iteration #47 loss: 0.0203\n",
            "Iteration #48 loss: 0.0099\n",
            "Iteration #49 loss: 0.0120\n",
            "Iteration #50 loss: 0.0183\n",
            "Training Iteration #50 loss: 0.018273335933957448\n",
            "Iteration #51 loss: 0.0131\n",
            "Iteration #52 loss: 0.0200\n",
            "Iteration #53 loss: 0.0181\n",
            "Iteration #54 loss: 0.0229\n",
            "Iteration #55 loss: 0.0186\n",
            "Iteration #56 loss: 0.0361\n",
            "Iteration #57 loss: 0.0204\n",
            "Iteration #58 loss: 0.0690\n",
            "Iteration #59 loss: 0.0209\n",
            "Iteration #60 loss: 0.0575\n",
            "Iteration #61 loss: 0.0105\n",
            "Iteration #62 loss: 0.0116\n",
            "Iteration #63 loss: 0.0160\n",
            "Iteration #64 loss: 0.0391\n",
            "Iteration #65 loss: 0.0243\n",
            "Iteration #66 loss: 0.0157\n",
            "Iteration #67 loss: 0.0143\n",
            "Iteration #68 loss: 0.0113\n",
            "Iteration #69 loss: 0.0142\n",
            "Iteration #70 loss: 0.0138\n",
            "Iteration #71 loss: 0.0216\n",
            "Iteration #72 loss: 0.0157\n",
            "Iteration #73 loss: 0.0200\n",
            "Iteration #74 loss: 0.0135\n",
            "Iteration #75 loss: 0.0166\n",
            "Iteration #76 loss: 0.0338\n",
            "Iteration #77 loss: 0.0205\n",
            "Iteration #78 loss: 0.0171\n",
            "Iteration #79 loss: 0.0106\n",
            "Iteration #80 loss: 0.0228\n",
            "Iteration #81 loss: 0.0123\n",
            "Iteration #82 loss: 0.0689\n",
            "Iteration #83 loss: 0.0419\n",
            "Iteration #84 loss: 0.0235\n",
            "Iteration #85 loss: 0.0264\n",
            "Iteration #86 loss: 0.0699\n",
            "Iteration #87 loss: 0.0150\n",
            "Iteration #88 loss: 0.0604\n",
            "Iteration #89 loss: 0.0224\n",
            "Iteration #90 loss: 0.0164\n",
            "Iteration #91 loss: 0.0204\n",
            "Iteration #92 loss: 0.0125\n",
            "Iteration #93 loss: 0.0231\n",
            "Iteration #94 loss: 0.0223\n",
            "Iteration #95 loss: 0.0246\n",
            "Iteration #96 loss: 0.0213\n",
            "Iteration #97 loss: 0.0181\n",
            "Iteration #98 loss: 0.0241\n",
            "Iteration #99 loss: 0.0250\n",
            "Iteration #100 loss: 0.0148\n",
            "Training Iteration #100 loss: 0.014788835615034062\n",
            "Iteration #101 loss: 0.0152\n",
            "Iteration #102 loss: 0.0490\n",
            "Iteration #103 loss: 0.0192\n",
            "Iteration #104 loss: 0.0274\n",
            "Iteration #105 loss: 0.0237\n",
            "Iteration #106 loss: 0.0148\n",
            "Iteration #107 loss: 0.0358\n",
            "Iteration #108 loss: 0.0186\n",
            "Iteration #109 loss: 0.0242\n",
            "Iteration #110 loss: 0.0360\n",
            "Iteration #111 loss: 0.0191\n",
            "Iteration #112 loss: 0.0240\n",
            "Iteration #113 loss: 0.0965\n",
            "Iteration #114 loss: 0.0150\n",
            "Iteration #115 loss: 0.0218\n",
            "Iteration #116 loss: 0.0358\n",
            "Iteration #117 loss: 0.0534\n",
            "Iteration #118 loss: 0.0190\n",
            "Iteration #119 loss: 0.0190\n",
            "Iteration #120 loss: 0.0691\n",
            "Iteration #121 loss: 0.0123\n",
            "Iteration #122 loss: 0.0202\n",
            "Iteration #123 loss: 0.0193\n",
            "Iteration #124 loss: 0.0142\n",
            "Iteration #125 loss: 0.0111\n",
            "Iteration #126 loss: 0.0149\n",
            "Iteration #127 loss: 0.0204\n",
            "Iteration #128 loss: 0.0172\n",
            "Iteration #129 loss: 0.0229\n",
            "Iteration #130 loss: 0.0219\n",
            "Iteration #131 loss: 0.0181\n",
            "Iteration #132 loss: 0.0149\n",
            "Iteration #133 loss: 0.0344\n",
            "Iteration #134 loss: 0.0260\n",
            "Iteration #135 loss: 0.0397\n",
            "Iteration #136 loss: 0.0167\n",
            "Iteration #137 loss: 0.0375\n",
            "Iteration #138 loss: 0.0165\n",
            "Iteration #139 loss: 0.0179\n",
            "Iteration #140 loss: 0.0294\n",
            "Iteration #141 loss: 0.0443\n",
            "Iteration #142 loss: 0.0334\n",
            "Iteration #143 loss: 0.0430\n",
            "Iteration #144 loss: 0.0237\n",
            "Iteration #145 loss: 0.0457\n",
            "Iteration #146 loss: 0.0145\n",
            "Iteration #147 loss: 0.0203\n",
            "Iteration #148 loss: 0.0125\n",
            "Iteration #149 loss: 0.0155\n",
            "Iteration #150 loss: 0.0150\n",
            "Training Iteration #150 loss: 0.015044652106410343\n",
            "Iteration #151 loss: 0.0281\n",
            "Iteration #152 loss: 0.0260\n",
            "Iteration #153 loss: 0.0165\n",
            "Iteration #154 loss: 0.0257\n",
            "Iteration #155 loss: 0.0141\n",
            "Iteration #156 loss: 0.0089\n",
            "Iteration #157 loss: 0.0123\n",
            "Iteration #158 loss: 0.0157\n",
            "Iteration #159 loss: 0.0960\n",
            "Iteration #160 loss: 0.0106\n",
            "Iteration #161 loss: 0.0107\n",
            "Iteration #162 loss: 0.0239\n",
            "Iteration #163 loss: 0.0175\n",
            "Iteration #164 loss: 0.0178\n",
            "Iteration #165 loss: 0.0477\n",
            "Iteration #166 loss: 0.0505\n",
            "Iteration #167 loss: 0.0188\n",
            "Iteration #168 loss: 0.0158\n",
            "Iteration #169 loss: 0.0366\n",
            "Iteration #170 loss: 0.0129\n",
            "Iteration #171 loss: 0.0224\n",
            "Iteration #172 loss: 0.0270\n",
            "Iteration #173 loss: 0.0218\n",
            "Iteration #174 loss: 0.0162\n",
            "Iteration #175 loss: 0.0124\n",
            "Iteration #176 loss: 0.0162\n",
            "Iteration #177 loss: 0.0231\n",
            "Iteration #178 loss: 0.0455\n",
            "Iteration #179 loss: 0.0098\n",
            "Iteration #180 loss: 0.0196\n",
            "Iteration #181 loss: 0.0450\n",
            "Iteration #182 loss: 0.0135\n",
            "Iteration #183 loss: 0.0368\n",
            "Iteration #184 loss: 0.0194\n",
            "Iteration #185 loss: 0.0179\n",
            "Iteration #186 loss: 0.0170\n",
            "Iteration #187 loss: 0.0125\n",
            "Iteration #188 loss: 0.0172\n",
            "Iteration #189 loss: 0.0184\n",
            "Iteration #190 loss: 0.0145\n",
            "Iteration #191 loss: 0.0164\n",
            "Iteration #192 loss: 0.0393\n",
            "Iteration #193 loss: 0.0721\n",
            "Iteration #194 loss: 0.0183\n",
            "Iteration #195 loss: 0.0507\n",
            "Iteration #196 loss: 0.0182\n",
            "Iteration #197 loss: 0.0505\n",
            "Iteration #198 loss: 0.0205\n",
            "Iteration #199 loss: 0.0206\n",
            "Iteration #200 loss: 0.0385\n",
            "Training Iteration #200 loss: 0.038492174184587405\n",
            "Iteration #201 loss: 0.0331\n",
            "Iteration #202 loss: 0.0261\n",
            "Iteration #203 loss: 0.0314\n",
            "Iteration #204 loss: 0.0153\n",
            "Iteration #205 loss: 0.0273\n",
            "Iteration #206 loss: 0.0173\n",
            "Iteration #207 loss: 0.0125\n",
            "Iteration #208 loss: 0.0120\n",
            "Iteration #209 loss: 0.0182\n",
            "Iteration #210 loss: 0.0204\n",
            "Iteration #211 loss: 0.0301\n",
            "Iteration #212 loss: 0.0200\n",
            "Iteration #213 loss: 0.0157\n",
            "Iteration #214 loss: 0.0135\n",
            "Iteration #215 loss: 0.0149\n",
            "Iteration #216 loss: 0.0270\n",
            "Iteration #217 loss: 0.0122\n",
            "Iteration #218 loss: 0.0112\n",
            "Iteration #219 loss: 0.0177\n",
            "Iteration #220 loss: 0.1163\n",
            "Iteration #221 loss: 0.0069\n",
            "Iteration #222 loss: 0.0125\n",
            "Iteration #223 loss: 0.0165\n",
            "Iteration #224 loss: 0.0211\n",
            "Iteration #225 loss: 0.0145\n",
            "Iteration #226 loss: 0.0193\n",
            "Iteration #227 loss: 0.0126\n",
            "Iteration #228 loss: 0.0150\n",
            "Iteration #229 loss: 0.0184\n",
            "Iteration #230 loss: 0.0180\n",
            "Iteration #231 loss: 0.0187\n",
            "Iteration #232 loss: 0.0228\n",
            "Iteration #233 loss: 0.0315\n",
            "Iteration #234 loss: 0.0317\n",
            "Iteration #235 loss: 0.0168\n",
            "Iteration #236 loss: 0.0373\n",
            "Iteration #237 loss: 0.0147\n",
            "Iteration #238 loss: 0.0207\n",
            "Iteration #239 loss: 0.0269\n",
            "Iteration #240 loss: 0.0268\n",
            "Iteration #241 loss: 0.0132\n",
            "Iteration #242 loss: 0.0156\n",
            "Iteration #243 loss: 0.0206\n",
            "Iteration #244 loss: 0.0118\n",
            "Iteration #245 loss: 0.0237\n",
            "Iteration #246 loss: 0.0320\n",
            "Iteration #247 loss: 0.0250\n",
            "Iteration #248 loss: 0.0159\n",
            "Iteration #249 loss: 0.0321\n",
            "Iteration #250 loss: 0.0348\n",
            "Training Iteration #250 loss: 0.034765014308579346\n",
            "Iteration #251 loss: 0.0106\n",
            "Iteration #252 loss: 0.0521\n",
            "Iteration #253 loss: 0.0113\n",
            "Epoch #18 Training loss: 0.023630841506707252\n",
            "Epoch #18 Training loss: 0.0236\n",
            "Start validation for Epoch #18...\n",
            "Starting validation for Epoch #18...\n",
            "Iteration #1 loss: 0.0860\n",
            "Iteration #2 loss: 0.0339\n",
            "Iteration #3 loss: 0.1027\n",
            "Iteration #4 loss: 0.0390\n",
            "Iteration #5 loss: 0.0496\n",
            "Iteration #6 loss: 0.0340\n",
            "Iteration #7 loss: 0.0555\n",
            "Iteration #8 loss: 0.0407\n",
            "Iteration #9 loss: 0.0440\n",
            "Iteration #10 loss: 0.0799\n",
            "Iteration #11 loss: 0.0370\n",
            "Iteration #12 loss: 0.0623\n",
            "Iteration #13 loss: 0.0853\n",
            "Iteration #14 loss: 0.0668\n",
            "Iteration #15 loss: 0.0733\n",
            "Iteration #16 loss: 0.0312\n",
            "Iteration #17 loss: 0.0693\n",
            "Epoch #18 Validation loss: 0.0583\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #19...\n",
            "Starting training for Epoch #19...\n",
            "Iteration #1 loss: 0.0384\n",
            "Iteration #2 loss: 0.0179\n",
            "Iteration #3 loss: 0.0083\n",
            "Iteration #4 loss: 0.0159\n",
            "Iteration #5 loss: 0.0140\n",
            "Iteration #6 loss: 0.0121\n",
            "Iteration #7 loss: 0.0119\n",
            "Iteration #8 loss: 0.0104\n",
            "Iteration #9 loss: 0.0103\n",
            "Iteration #10 loss: 0.0142\n",
            "Iteration #11 loss: 0.0196\n",
            "Iteration #12 loss: 0.0147\n",
            "Iteration #13 loss: 0.0192\n",
            "Iteration #14 loss: 0.0553\n",
            "Iteration #15 loss: 0.0134\n",
            "Iteration #16 loss: 0.0134\n",
            "Iteration #17 loss: 0.0337\n",
            "Iteration #18 loss: 0.0107\n",
            "Iteration #19 loss: 0.0156\n",
            "Iteration #20 loss: 0.0195\n",
            "Iteration #21 loss: 0.0202\n",
            "Iteration #22 loss: 0.0309\n",
            "Iteration #23 loss: 0.0178\n",
            "Iteration #24 loss: 0.0106\n",
            "Iteration #25 loss: 0.0534\n",
            "Iteration #26 loss: 0.0128\n",
            "Iteration #27 loss: 0.0236\n",
            "Iteration #28 loss: 0.0168\n",
            "Iteration #29 loss: 0.0130\n",
            "Iteration #30 loss: 0.0206\n",
            "Iteration #31 loss: 0.0153\n",
            "Iteration #32 loss: 0.0133\n",
            "Iteration #33 loss: 0.0166\n",
            "Iteration #34 loss: 0.0113\n",
            "Iteration #35 loss: 0.0164\n",
            "Iteration #36 loss: 0.0226\n",
            "Iteration #37 loss: 0.0128\n",
            "Iteration #38 loss: 0.0318\n",
            "Iteration #39 loss: 0.0175\n",
            "Iteration #40 loss: 0.0431\n",
            "Iteration #41 loss: 0.0260\n",
            "Iteration #42 loss: 0.0464\n",
            "Iteration #43 loss: 0.0317\n",
            "Iteration #44 loss: 0.0139\n",
            "Iteration #45 loss: 0.0112\n",
            "Iteration #46 loss: 0.0071\n",
            "Iteration #47 loss: 0.0158\n",
            "Iteration #48 loss: 0.0134\n",
            "Iteration #49 loss: 0.0117\n",
            "Iteration #50 loss: 0.0164\n",
            "Training Iteration #50 loss: 0.01636047396133206\n",
            "Iteration #51 loss: 0.0222\n",
            "Iteration #52 loss: 0.0241\n",
            "Iteration #53 loss: 0.0139\n",
            "Iteration #54 loss: 0.0220\n",
            "Iteration #55 loss: 0.0202\n",
            "Iteration #56 loss: 0.0386\n",
            "Iteration #57 loss: 0.0155\n",
            "Iteration #58 loss: 0.0143\n",
            "Iteration #59 loss: 0.0107\n",
            "Iteration #60 loss: 0.0220\n",
            "Iteration #61 loss: 0.0127\n",
            "Iteration #62 loss: 0.0232\n",
            "Iteration #63 loss: 0.0155\n",
            "Iteration #64 loss: 0.0167\n",
            "Iteration #65 loss: 0.0102\n",
            "Iteration #66 loss: 0.0085\n",
            "Iteration #67 loss: 0.0135\n",
            "Iteration #68 loss: 0.0222\n",
            "Iteration #69 loss: 0.0107\n",
            "Iteration #70 loss: 0.0297\n",
            "Iteration #71 loss: 0.0174\n",
            "Iteration #72 loss: 0.0156\n",
            "Iteration #73 loss: 0.0154\n",
            "Iteration #74 loss: 0.0197\n",
            "Iteration #75 loss: 0.0272\n",
            "Iteration #76 loss: 0.0177\n",
            "Iteration #77 loss: 0.0147\n",
            "Iteration #78 loss: 0.0145\n",
            "Iteration #79 loss: 0.0177\n",
            "Iteration #80 loss: 0.0116\n",
            "Iteration #81 loss: 0.0136\n",
            "Iteration #82 loss: 0.0183\n",
            "Iteration #83 loss: 0.0103\n",
            "Iteration #84 loss: 0.0128\n",
            "Iteration #85 loss: 0.0117\n",
            "Iteration #86 loss: 0.0408\n",
            "Iteration #87 loss: 0.0115\n",
            "Iteration #88 loss: 0.0260\n",
            "Iteration #89 loss: 0.0092\n",
            "Iteration #90 loss: 0.0223\n",
            "Iteration #91 loss: 0.0303\n",
            "Iteration #92 loss: 0.0127\n",
            "Iteration #93 loss: 0.0329\n",
            "Iteration #94 loss: 0.0316\n",
            "Iteration #95 loss: 0.0466\n",
            "Iteration #96 loss: 0.0219\n",
            "Iteration #97 loss: 0.0138\n",
            "Iteration #98 loss: 0.0203\n",
            "Iteration #99 loss: 0.0319\n",
            "Iteration #100 loss: 0.0134\n",
            "Training Iteration #100 loss: 0.013376152426264828\n",
            "Iteration #101 loss: 0.0199\n",
            "Iteration #102 loss: 0.0141\n",
            "Iteration #103 loss: 0.0133\n",
            "Iteration #104 loss: 0.0322\n",
            "Iteration #105 loss: 0.0241\n",
            "Iteration #106 loss: 0.0108\n",
            "Iteration #107 loss: 0.0226\n",
            "Iteration #108 loss: 0.0100\n",
            "Iteration #109 loss: 0.0177\n",
            "Iteration #110 loss: 0.0172\n",
            "Iteration #111 loss: 0.0126\n",
            "Iteration #112 loss: 0.0194\n",
            "Iteration #113 loss: 0.0193\n",
            "Iteration #114 loss: 0.0124\n",
            "Iteration #115 loss: 0.0340\n",
            "Iteration #116 loss: 0.0192\n",
            "Iteration #117 loss: 0.0225\n",
            "Iteration #118 loss: 0.0133\n",
            "Iteration #119 loss: 0.0092\n",
            "Iteration #120 loss: 0.0140\n",
            "Iteration #121 loss: 0.0091\n",
            "Iteration #122 loss: 0.0279\n",
            "Iteration #123 loss: 0.0173\n",
            "Iteration #124 loss: 0.0217\n",
            "Iteration #125 loss: 0.0185\n",
            "Iteration #126 loss: 0.0522\n",
            "Iteration #127 loss: 0.0144\n",
            "Iteration #128 loss: 0.0165\n",
            "Iteration #129 loss: 0.0226\n",
            "Iteration #130 loss: 0.0226\n",
            "Iteration #131 loss: 0.0244\n",
            "Iteration #132 loss: 0.0114\n",
            "Iteration #133 loss: 0.0883\n",
            "Iteration #134 loss: 0.0141\n",
            "Iteration #135 loss: 0.0349\n",
            "Iteration #136 loss: 0.0182\n",
            "Iteration #137 loss: 0.0262\n",
            "Iteration #138 loss: 0.0556\n",
            "Iteration #139 loss: 0.0177\n",
            "Iteration #140 loss: 0.0199\n",
            "Iteration #141 loss: 0.0172\n",
            "Iteration #142 loss: 0.0218\n",
            "Iteration #143 loss: 0.0182\n",
            "Iteration #144 loss: 0.0366\n",
            "Iteration #145 loss: 0.0153\n",
            "Iteration #146 loss: 0.0157\n",
            "Iteration #147 loss: 0.0174\n",
            "Iteration #148 loss: 0.0179\n",
            "Iteration #149 loss: 0.0229\n",
            "Iteration #150 loss: 0.0109\n",
            "Training Iteration #150 loss: 0.010929371409785957\n",
            "Iteration #151 loss: 0.0135\n",
            "Iteration #152 loss: 0.0807\n",
            "Iteration #153 loss: 0.0383\n",
            "Iteration #154 loss: 0.0228\n",
            "Iteration #155 loss: 0.0169\n",
            "Iteration #156 loss: 0.0231\n",
            "Iteration #157 loss: 0.0290\n",
            "Iteration #158 loss: 0.0219\n",
            "Iteration #159 loss: 0.0181\n",
            "Iteration #160 loss: 0.0409\n",
            "Iteration #161 loss: 0.0158\n",
            "Iteration #162 loss: 0.0118\n",
            "Iteration #163 loss: 0.0184\n",
            "Iteration #164 loss: 0.0169\n",
            "Iteration #165 loss: 0.0148\n",
            "Iteration #166 loss: 0.0207\n",
            "Iteration #167 loss: 0.0208\n",
            "Iteration #168 loss: 0.0201\n",
            "Iteration #169 loss: 0.0328\n",
            "Iteration #170 loss: 0.0595\n",
            "Iteration #171 loss: 0.0793\n",
            "Iteration #172 loss: 0.0644\n",
            "Iteration #173 loss: 0.0308\n",
            "Iteration #174 loss: 0.0282\n",
            "Iteration #175 loss: 0.0346\n",
            "Iteration #176 loss: 0.0238\n",
            "Iteration #177 loss: 0.0174\n",
            "Iteration #178 loss: 0.0302\n",
            "Iteration #179 loss: 0.0210\n",
            "Iteration #180 loss: 0.0631\n",
            "Iteration #181 loss: 0.0249\n",
            "Iteration #182 loss: 0.0131\n",
            "Iteration #183 loss: 0.0101\n",
            "Iteration #184 loss: 0.0190\n",
            "Iteration #185 loss: 0.0120\n",
            "Iteration #186 loss: 0.0234\n",
            "Iteration #187 loss: 0.0127\n",
            "Iteration #188 loss: 0.0960\n",
            "Iteration #189 loss: 0.0123\n",
            "Iteration #190 loss: 0.0119\n",
            "Iteration #191 loss: 0.0378\n",
            "Iteration #192 loss: 0.0232\n",
            "Iteration #193 loss: 0.0130\n",
            "Iteration #194 loss: 0.0143\n",
            "Iteration #195 loss: 0.0228\n",
            "Iteration #196 loss: 0.0176\n",
            "Iteration #197 loss: 0.0143\n",
            "Iteration #198 loss: 0.0187\n",
            "Iteration #199 loss: 0.0245\n",
            "Iteration #200 loss: 0.0125\n",
            "Training Iteration #200 loss: 0.012475147977572213\n",
            "Iteration #201 loss: 0.0315\n",
            "Iteration #202 loss: 0.0210\n",
            "Iteration #203 loss: 0.0495\n",
            "Iteration #204 loss: 0.0303\n",
            "Iteration #205 loss: 0.0189\n",
            "Iteration #206 loss: 0.0754\n",
            "Iteration #207 loss: 0.0203\n",
            "Iteration #208 loss: 0.0385\n",
            "Iteration #209 loss: 0.0233\n",
            "Iteration #210 loss: 0.0153\n",
            "Iteration #211 loss: 0.0202\n",
            "Iteration #212 loss: 0.0326\n",
            "Iteration #213 loss: 0.0313\n",
            "Iteration #214 loss: 0.0172\n",
            "Iteration #215 loss: 0.0186\n",
            "Iteration #216 loss: 0.0104\n",
            "Iteration #217 loss: 0.0106\n",
            "Iteration #218 loss: 0.0476\n",
            "Iteration #219 loss: 0.0154\n",
            "Iteration #220 loss: 0.0508\n",
            "Iteration #221 loss: 0.0232\n",
            "Iteration #222 loss: 0.0182\n",
            "Iteration #223 loss: 0.0125\n",
            "Iteration #224 loss: 0.0173\n",
            "Iteration #225 loss: 0.0156\n",
            "Iteration #226 loss: 0.0085\n",
            "Iteration #227 loss: 0.0177\n",
            "Iteration #228 loss: 0.0197\n",
            "Iteration #229 loss: 0.0173\n",
            "Iteration #230 loss: 0.0193\n",
            "Iteration #231 loss: 0.0199\n",
            "Iteration #232 loss: 0.0152\n",
            "Iteration #233 loss: 0.0161\n",
            "Iteration #234 loss: 0.0143\n",
            "Iteration #235 loss: 0.0211\n",
            "Iteration #236 loss: 0.0176\n",
            "Iteration #237 loss: 0.0193\n",
            "Iteration #238 loss: 0.0136\n",
            "Iteration #239 loss: 0.0137\n",
            "Iteration #240 loss: 0.0422\n",
            "Iteration #241 loss: 0.0117\n",
            "Iteration #242 loss: 0.0082\n",
            "Iteration #243 loss: 0.0144\n",
            "Iteration #244 loss: 0.0177\n",
            "Iteration #245 loss: 0.0147\n",
            "Iteration #246 loss: 0.0221\n",
            "Iteration #247 loss: 0.0319\n",
            "Iteration #248 loss: 0.0149\n",
            "Iteration #249 loss: 0.0182\n",
            "Iteration #250 loss: 0.0132\n",
            "Training Iteration #250 loss: 0.01318584638682331\n",
            "Iteration #251 loss: 0.0513\n",
            "Iteration #252 loss: 0.0207\n",
            "Iteration #253 loss: 0.0125\n",
            "Epoch #19 Training loss: 0.022110993131259145\n",
            "Epoch #19 Training loss: 0.0221\n",
            "Start validation for Epoch #19...\n",
            "Starting validation for Epoch #19...\n",
            "Iteration #1 loss: 0.0705\n",
            "Iteration #2 loss: 0.0292\n",
            "Iteration #3 loss: 0.1241\n",
            "Iteration #4 loss: 0.0418\n",
            "Iteration #5 loss: 0.0455\n",
            "Iteration #6 loss: 0.0416\n",
            "Iteration #7 loss: 0.0566\n",
            "Iteration #8 loss: 0.0420\n",
            "Iteration #9 loss: 0.0450\n",
            "Iteration #10 loss: 0.0791\n",
            "Iteration #11 loss: 0.0343\n",
            "Iteration #12 loss: 0.0702\n",
            "Iteration #13 loss: 0.0836\n",
            "Iteration #14 loss: 0.0799\n",
            "Iteration #15 loss: 0.0916\n",
            "Iteration #16 loss: 0.0306\n",
            "Iteration #17 loss: 0.0610\n",
            "Epoch #19 Validation loss: 0.0604\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n",
            "Starting Epoch #20...\n",
            "Starting training for Epoch #20...\n",
            "Iteration #1 loss: 0.0191\n",
            "Iteration #2 loss: 0.0919\n",
            "Iteration #3 loss: 0.0111\n",
            "Iteration #4 loss: 0.0176\n",
            "Iteration #5 loss: 0.0243\n",
            "Iteration #6 loss: 0.0159\n",
            "Iteration #7 loss: 0.0161\n",
            "Iteration #8 loss: 0.0189\n",
            "Iteration #9 loss: 0.0185\n",
            "Iteration #10 loss: 0.0143\n",
            "Iteration #11 loss: 0.0248\n",
            "Iteration #12 loss: 0.0214\n",
            "Iteration #13 loss: 0.0095\n",
            "Iteration #14 loss: 0.0132\n",
            "Iteration #15 loss: 0.0267\n",
            "Iteration #16 loss: 0.0441\n",
            "Iteration #17 loss: 0.0216\n",
            "Iteration #18 loss: 0.0145\n",
            "Iteration #19 loss: 0.0158\n",
            "Iteration #20 loss: 0.0155\n",
            "Iteration #21 loss: 0.0107\n",
            "Iteration #22 loss: 0.0135\n",
            "Iteration #23 loss: 0.0151\n",
            "Iteration #24 loss: 0.0262\n",
            "Iteration #25 loss: 0.0270\n",
            "Iteration #26 loss: 0.0183\n",
            "Iteration #27 loss: 0.0157\n",
            "Iteration #28 loss: 0.0105\n",
            "Iteration #29 loss: 0.0124\n",
            "Iteration #30 loss: 0.0096\n",
            "Iteration #31 loss: 0.0236\n",
            "Iteration #32 loss: 0.0320\n",
            "Iteration #33 loss: 0.0160\n",
            "Iteration #34 loss: 0.0187\n",
            "Iteration #35 loss: 0.0142\n",
            "Iteration #36 loss: 0.0123\n",
            "Iteration #37 loss: 0.0139\n",
            "Iteration #38 loss: 0.0126\n",
            "Iteration #39 loss: 0.0096\n",
            "Iteration #40 loss: 0.0066\n",
            "Iteration #41 loss: 0.0132\n",
            "Iteration #42 loss: 0.0275\n",
            "Iteration #43 loss: 0.0068\n",
            "Iteration #44 loss: 0.0248\n",
            "Iteration #45 loss: 0.0650\n",
            "Iteration #46 loss: 0.0125\n",
            "Iteration #47 loss: 0.0156\n",
            "Iteration #48 loss: 0.0177\n",
            "Iteration #49 loss: 0.0136\n",
            "Iteration #50 loss: 0.0137\n",
            "Training Iteration #50 loss: 0.013742533043455948\n",
            "Iteration #51 loss: 0.0131\n",
            "Iteration #52 loss: 0.0150\n",
            "Iteration #53 loss: 0.0190\n",
            "Iteration #54 loss: 0.0183\n",
            "Iteration #55 loss: 0.0362\n",
            "Iteration #56 loss: 0.0088\n",
            "Iteration #57 loss: 0.0164\n",
            "Iteration #58 loss: 0.0172\n",
            "Iteration #59 loss: 0.0292\n",
            "Iteration #60 loss: 0.0110\n",
            "Iteration #61 loss: 0.0118\n",
            "Iteration #62 loss: 0.0135\n",
            "Iteration #63 loss: 0.0194\n",
            "Iteration #64 loss: 0.0184\n",
            "Iteration #65 loss: 0.0082\n",
            "Iteration #66 loss: 0.0129\n",
            "Iteration #67 loss: 0.0191\n",
            "Iteration #68 loss: 0.0166\n",
            "Iteration #69 loss: 0.0416\n",
            "Iteration #70 loss: 0.0300\n",
            "Iteration #71 loss: 0.0210\n",
            "Iteration #72 loss: 0.0183\n",
            "Iteration #73 loss: 0.0180\n",
            "Iteration #74 loss: 0.0173\n",
            "Iteration #75 loss: 0.0168\n",
            "Iteration #76 loss: 0.0158\n",
            "Iteration #77 loss: 0.0563\n",
            "Iteration #78 loss: 0.0171\n",
            "Iteration #79 loss: 0.0196\n",
            "Iteration #80 loss: 0.0168\n",
            "Iteration #81 loss: 0.0278\n",
            "Iteration #82 loss: 0.0135\n",
            "Iteration #83 loss: 0.0249\n",
            "Iteration #84 loss: 0.0625\n",
            "Iteration #85 loss: 0.0152\n",
            "Iteration #86 loss: 0.0516\n",
            "Iteration #87 loss: 0.0153\n",
            "Iteration #88 loss: 0.0205\n",
            "Iteration #89 loss: 0.0126\n",
            "Iteration #90 loss: 0.0179\n",
            "Iteration #91 loss: 0.0201\n",
            "Iteration #92 loss: 0.0355\n",
            "Iteration #93 loss: 0.0310\n",
            "Iteration #94 loss: 0.0229\n",
            "Iteration #95 loss: 0.0325\n",
            "Iteration #96 loss: 0.0495\n",
            "Iteration #97 loss: 0.0194\n",
            "Iteration #98 loss: 0.0081\n",
            "Iteration #99 loss: 0.0129\n",
            "Iteration #100 loss: 0.0143\n",
            "Training Iteration #100 loss: 0.014339987489320773\n",
            "Iteration #101 loss: 0.0115\n",
            "Iteration #102 loss: 0.0231\n",
            "Iteration #103 loss: 0.0194\n",
            "Iteration #104 loss: 0.0131\n",
            "Iteration #105 loss: 0.0181\n",
            "Iteration #106 loss: 0.0378\n",
            "Iteration #107 loss: 0.0214\n",
            "Iteration #108 loss: 0.0212\n",
            "Iteration #109 loss: 0.0128\n",
            "Iteration #110 loss: 0.0092\n",
            "Iteration #111 loss: 0.0202\n",
            "Iteration #112 loss: 0.0209\n",
            "Iteration #113 loss: 0.0232\n",
            "Iteration #114 loss: 0.0143\n",
            "Iteration #115 loss: 0.0147\n",
            "Iteration #116 loss: 0.0115\n",
            "Iteration #117 loss: 0.0122\n",
            "Iteration #118 loss: 0.0201\n",
            "Iteration #119 loss: 0.0129\n",
            "Iteration #120 loss: 0.0171\n",
            "Iteration #121 loss: 0.0112\n",
            "Iteration #122 loss: 0.0536\n",
            "Iteration #123 loss: 0.0209\n",
            "Iteration #124 loss: 0.0183\n",
            "Iteration #125 loss: 0.0351\n",
            "Iteration #126 loss: 0.0177\n",
            "Iteration #127 loss: 0.0110\n",
            "Iteration #128 loss: 0.0168\n",
            "Iteration #129 loss: 0.0223\n",
            "Iteration #130 loss: 0.0161\n",
            "Iteration #131 loss: 0.0228\n",
            "Iteration #132 loss: 0.0281\n",
            "Iteration #133 loss: 0.0670\n",
            "Iteration #134 loss: 0.0094\n",
            "Iteration #135 loss: 0.0180\n",
            "Iteration #136 loss: 0.0155\n",
            "Iteration #137 loss: 0.0149\n",
            "Iteration #138 loss: 0.0163\n",
            "Iteration #139 loss: 0.0293\n",
            "Iteration #140 loss: 0.0659\n",
            "Iteration #141 loss: 0.0153\n",
            "Iteration #142 loss: 0.0141\n",
            "Iteration #143 loss: 0.0602\n",
            "Iteration #144 loss: 0.0168\n",
            "Iteration #145 loss: 0.0538\n",
            "Iteration #146 loss: 0.0348\n",
            "Iteration #147 loss: 0.0116\n",
            "Iteration #148 loss: 0.0220\n",
            "Iteration #149 loss: 0.0188\n",
            "Iteration #150 loss: 0.0207\n",
            "Training Iteration #150 loss: 0.020738569715192584\n",
            "Iteration #151 loss: 0.0155\n",
            "Iteration #152 loss: 0.0111\n",
            "Iteration #153 loss: 0.0113\n",
            "Iteration #154 loss: 0.0150\n",
            "Iteration #155 loss: 0.0122\n",
            "Iteration #156 loss: 0.0087\n",
            "Iteration #157 loss: 0.0118\n",
            "Iteration #158 loss: 0.0197\n",
            "Iteration #159 loss: 0.0181\n",
            "Iteration #160 loss: 0.0149\n",
            "Iteration #161 loss: 0.0131\n",
            "Iteration #162 loss: 0.0459\n",
            "Iteration #163 loss: 0.0189\n",
            "Iteration #164 loss: 0.0768\n",
            "Iteration #165 loss: 0.0306\n",
            "Iteration #166 loss: 0.0134\n",
            "Iteration #167 loss: 0.0175\n",
            "Iteration #168 loss: 0.0273\n",
            "Iteration #169 loss: 0.0114\n",
            "Iteration #170 loss: 0.0110\n",
            "Iteration #171 loss: 0.0151\n",
            "Iteration #172 loss: 0.0195\n",
            "Iteration #173 loss: 0.0091\n",
            "Iteration #174 loss: 0.0106\n",
            "Iteration #175 loss: 0.0121\n",
            "Iteration #176 loss: 0.0179\n",
            "Iteration #177 loss: 0.0227\n",
            "Iteration #178 loss: 0.0122\n",
            "Iteration #179 loss: 0.0143\n",
            "Iteration #180 loss: 0.0180\n",
            "Iteration #181 loss: 0.0190\n",
            "Iteration #182 loss: 0.0536\n",
            "Iteration #183 loss: 0.0417\n",
            "Iteration #184 loss: 0.0138\n",
            "Iteration #185 loss: 0.0105\n",
            "Iteration #186 loss: 0.0151\n",
            "Iteration #187 loss: 0.0140\n",
            "Iteration #188 loss: 0.0115\n",
            "Iteration #189 loss: 0.0160\n",
            "Iteration #190 loss: 0.0224\n",
            "Iteration #191 loss: 0.0144\n",
            "Iteration #192 loss: 0.0111\n",
            "Iteration #193 loss: 0.0322\n",
            "Iteration #194 loss: 0.0127\n",
            "Iteration #195 loss: 0.1126\n",
            "Iteration #196 loss: 0.0130\n",
            "Iteration #197 loss: 0.0181\n",
            "Iteration #198 loss: 0.0162\n",
            "Iteration #199 loss: 0.0246\n",
            "Iteration #200 loss: 0.0522\n",
            "Training Iteration #200 loss: 0.05220815448488671\n",
            "Iteration #201 loss: 0.0128\n",
            "Iteration #202 loss: 0.0303\n",
            "Iteration #203 loss: 0.0136\n",
            "Iteration #204 loss: 0.0304\n",
            "Iteration #205 loss: 0.0188\n",
            "Iteration #206 loss: 0.0341\n",
            "Iteration #207 loss: 0.0185\n",
            "Iteration #208 loss: 0.0191\n",
            "Iteration #209 loss: 0.0287\n",
            "Iteration #210 loss: 0.0201\n",
            "Iteration #211 loss: 0.0112\n",
            "Iteration #212 loss: 0.0325\n",
            "Iteration #213 loss: 0.0159\n",
            "Iteration #214 loss: 0.0139\n",
            "Iteration #215 loss: 0.0096\n",
            "Iteration #216 loss: 0.0243\n",
            "Iteration #217 loss: 0.0728\n",
            "Iteration #218 loss: 0.0184\n",
            "Iteration #219 loss: 0.0168\n",
            "Iteration #220 loss: 0.0254\n",
            "Iteration #221 loss: 0.0508\n",
            "Iteration #222 loss: 0.0286\n",
            "Iteration #223 loss: 0.0140\n",
            "Iteration #224 loss: 0.0235\n",
            "Iteration #225 loss: 0.0137\n",
            "Iteration #226 loss: 0.0118\n",
            "Iteration #227 loss: 0.0128\n",
            "Iteration #228 loss: 0.0229\n",
            "Iteration #229 loss: 0.0156\n",
            "Iteration #230 loss: 0.0110\n",
            "Iteration #231 loss: 0.0244\n",
            "Iteration #232 loss: 0.0205\n",
            "Iteration #233 loss: 0.0172\n",
            "Iteration #234 loss: 0.0133\n",
            "Iteration #235 loss: 0.0283\n",
            "Iteration #236 loss: 0.0160\n",
            "Iteration #237 loss: 0.0429\n",
            "Iteration #238 loss: 0.0185\n",
            "Iteration #239 loss: 0.0270\n",
            "Iteration #240 loss: 0.0739\n",
            "Iteration #241 loss: 0.0255\n",
            "Iteration #242 loss: 0.0132\n",
            "Iteration #243 loss: 0.0227\n",
            "Iteration #244 loss: 0.0238\n",
            "Iteration #245 loss: 0.0272\n",
            "Iteration #246 loss: 0.0198\n",
            "Iteration #247 loss: 0.0227\n",
            "Iteration #248 loss: 0.0345\n",
            "Iteration #249 loss: 0.0132\n",
            "Iteration #250 loss: 0.0279\n",
            "Training Iteration #250 loss: 0.027862733714441992\n",
            "Iteration #251 loss: 0.0106\n",
            "Iteration #252 loss: 0.0136\n",
            "Iteration #253 loss: 0.0225\n",
            "Epoch #20 Training loss: 0.021852210182576395\n",
            "Epoch #20 Training loss: 0.0219\n",
            "Start validation for Epoch #20...\n",
            "Starting validation for Epoch #20...\n",
            "Iteration #1 loss: 0.0902\n",
            "Iteration #2 loss: 0.0295\n",
            "Iteration #3 loss: 0.1307\n",
            "Iteration #4 loss: 0.0427\n",
            "Iteration #5 loss: 0.0496\n",
            "Iteration #6 loss: 0.0412\n",
            "Iteration #7 loss: 0.0629\n",
            "Iteration #8 loss: 0.0477\n",
            "Iteration #9 loss: 0.0448\n",
            "Iteration #10 loss: 0.0743\n",
            "Iteration #11 loss: 0.0379\n",
            "Iteration #12 loss: 0.0675\n",
            "Iteration #13 loss: 0.1206\n",
            "Iteration #14 loss: 0.0845\n",
            "Iteration #15 loss: 0.0909\n",
            "Iteration #16 loss: 0.0385\n",
            "Iteration #17 loss: 0.0647\n",
            "Epoch #20 Validation loss: 0.0658\n",
            "Finished Training.\n",
            "Training logs can be found at /content/logs/detection/training_history/training_logs.txt\n",
            "Best Weight Model can be found at /content/logs/detection/best_model/best_model_weights.pth\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# current_file_path = Path(__file__)\n",
        "# project_dir = current_file_path.parent.parent.parent\n",
        "\n",
        "DIR_TRAIN = f\"{project_dir}/data/detection/train\"\n",
        "DIR_VALID = f\"{project_dir}/data/detection/valid\"\n",
        "\n",
        "# Dataset Preparation\n",
        "\n",
        "df_images_train, df_annot_train_filter  = load_dataset(DIR_TRAIN)\n",
        "df_images_valid, df_annot_valid_filter = load_dataset(DIR_VALID)\n",
        "\n",
        "train_dataset = CarDataset(df_images_train, df_annot_train_filter, DIR_TRAIN, None)\n",
        "valid_dataset = CarDataset(df_images_valid, df_annot_valid_filter, DIR_VALID, None)\n",
        "\n",
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "valid_data_loader = DataLoader(\n",
        "    valid_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Training the model\n",
        "model = build_model_object_detection(backbone='resnet50', num_class=2, use_pretrained=True)\n",
        "\n",
        "model.to(device)\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "lr_scheduler = None\n",
        "num_epochs = 20\n",
        "\n",
        "# Create directories for saving logs and best model if they don't exist\n",
        "logs_dir = f\"{project_dir}/logs/detection\"\n",
        "training_history_dir = f\"{logs_dir}/training_history\"\n",
        "best_model_dir = f\"{logs_dir}/best_model\"\n",
        "\n",
        "os.makedirs(logs_dir, exist_ok=True)\n",
        "os.makedirs(training_history_dir, exist_ok=True)\n",
        "os.makedirs(best_model_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Ensured directories exist: {training_history_dir}, {best_model_dir}\")\n",
        "\n",
        "training_loss_tracker = Averager()\n",
        "validation_loss_tracker = Averager()\n",
        "\n",
        "loss_history = training_model(train_data_loader,\n",
        "                   training_loss_tracker,\n",
        "                   valid_data_loader,\n",
        "                   validation_loss_tracker,\n",
        "                   model,\n",
        "                   optimizer,\n",
        "                   device,\n",
        "                   num_epochs,\n",
        "                   lr_scheduler=lr_scheduler,\n",
        "                   path_to_save=logs_dir)\n",
        "\n",
        "plot_loss_history(loss_history, training_history_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN-5EWCqM0JB"
      },
      "source": [
        "### model/detection/eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O41AgwtCMmT7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# from submission.model.detection.utils import CarDataset, collate_fn, Averager\n",
        "# from submission.model.detection.model import build_model_object_detection\n",
        "# from ..utils import load_model\n",
        "# from .utils import loss_epoch, load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjeVCofbR9J",
        "outputId": "538812f0-c802-443a-e612-9fd3bab517ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights loaded successfully from /content/logs/detection/best_model/best_model_weights.pth\n",
            "Model weights loaded successfully from /content/logs/detection/best_model/best_model_weights.pth\n",
            "Test Loss: 0.07784749371191242\n"
          ]
        }
      ],
      "source": [
        "DIR_TEST = f\"{project_dir}/data/detection/test\"\n",
        "\n",
        "df_images_test, df_annot_test_filter  = load_dataset(DIR_TEST)\n",
        "\n",
        "test_dataset = CarDataset(df_images_test, df_annot_test_filter, DIR_TEST, None)\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model = build_model_object_detection(backbone='resnet50', num_class=2, use_pretrained=False)\n",
        "model_save_path = f'{project_dir}/logs/detection/best_model/best_model_weights.pth'\n",
        "load_model(model, model_save_path, device)\n",
        "model.to(device)\n",
        "\n",
        "print(f\"Model weights loaded successfully from {model_save_path}\")\n",
        "\n",
        "test_loss_tracker = Averager()\n",
        "\n",
        "test_loss = loss_epoch(process_name=\"Testing\",\n",
        "                       loss_tracker=test_loss_tracker, \n",
        "                       data_loader=test_data_loader, \n",
        "                       model = model,\n",
        "                       optimizer=None,\n",
        "                       device=device, \n",
        "                       path_to_save=None\n",
        "                    )\n",
        "print(f\"Test Loss: {test_loss}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
